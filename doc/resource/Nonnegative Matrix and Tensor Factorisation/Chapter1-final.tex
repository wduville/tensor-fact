

\setcounter{chapter}{0}
\input{def11.set}

\chapter{Introduction -- Problem Statements and  Models}
\label{Ch1}

\markboth{\uppercase{A. Cichocki, R. Zdunek, A.H. Phan, S. Amari}}{}

\vspace{0.8cm}

\begin{bibunit} [plain]%[vancouver]%

%	\setstretch{1.66}

Matrix  factorization is an important and  unifying topic in signal
processing and linear algebra, which has found numerous applications in
many other areas.
This chapter introduces basic linear and multi-linear{\footnote{A function in two or more variables is said to be multi-linear if it is linear in each variable separately.}} models for matrix and tensor factorizations and decompositions,  and formulates the analysis framework for the solution
of  problems posed in this book. The workhorse in this book is Nonnegative Matrix Factorization (NMF)
 for sparse representation of data  and its extensions including the
 multi-layer NMF, semi-NMF, sparse NMF, tri-NMF,
 symmetric NMF, orthogonal NMF, non-smooth NMF (nsNMF), overlapping NMF, convolutive NMF (CNMF),
 and large-scale NMF.
 Our particular emphasis is on NMF and semi-NMF models and their extensions to
 multi-way models (i.e.,
multi-linear models  which perform multi-way array (tensor)
decompositions) with nonnegativity and sparsity constraints,
including, Nonnegative Tucker Decompositions (NTD),  Constrained Tucker Decompositions, Nonnegative and
semi-nonnegative Tensor Factorizations
(NTF) that are mostly based on a family of the TUCKER, PARAFAC and PARATUCK models.\\


%The main objectives and motivation  in this book are to:

As the theory and applications of NMF, NTF and  NTD are still being developed, our aim is to produce a
unified, state-of-the-art framework for the analysis and development of efficient and robust algorithms.
In doing so, our main goals are to:
\begin{enumerate}


\item Develop  various working tools and algorithms for data decomposition and  feature extraction
based on nonnegative
matrix factorization (NMF) and sparse component analysis (SCA)
approaches. We thus  integrate several emerging techniques in order to
estimate physically, physiologically, and neuroanatomically
meaningful sources or latent (hidden) components with morphological
constraints. These constraints include nonnegativity, sparsity, orthogonality,
smoothness, and semi-orthogonality.

\item Extend NMF models to multi-way array (tensor)  decompositions, factorizations, and filtering,
and to derive efficient learning algorithms for these models.

\item Develop a class of advanced blind source separation
(BSS), unsupervised feature extraction and clustering  algorithms,
and to evaluate their performance using {\it a priori} knowledge and
morphological constraints.

\item Develop computational methods to efficiently solve  the bi-linear
system $\bY= \bA \bX + \bE$ for noisy data, where $\bY$ is an input
data matrix, $\bA$ and $\bX$ represent unknown matrix factors to be estimated, and
the matrix $\bE$ represents error or noise (which should be
minimized using suitably designed cost function). %\cite{CiAm02}.

\item  Describe and analyze various cost functions (also referred to as (dis)similarity measures or divergences) and  apply  optimization criteria  to ensure
 robustness with respect to uncertainty, ill-conditioning, interference  and noise distribution.

\item Present various  optimization techniques and  statistical methods to derive efficient  and robust learning (update) rules.

\item Study what kind of prior information and constraints can be used to render
the problem solvable, and illustrate how to use this information in
practice.


\item Combine  information from different
 imaging modalities (e.g., electroencephalography \linebreak(EEG), magnetoencephalography (MEG), electromyography (EMG),
 electrooculography (EOG), functional magnetic resonance imaging (fMRI), positron emission tomography (PET)),
  in order to provide data integration and assimilation.

 \item Implement and optimize algorithms for  NMF,   NTF
 and  NTD together with providing pseudo-source codes and/or efficient source codes  in MATLAB, suitable for parallel computing and large-scale-problems.

 \item Develop  user-friendly toolboxes which supplement this book: NMFLAB and MULTI-WAY-LAB for
 potential applications to data analysis, data mining, and blind source separation.

\end{enumerate}
%
 Probably the most useful and best understood
matrix factorizations are the Singular Value Decomposition (SVD), Principal Component Analysis (PCA), and
LU, QR, and Cholesky decompositions (see Appendix).
In this book we mainly focus on nonnegativity and
sparsity constraints for factor matrices.
We shall therefore attempt to
illustrate why nonnegativity and sparsity constraints play a key role in our investigations.



\section{Blind Source Separation and Linear Generalized Component Analysis}

Blind source separation (BSS)\inxx{Blind Source Separation} and related methods, e.g., independent
component analysis (ICA)\inxx{ICA}, employ a wide class of
unsupervised learning algorithms and have found important
applications across several areas from engineering to neuroscience
\cite{CiAm02}. The recent trends in blind source separation and
generalized (flexible) component analysis (GCA) are to consider
problems in the framework of matrix factorization or more general
multi-dimensional data or signal decomposition with probabilistic
generative  models and exploit  {\it a priori} knowledge about true
nature, morphology or structure of latent (hidden) variables or
sources such as nonnegativity, sparseness, spatio-temporal
decorrelation, statistical independence,  smoothness or lowest
possible complexity. The goal of BSS can be considered as estimation
of true physical sources and parameters of a mixing system, while
the objective of GCA is to find a new reduced or hierarchical and
structured component representation for the observed (sensor) data
that can be interpreted as physically or physiologically meaningful
coding or blind signal decomposition. The key issue is to find such
a transformation or coding  which has true physical meaning and
interpretation.

Throughout this book we discuss some promising applications
of BSS/GCA in analyzing multi-modal, multi-sensory data, especially
brain  data.  Furthermore, we  derive some efficient
unsupervised learning algorithms for linear blind source separation,
and generalized component analysis using
various criteria, constraints and assumptions.

Figure  \ref{Fig1-Ch1} illustrates a  fairly general BSS problem also referred to as blind signal
decomposition or blind source extraction (BSE).
\begin{figure}[t]
\subfigure[tight][]{
\psfrag{Sources}{Sources}
\psfrag{e1t}[c][c]{$e_1(t)$}
\psfrag{e2t}[c][c]{$e_2(t)$}
\psfrag{e3t}[c][c]{$e_I(t)$}
\psfrag{x1t}[c][c]{$x_1(t)$}
\psfrag{x2t}[c][c]{$x_2(t)$}
\psfrag{x3t}[c][c]{$x_J(t)$}
\psfrag{y1t}[c][c]{$y_1(t)$}
\psfrag{y2t}[c][c]{$y_2(t)$}
\psfrag{y3t}[c][c]{$y_I(t)$}
\includegraphics[width=7.1cm,height=6cm]{bspa_c_2}\label{Fig1-Ch1_1}}
\hfill\subfigure[tight][]{\includegraphics[width=5.8cm,height=5.4cm] {C1meg}\label{Fig1-Ch1_2}}
%    \hspace{0.5cm}  (a) \hspace{8.6cm} (b)%\\
%    \begin{center}
%       \leavevmode
%    \includegraphics[width=7.2 cm,height=6cm]{bspa_c}
%    \hspace{1.2cm}
%     \leavevmode
%    \includegraphics[width=4.9cm,height=5.4cm] {C1meg}
%    \end{center}
   % \par
    \caption{\subref{Fig1-Ch1_1} General model illustrating blind source separation (BSS),
    \subref{Fig1-Ch1_2} Such models are exploited, for example, in noninvasive
    multi-sensor recording  of brain activity using EEG (electroencephalography)
    or MEG (magnetoencephalography).
    It is assumed that the scalp sensors (e.g., electrodes, magnetic or optical sensors)
    pick up a superposition of neuronal brain sources and non-neuronal
    sources (noise or physiological artifacts) related, for example,  to movements of eyes and  muscles.
    Our objective is to identify the individual signals coming from different
    areas of the brain.}
    \label{Fig1-Ch1}
    \end{figure}
%
%\begin{figure}[t]
%\subfigure[tight][]{\includegraphics[width=7.1cm,height=6cm]{bspa_c}\label{Fig1-Ch1_1}}
%\hfill\subfigure[tight][]{\includegraphics[width=5.8cm,height=5.4cm] {C1meg}\label{Fig1-Ch1_2}}
%%    \hspace{0.5cm}  (a) \hspace{8.6cm} (b)%\\
%%    \begin{center}
%%       \leavevmode
%%    \includegraphics[width=7.2 cm,height=6cm]{bspa_c}
%%    \hspace{1.2cm}
%%     \leavevmode
%%    \includegraphics[width=4.9cm,height=5.4cm] {C1meg}
%%    \end{center}
%   % \par
%    \caption{\subref{Fig1-Ch1_1} General model illustrating blind source separation (BSS),
%    \subref{Fig1-Ch1_2} Such models are exploited for example in non-invasive
%    multi-sensor recording  of brain activity using EEG (electroencephalography)
%    or MEG (magnetoencephalography).
%    It is assumed that the scalp sensors (e.g., electrodes, magnetic or optical sensors)
%    pick up a superposition of neuronal brain sources and non-neuronal
%    sources (noise or physiological artifacts) related, for example,  to movements of eyes and  muscles.
%    Our objective is to identify the individual signals coming from different
%    areas of the brain.}
%    \label{Fig1-Ch1}
%    \end{figure}
%
%\begin{figure}[h]
%    \hspace{0.5cm}  (a) \hspace{8.6cm} (b)%\\
%    \begin{center}
%       \leavevmode
%    \includegraphics[width=7.2 cm,height=6cm]{bspa_c}
%    \hspace{1.2cm}
%     \leavevmode
%    \includegraphics[width=4.9cm,height=5.4cm] {C1meg}
%    \end{center}
%   % \par
%    \caption{(a) General model illustrating blind source separation (BSS),
%    (b) Such models are exploited for example in non-invasive
%    multi-sensor recording  of brain activity using EEG (electroencephalography)
%    or MEG (magnetoencephalography).
%    It is assumed that the scalp sensors (electrodes, SQUIDs)
%    picks up superposition neuronal brain sources and non-brain
%    sources (noise or physiological artifacts) related for example  to movements of eyes,  muscles and noise.
%    Objective is to identify the individual signals coming from different
%    areas of the brain.}
%    \label{Fig1-Ch1}
%    \end{figure}
%%
%
We observe records of $I$ sensor signals   $\by(t) =[y_1(t), y_2(t),$ $\ldots, y_I(t)]^T$ coming from a MIMO (multiple-input/multiple-output) mixing and filtering system,
         where $t$ is usually a discrete time sample,{\footnote{Data are often represented not in
the time domain but in the complex frequency or  time-frequency
domain, so, the index $t$ may have a different meaning and can be multi-dimensional.}} and
         $(\cdot)^T$ denotes transpose of a vector.
 These signals are usually a superposition (mixture) of $J$ unknown source signals $\bx(t) = [x_1(t), x_2(t),\ldots, x_J(t)]^T$
 and noises $\mbi e(t) =[e_1(t),e_2(t),$$ \ldots, e_I(t)]^T$.
The primary objective is  to estimate all the primary
    source signals $x_j(t)=x_{jt}$
    or only some of them with specific
    properties.
    This estimation is usually performed  based only on the output (sensor, observed) signals $y_{it}=y_i(t)$.

In order to estimate sources, sometimes we try  first  to identify the mixing system or its
 inverse (unmixing) system and then estimate the sources.
Usually, the inverse (unmixing)  system should be  adaptive
 in such a way that it has
 some tracking capability in a nonstationary environment.
    %
 Instead of estimating the source signals directly by projecting observed signals  using
 the unmixing system,
 it is often  more convenient to identify an unknown mixing and
 filtering system  (e.g., when the unmixing system does not  exist, especially
    when the system is underdetermined\inxx{underdetermined model},
    i.e., the number of observations is lower
than the number of source signals with  $I < J$) and simultaneously estimate
the source signals  by exploiting some {\it a priori}
information about the source signals and applying a suitable
optimization procedure.

There appears to be something magical about blind source
separation since
    we are estimating the original source signals without
    knowing the parameters
    of the mixing and/or filtering processes.
    It is difficult to imagine that one can estimate this at all.
    In fact, without some {\it a priori} knowledge, it is
    not possible to {\em uniquely}  estimate the
    original source signals. However, one can usually estimate them up
    to certain indeterminacies. In mathematical terms these
    indeterminacies and ambiguities can be expressed as arbitrary
    scaling and permutation of the estimated source signals.
    These indeterminacies preserve, however, the waveforms of original
    sources.
    Although these indeterminacies\inxx{ambiguities} seem to be rather
        severe limitations,  in a great  number of applications these
    limitations are not crucial, since the most relevant
    information about the source signals is contained
    in the temporal waveforms or time-frequency patterns of the source
    signals and usually not in
    their amplitudes  or the order in which they are arranged
    in the system output.{\footnote{
    For some  models, however, there is no guarantee that the
    estimated or extracted signals have exactly the same
    waveforms as the source signals, and then the requirements
    must be sometimes further relaxed to the extent that the
    extracted waveforms are distorted (i.e., time delayed, filtered or convolved)
    versions of the primary source signals.}}


The  problem  of separating or  extracting source
    signals  from a sensor array, without knowing the transmission channel
    characteristics and the sources, can be expressed briefly as a number of
    related BSS or GCA methods such  as  ICA\inxx{ICA}
     and its extensions: Topographic ICA, Multi-way ICA,
    Kernel ICA, Tree-dependent Component Analysis, Multi-resolution Subband Decomposition -ICA
    \cite{Hyv01},\cite{Cruces-Ci-Lath},\cite{CDA98N},\cite{CiGe03},
    Non-negative Matrix Factorization (NMF) \cite{Lee1999},\cite{SajdaNMF},\cite{CichZd_ICA06},
    Sparse Component Analysis (SCA)\inxx{SCA} \cite{Li-NN06},\cite{Li-SP06},\cite{Washizawa06},\cite{He-LNCS07},\cite{He-NC},
    and Multi-channel Morphological Component Analysis (MCA) \cite{BobinSFMD07}
    % \cite{conf/ica/BobinMFS07,journals/tip/BobinSFMD07,BobinCS2008}
    (see Figure  \ref{Fig2-Ch1}).
% Smooth Component
%Analysis (SmoCA) \cite{CiAm02},  Parallel Factor Analysis
%(PARAFAC) \cite{Yamaguchi04}, Time-Frequency Component Analyzer
%(TFCA) \cite{BelouchraniAm96} and Multichannel Blind Deconvolution
%(MBD) \cite{AC98pieee,ZhangL2004a,ChoiS2002a}.
 \begin{figure}
%\includegraphics[scale = .5]{graf}
\centering
\includegraphics[scale = .5]{grafC}
\caption{Four basic component analysis methods: Independent Component Analysis (ICA), Nonnegative Matrix Factorization (NMF), Sparse Component Analysis (SCA) and Morphological Component Analysis (MCA).}
\label{Fig2-Ch1}
\end{figure}


The mixing and filtering processes of the unknown input sources
$x_{j}(t), \; (j=1,2,\ldots, J)$  may  have different mathematical or physical models,
depending on the specific applications \cite{Hyv01},\cite{AC98pieee}.
Most linear BSS\inxx{BSS} models in their simplest forms can be expressed
algebraically as some specific forms of matrix factorization:
Given observation (often called sensor or input data matrix)
$\bY=[y_{it}]=[\by(1),\ldots,\by(T)] \in \Real^{I \times T}$ perform the matrix
factorization (see Figure  \ref{fig_basicBSS_block}):
%
\be
\hlbox{
\bY = \bA \bX +\bE,
}
\label{XAS1}
\ee
%
where $\bA \in \Real^{I \times J} $ represents the unknown basis matrix or mixing matrix
(depending on the application), $\bE \in \Real^{I \times T}$ is an
unknown  matrix representing errors or noises,
$\bX=[x_{jt}]=[\bx(1), \bx(2),$ $ \ldots,\bx(T)] \in \Real^{J \times T}$ contains the
corresponding latent (hidden) components that give the
contribution of each basis vector, $T$ is the number of available samples, $I$ is the number of
observations and $J$ is the number of sources or components.
%
\begin{figure}[t]
%\begin{center}
%     \subfigure[tight][]{\includegraphics[width=7.7cm,height=3.5cm]{NMFBL_c}
%     \label{Fig-BSS_1}}\\
%\end{center}
%\begin{center}
%     \subfigure[tight][]{\includegraphics[width=9.7cm,height=6.9cm]{bssn_c}
%     \label{Fig-BSS_2}}
%\end{center}
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+6.7cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{x}[bc][bc]{\color{black}\scriptsize$\times$}
\includegraphics[width=6.7cm,height=2.5cm]{NMFBL_c}\label{fig_basicBSS_block}}
\hfill
\setlength{\templength}{(-\textwidth+8.7cm)/2}
\subfigcapmargin \templength
\subfigure[]{\includegraphics[width=8.7cm,height=5.9cm]{bssn_c_fix}\label{fig_basicBSS_detail}}
%\includegraphics[width=7.7cm,height=3.5cm]{NMFBL_c}\\
%\vspace{0.8cm}
\caption{Basic linear instantaneous BSS model: \subref{fig_basicBSS_block} Block diagram, \subref{fig_basicBSS_detail} detailed model.}
\label{Fig-BSS}
\end{figure}
%(TODO - This chapter is still in preparation.
% Drafts of Chapters 2,3,4,5,6,7 are almost ready)
In general, the number of
source signals $J$ is unknown and can be larger, equal or smaller
than the number of observations.
 The above model can be written in an equivalent scalar (element-wise) form (see Figure  \ref{fig_basicBSS_detail}):
 \be
 \hlbox{
 y_{it}= \sum_{j=1}^J a_{ij} \; x_{jt} + e_{it} \qquad \mbox{or} \qquad y_{i}(t)= \sum_{j=1}^J a_{ij} \; x_{j}(t) + e_{i}(t).
 }
 \ee
%
 Usually, the latent
 components represent unknown source signals with specific statistical properties or
 temporal structures. The matrices usually have clear statistical
 properties and meanings. For example, the rows of the matrix $\bX$ that represent
sources or components should be statistically
independent  for ICA, sparse  for SCA
\cite{Li-NN06},\cite{Li-SP06},\cite{He-LNCS07},\cite{He-LNCS06},\cite{He-NC}, nonnegative for
NMF, or have other specific and additional morphological
properties such as sparsity, smoothness, continuity, or
orthogonality in GCA \cite{CiGe03},\cite{BobinSFMD07},\cite{CiAm02}.

\begin{figure}[t!]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+11.7cm)/2}
\subfigcapmargin \templength
\subfigure[]{\includegraphics[width=11.7cm,height=3.2cm,trim = -.5cm 0cm .5cm 0cm,clip = true ]{bss_c}\label{Fig-BSSICA_a}}
\subfigure[]{
\psfrag{I}{\color{black}\scriptsize$I$}
\psfrag{J}{\color{black}\scriptsize$J$}
\psfrag{1}{\color{black}\scriptsize 1}
\psfrag{1I}{\color{black}\scriptsize 1$I$}
\psfrag{IJ}{\color{black}\scriptsize $IJ$}
\includegraphics[width=11.7cm,height=8.1cm]{bssd_c}\label{Fig-BSSICA_b}}
\caption{Blind source separation using unmixing (inverse) model: (a) block diagram, and(b) detailed model.}
\label{Fig-BSSICA}
\end{figure}

In some applications the mixing matrix $\bA$ is
ill-conditioned or even singular. In such cases, some special
models and algorithms should be applied.
 %
  %  \begin{figure}[tp]
%    \par
%    (a)
%    \begin{center}
%        \leavevmode
%    \includegraphics[width=8.2 cm]{fig04}
%     \end{center}
%    \vspace{0.1cm}
%    \par
%    (b)
%    \begin{center}
%     \leavevmode
%    \includegraphics[width=9.2cm] {fig02}
%    \end{center}
%    \par
%    \caption{Block diagrams illustrating linear blind source separation or
%    blind identification problem: (a) General schema with optional whitening,
%    (b) Detailed model. For the overcomplete problem
%    ($I < J$) the separating matrix $\bW$ may not exist; in such cases we attempt to identify the
%    mixing matrix $\bA$ first and next to estimate sources by exploiting some {\it a priori}
%    knowledge such as sparsity or independence of unknown sources.}
%    \label{fig:BSS.eps}
%    %\end{center}
%    \end{figure}
%%
Although some decompositions or matrix factorizations provide an
exact reconstruction of the data (i.e., $\bY=\bA\bX$), we shall consider
here factorizations which are approximative in nature.
 In fact, many  problems in signal and image processing can be
solved in terms of matrix factorization. However, different
cost functions and imposed constraints may lead to different types
of matrix factorization. In many signal processing applications the
data matrix $\bY=[\by(1),\by(2)\ldots,\by(T)] \in \Real^{I \times
T}$ is represented by vectors $\by(t) \in \Real^I$
($t=1,2,\ldots,T$) for a set of discrete time instants $t$ as
multiple measurements or recordings. As mentioned above, the compact aggregated
matrix equation (\ref{XAS1}) can be written in a vector form as
 a system of linear equations (see Figure  \ref{Fig-BSSICA_a}), that is,
%
\be \label{xAs1}
 \by(t)= \bA \, \bx(t) + \mbi e(t), \qquad (t=1,2, \ldots,T),
 %
 \ee
 where $\by(t)= [y_1(t),y_2(t), \ldots, y_I(t)]^T$ is a vector of the
observed signals at the discrete time instant $t$ whereas $\bx(t)=
[x_1(t),x_2(t),\ldots, x_J(t)]^T $ is a vector of unknown sources at
the same time instant.
%
The  problems formulated above are closely related  to the concept of   linear
inverse problems or more generally, to solving a large
ill-conditioned system of linear equations (\inx{overdetermined} or
\inx{underdetermined}), where it is required to
 estimate  vectors $\bx(t)$ (also in some cases  to
identify a matrix $\bA$) from noisy data
\cite{Kreutz-Delgado03},\cite{CiAm02},\cite{cichocki:unbehauen93}.
%
Physical systems  are often contaminated by noise,
thus, our task is generally to find an optimal  and robust solution in
a  noisy environment.
%
 Wide classes of extrapolation,
reconstruction, estimation, approximation, interpolation and inverse
problems can be converted into  minimum norm problems of solving
underdetermined systems of linear equations (\ref{xAs1}) for $J > I$
\cite{Kreutz-Delgado03},\cite{CiAm02}.{\footnote{Generally speaking, in signal
processing applications, an overdetermined ($I>J$) system of linear
equations (\ref{xAs1}) describes filtering, enhancement,
deconvolution and identification problems, while the underdetermined
case describes inverse and extrapolation problems
\cite{cichocki:unbehauen93},\cite{CiAm02}.}}
%
%
%
It is often assumed that only the sensor vectors $\by (t)$ are available
and we need to estimate parameters of the unmixing system online.
This  enables us to perform
indirect identification of the mixing matrix $\bA$ (for $I \geq J$) by estimating
the  separating matrix $\bW= \hat \bA^\dagger$, where the symbol $(\cdot)^\dagger$
denotes the Moore-Penrose pseudo-inverse and simultaneously estimate the sources.
 In other words, for $I \geq J$  the original sources can be estimated
by  the linear transformation \be \hat \bx(t) =\bW \, \by(t), \qquad
(t=1,2,\ldots, T). \ee
%
Although many different BSS criteria and algorithms
are available, most of them  exploit various \inx{diversities}{\footnote{By diversities we mean usually different
morphological characteristics or features of the signals.}} or constraints imposed for estimated components and/or
mixing matrices  such as
mutual independence, nonnegativity, sparsity,  smoothness, predictability or lowest complexity.
 More sophisticated or advanced approaches use  combinations
or integration of various diversities, in order to separate or
extract sources with various constraints, morphology, structures or statistical
properties and to reduce the influence of noise and undesirable
interferences \cite{CiAm02}.

All the above-mentioned BSS methods
 belong to a wide class of unsupervised learning algorithms.
Unsupervised learning algorithms try to discover a structure
underlying a data set, extract of meaningful features, and
find useful representations of the given data. Since data can always be
interpreted in many different ways, some knowledge  is needed
to determine which features or properties best represent our true latent
(hidden) components. For example, PCA finds a low-dimensional
representation of the data that captures most of its variance.
%Similarly, spatio-temporal decorrelation
%(STD) decompose data to uncorrelated components in time and space.
On the other hand, SCA tries to
 explain data as a mixture of sparse components (usually, in the
 time-frequency domain), and NMF seeks to explain data by
 parts-based localized additive  representations (with nonnegativity
 constraints).

Generalized component analysis algorithms, i.e., a combination of ICA, SCA, NMF,
 and MCA,  are often considered as pure mathematical
formulas,  powerful, but rather mechanical procedures. There is an illusion that
 there is not very much  left for the user to do after the machinery has been optimally
 implemented. However, the successful and efficient use of such tools
  strongly depends on {\it a priori} knowledge, common sense,
  and appropriate use of the preprocessing and postprocessing tools.
  In other words, it is the preprocessing of data and postprocessing
  of models where expertise is truly needed in order to extract and identify
  physically significant and meaningful hidden components.
%\begin{figure}[t]
%\begin{center}
%\leavevmode\includegraphics[width=10.9cm,height=3.8cm] %{flow-proc1}%
%{Fig4bw}
%\end{center}
%\par
%\caption{Fundamental three stages implemented and exploited in the
%BSS/GCA for efficient separation, decomposition and/or extraction
%of signals.} \label{Fig11b}
%\end{figure}
%%
%Typical preprocessing   tools include: Principal Component Analysis
%(PCA),  Factor Analysis, (FA), whitening, model reduction,
%filtering, Fast Fourier Transform (FFT), Time Frequency
%Representation (TFR) and sparsification (Wavelets Package
%transformation, DCT, Curvelets, Ridgelets) of data (see Figure
%\ref{Fig11b}).
%%
%Postprocessing tools  includes: Deflation and reconstruction
%("cleaning") of original raw data by removing undesirable
%components, noise or artifacts (see Figure \ref{fig:C1arti.eps}).
%On the other hand, the assumed linear mixing models must be valid
%at least approximately and original source signals should have
%specified statistical properties \cite{CiAm02,AC98pieee,ICALAB}.
%
%The main objective of this contribution is to propose an extended
%BSS or GCA approaches which integrate or  combine several different
%criteria  in order to extract physiologically and neuroanatomically
%meaningful and plausible brain sources and to present their
%potential applications for analysis EEG/MEG, fMRI data, especially
%for very early detection of Alzheimer disease (AD) using EEG
%recordings.

%\section{Nonnegative Matrix Factorization Models and Problems}

%Matrix factorization is an important and a unifying topic in signal
%processing, linear algebra, and has found numerous applications in
%many other areas. Probably the most useful and best understood
%matrix factorizations are the SVD, EVD, PCA, LU, QR and Cholesky.
%In this book we mainly focus on nonnegativity and
%sparsity constraints for factor matrices.
%We shall therefore next provide a brief review of basic properties of nonnegative and sparse matrices in order to
% the basic properties of nonnegative and sparse matrices and to understand
%illustrate why nonnegativity and sparsity constraints play a key role in our investigations.
%
%\subsection{Nonnegative Matrices}
%
%Matrices whose elements are all nonnegative  (positive) are called nonnegative
%(positive) matrices. We are using standard notations: $\bY \in \Real_+^{I \times T}$
%  denotes  an $I \times T$  nonnegative matrix and $\ba_i \in \Real_+^I$  denotes
% an $I$-dimensional nonnegative vector.
%These subsets are,  polyhedral cones, also called
%the nonnegative orthants.
%%%
%\begin{definition} A nonnegative matrix is said to be column (row) stochastic
%if the sums of the elements in every  column (row) sums is equal to one. A nonnegative matrix is
%said to be doubly stochastic if it is column stochastic and row stochastic.
%\end{definition}
%%
%One of the  most important results for nonnegative matrices is the following \cite{Ho2008}:
%%
%\begin{theorem}
%{\bf(Perron-Frobenius)}
%\label{theo_Perron-Frobenius}
%For a square nonnegative matrix $\bY$ there exist a largest modulus eigenvalue of $\bY$ which is nonnegative
%and a nonnegative eigenvector corresponding to it.
%\end{theorem}
%%
%%{\bf Theorem 1.1 (Perron-Frobenius, see [8])}. Let $\bY$ be a square nonnegative
%%matrix. There exist a largest modulus eigenvalue of $\bY$ which is nonnegative
%%and a nonnegative eigenvector corresponding to it.
%%
%The eigenvector satisfying the Perron-Frobenius theorem is usually referred to as the Perron vector of a
%nonnegative matrix. For a rectangular nonnegative matrix, a similar
%result can be established for the largest singular value and its
%corresponding singular vector:
%\begin{theorem}
%The leading
%singular vectors: $\bu_1, \;\bv_1$ corresponding to the largest singular value $\sigma_{max}=\sigma_1$ of a nonnegative matrix $\bY = \bU \mbi \Sigma \bV^T=\sum_i \sigma_i \bu_i \bv_i^T$ (with $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_I$) are nonnegative.
%\end{theorem}
%%This is a consequence of the basic Perron-Frobenius theorem.
%%
%Based on this observation, it
%is straightforward to compute a rank-one NMF as $\hat \bY_1 = \sigma_1 \bu_1 \bv_1^T$, this idea can be extended to approximate
%a higher order NMF. If we compute the rank-one NMF and
%subtract it from the original matrix $\hat \bY_2 =\bY - \sigma_1 \bu_1 \bv_1^T$, the input data matrix will no longer be
%nonnegative, however, all negative elements can be forced to be zero or are positive and
%the procedure can be repeated \cite{Biggs2008}.
%%
%\begin{lstlisting}
%function [A,X] = lsv_NMF(Y,J)
%% NMF based on the leading singular vectors
%% J:    number of components
%% Ref:  svds MATLAB function
%
%A = zeros(size(Y,1),J);
%X = zeros(J,size(Y,2));
%for k = 1:J
%    [u,s,v] = svds(Y,1);
%    A(:,k)  = abs(u);
%    X(k,:)  = abs(sig*v);
%    Y = Y -  u*s*v';
%    Y(Y<0) = 0;
%end
%\end{lstlisting}
%
%The lsv\_NMF function employs the {\tt svds} MATLAB function to compute the first leading singular vectors
%of $\bY$. However, generally it is not robust enough, and often is used  only as initialization method  for the NMF.
%In Chapter 4, we will present another more efficient  methods that compute rank-one matrix via local learning rules.
%
%????

%Given a subset $\hat \bV$ and a matrix $\bY$, the nearest element
%of $\bV$ to $\bY$ ( with respect to a distance) is called the projection of

%$\bY$ on $\bV$, denoted by $\mathcal{P}_{\Omega}(\bY)$. When the target subset $\bV$ is the nonnegative
%orthant and the considered distance is the Euclidean distance, the
%projection of $\bY$ is denoted by [\bY]+ and defined as:

\vspace{0.5cm}

\section{Matrix Factorization Models with Nonnegativity and Sparsity Constraints}

\subsection {Why Nonnegativity and Sparsity Constraints?}

Many real-world data are nonnegative and the corresponding hidden components have a physical
meaning only when  nonnegative\inxx{why nonnegativity}. In practice, both
nonnegative and sparse decompositions of data are often either desirable or
necessary when the underlying components have a physical
interpretation. For example, in image processing and computer
vision, involved variables and parameters may correspond to pixels,
and nonnegative sparse decomposition is related to the extraction of
relevant parts from the images \cite{Lee1999},\cite{Lee2001}. In computer vision and
graphics, we often encounter multi-dimensional data, such as images,
video, and medical data, one type of which is MRI (magnetic resonance imaging).
A color image can be considered as 3D nonnegative data, two
of the dimensions (rows and columns) being spatial, and the third
one  being a color plane (channel) depending on its color space,
%(RGB, YIQ, CRCB, etc.),
while a color video sequence can be
considered as 4D nonnegative data, time being the fourth dimension.
A sparse representation of the data by
a limited number of components is an important research problem. In
machine learning, sparseness is closely related to feature selection
and certain generalizations in learning algorithms, while
nonnegativity relates to probability distributions\inxx{why sparsity}. In economics,
variables and data such as  volume, price and many other factors are
nonnegative and sparse. Sparseness constraints may  increase the efficiency
of a portfolio, while nonnegativity both increases  efficiency
and reduces  risk \cite{Zass-PCA},\cite{ShashuaZass}. In microeconomics,
household expenditures in different commodity/service groups are
recorded as a relative proportion. In information retrieval,
documents are usually represented as relative frequencies of words
in a prescribed vocabulary.  In environmental science, scientists
investigate a relative proportion of different  pollutants in water
or air \cite{Berry}.
 In biology, each
coordinate axis may correspond to a specific gene and the sparseness is
necessary for finding local patterns hidden in data, whereas
the nonnegativity is required to give physical or physiological
meaning. This is also important for the robustness of biological systems,
 where any observed
change in the expression level of a specific gene emerges from
either positive or negative influence, rather than a combination of
both, which partly cancel each other \cite{Lee2001},\cite{Zass-PCA}.

It is clear, however,
 that with  constraints such as sparsity and nonnegativity
some of the explained variance (FIT) may decrease.
In other words, it is natural to seek a trade-off between the two goals of  interpretability
(making sure that the estimated components have physical or physiological sense and meaning)
and statistical fidelity (explaining most of the variance of the data, if the data are consistent
and  do not contain too much noise).
%Solutions which have only a few nonzero and/or nonnegative components
%usually easier to interpret.
Generally, compositional data (i.e., positive sum  of components or
real vectors) are natural representations when the variables
(features) are essentially the probabilities of complementary and
mutually exclusive events. Furthermore, note that  NMF is an
additive model which does not allow subtraction; therefore it often quantitatively
describes the parts that comprise the entire entity. In other words,
NMF can be considered as a part-based representation in which a
zero-value represents the absence and a positive number represents the
presence of some event or component. Specifically, in the case of
facial image data, the additive or part-based nature of NMF has been
shown to result in a basis of facial features, such as eyes, nose,
and lips \cite{Lee1999}.
Furthermore,  matrix factorization methods that exploit nonnegativity and sparsity constraints
 usually  lead to estimation of the hidden components with specific structures and physical interpretations,
 in contrast to other blind source separation methods.


\subsection{Basic NMF Model}

NMF has been investigated by many researchers, e.g. Paatero and Tapper  \cite{Paattero94},
but it has gained popularity through the works of Lee and Seung published
in Nature and NIPS \cite{Lee1999},\cite{Lee2001}. Based on the argument that the
nonnegativity is important in human perception they proposed
simple algorithms (often called  the Lee-Seung algorithms) for
finding nonnegative representations of nonnegative data and images.

The basic NMF problem\inxx{Basic NMF model} can be stated as follows: Given a  nonnegative data
matrix $\bY \in \Real_+^{I \times T}$  (with $y_{it} \geq 0 $  or equivalently $\bY \geq \0$)
and a reduced rank $J$ ($J \leq \min(I, T)$),  find two nonnegative
matrices $\bA =[\ba_1,\ba_2, \ldots, \ba_J] \in \Real_+^{I \times J}$ and $\bX= \bB^T=[\bb_1,\bb_2, \ldots, \bb_J]^T \in \Real_+^{J \times T}$
 which  factorize $\bY$ as well as possible, that is (see Figure  \ref{Fig-BSS}):
 \be
 \hlbox{
 \bY = \bA  \bX +\bE = \bA  \bB^T +\bE,
 }
 \ee
where the matrix $\bE \in \Real^{I \times T}$ represents
 approximation error.{\footnote{Since we usually operate on  column vectors of matrices (in order to avoid a complex or confused notation) it is often convenient to use  the matrix $\bB=\bX^T$ instead of the matrix $\bX$.}}
 The factors $\bA$ and $\bX$ may have different physical meanings in different applications.
 In a BSS problem,  $\bA$ plays the role of   mixing matrix, while $\bX$ expresses source signals.
 In clustering problems,  $\bA$ is the basis matrix, while $\bX$ denotes the weight matrix.
  In acoustic analysis,  $\bA$ represents the basis patterns, while each row of $\bX$ expresses time points (positions) when sound patterns are activated.
%

In  standard NMF we only assume nonnegativity of  factor matrices $\bA$ and $\bX$.
Unlike blind source separation methods based on independent component analysis (ICA), here we
do not assume that the sources  are independent, although we will introduce
other assumptions or constraints on $\bA$ and/or $\bX$  later. Notice that this symmetry
of assumptions leads to a symmetry in the factorization: we could just
as easily write  $\bY^T \approx \bX^T \bA^T$ ,
so the meaning of  ``source'' and  ``mixture''  in NMF are often somewhat arbitrary.

The NMF model can  also be represented as a special form of the
\inx{bilinear model} (see Figure  \ref{Fig-bilinear}):
%
\be
\hlbox{
\bY = \sum_{j=1}^J
\ba_j \circ \bb_j + \bE = \sum_{j=1}^J \ba_j  \bb^T_j + \bE,
}
 \ee
where the symbol $\circ$ denotes the outer product of two vectors.
%WWWWN% ($\ba_j \circ \bb_j$ means $a_{ij} b_{yj}$
Thus, we can build an approximate representation of the nonnegative data matrix $\bY$ as a
 sum of rank-one nonnegative matrices $\ba_j \bb_j^T$.
 If such decomposition is exact (i.e., $\bE=\0$) then it is called the \inx{Nonnegative Rank Factorization} (NRF)
 \cite{NRF-Chu}. Among the many possible series representations of data matrix $\bY$ by nonnegative
rank-one matrices, the smallest integer $J$ for which such a nonnegative rank-one series representation
is attained is called the nonnegative rank of the nonnegative matrix $\bY$ and it is denoted by $\mbox{rank}_+(\bY)$. The nonnegative rank satisfies the following bounds \cite{NRF-Chu}:
 \be
 \mbox{rank}(\bY) \leq  \mbox{rank}_+(\bY) \leq \min\{I,T\}.
 \ee
It should be noted that an NMF is not necessarily an NRF\inxx{NRF} in the sense that the latter demands the exact factorization whereas the former is usually only an approximation.
%
\begin{figure}[ht]
\centering
\includegraphics[scale = .7]{AYmatrix_bT_c}
\caption{Bilinear NMF model. The nonnegative data matrix $\bY \in \Real_+^{I \times T}$ is approximately represented by a sum or linear combination of rank-one nonnegative matrices $\bY^{(j)}=\ba_j \circ \bb_j= \ba_j \bb_j^T \in \Real_+^{I \times T}$.}
\label{Fig-bilinear}
\end{figure}

Although the NMF can be applied to BSS problems for nonnegative sources
and nonnegative mixing matrices, its application is not limited
to BSS and it can be used in various and diverse applications far
beyond BSS (see Chapter \ref{Ch8}).
%
In many applications we require additional constraints on the elements of  matrices
$\bA$ and/or $\bX$, such as smoothness, sparsity, symmetry, and
orthogonality.


\subsection{Symmetric NMF}

In the special case when $\bA =\bB \in \Real_+^{I \times J}$ the NMF is called
a symmetric NMF\inxx{NMF,symmetric NMF}, given by
\be
\bY = \bA \bA^T +\bE.
\ee
%
This model is also considered equivalent to  Kernel K-means clustering and  Laplace spectral clustering
\cite{Ding_He05}.

If the exact  symmetric NMF ($\bE=\0$) exists then a nonnegative matrix $\bY \in \Real_+^{I \times I}$
is said to be completely positive (CP) and the smallest number of columns of $\bA \in \Real_+^{I \times J}$
satisfying the exact factorization $\bY=\bA \bA^T$ is called the
cp-rank of the matrix $\bY$, denoted by $\mbox{rank}_{cp}(\bY)$. If $\bY$ is CP,
then  the upper bound estimate of the cp-rank is given by\cite{NRF-Chu}:
\be
\mbox{rank}_{cp}(\bY) \leq \frac{\mbox{rank}(\bY)(\mbox{rank}(\bY)+1)}{2} -1,
\ee
provided $\mbox{rank}(\bY) > 1$.

%\subsubsection{\bf SemiOrthogonal NMF and Uniorthogonal NMF }
\subsection{Semi-Orthogonal NMF}

The semi-orthogonal NMF\inxx{NMF,orthogonal NMF} can be defined as \be \bY = \bA  \bX +\bE = \bA
\bB^T +\bE, \ee subject to nonnegativity constraints $\bA \geq \0$
and $\bX \geq \0$ (component-wise) and an additional orthogonality
constraint: $\bA^T \bA =\bI_J$ or $\bX \bX^T = \bI_J$.

Probably the simplest and most efficient  way  to impose
orthogonality onto the matrix $\bA$ or $\bX$  is to perform the following transformation
after each iteration
%
\be
\hlbox{
\bA \leftarrow
\bA\left[\bA^T \bA\right]^{-1/2} \, ,\qquad \mbox{or} \qquad \bX
\leftarrow \left[\bX \bX^T\right]^{-1/2} \bX.
}
\ee

\subsection{Semi-NMF and Nonnegative Factorization of Arbitrary Matrix}

In some applications the observed input data are unsigned (unconstrained or bipolar) as indicated
by
 $\bY =\bY_{\pm} \in \Real^{I \times T}$ which allows us to
 relax the constraints regarding nonnegativity of one factor (or only specific vectors of a matrix).
  This leads to approximative semi-NMF\inxx{NMF,semi-NMF} which can take the following form
  \be
\bY_{\pm} = \bA_{\pm}  \bX_+ +\bE, \qquad \mbox{or} \qquad \bY_{\pm} = \bA_+  \bX_{\pm} +\bE,
\ee
%
where the subscript in $\bA_+$ indicates that a matrix is forced to be nonnegative.

In  Chapter \ref{Ch4}
we discuss  models and  algorithms for approximative factorizations in
which the matrices $\bA$ and/or $\bX$ are restricted to contain nonnegative
entries, but  the data matrix $\bY$ %and one of the factorized matrices
may have entries with mixed signs, thus extending the  range of applications
of NMF. Such a model is often referred  to as \inx{Nonnegative Factorization} (NF) \cite{Gillis2008},\cite{Gillis2009}.
%Note that the columns of data (sensor) matrix  $\bY =[\by(1),\by(2), \ldots \by(T)]$,  can be approximated
%by a conic combination of $J$ nonnegative basis vectors that are the
%columns of $\bA =[\ba_1,\ba_2, \ldots, \ba_J]$, i.e.,
%\be
%\by(t) \cong \sum_{j=1}^J \ba_j x_{jt}, \qquad (t=1,2,\ldots,T)
%\ee
%%
%We can consider the columns of $\bA$ as the basis of the cone  completely
%contained inside the nonnegative orthant. Each column of $\bY$ is
%approximated by an element of $\bA$, typically the closest element to the
%column. We can also exchange the role of $\bA$ and $\bX$ in the sense
%each row of $\bY$ is approximated by an element of $\bX$, typically the closest
%element to the row, where $\bA$ is the cone generated by the column of $\bA$.

\subsection{Three-factor NMF}

Three-factor NMF\inxx{NMF,three-factor NMF} (also called the  \inx{tri-NMF}) can  be considered as a special case of the
multi-layer NMF and can take the following
general form \cite{Ding_li},\cite{Ding_tech}
%
\be
\hlbox{
\bY = \bA \bS \bX +\bE,
}
\ee
where nonnegativity constraints are  imposed to all or only to the selected
factor matrices:
$\bA \in \Real^{I \times J}, \; \bS \in \Real^{J \times R},$ and/or $\bX \in \Real^{R \times T}$.

It should be noted that  if we do not impose any additional
constraints to the factors (besides nonnegativity),  the
three-factor NMF can be reduced to the standard (two-factor) NMF by the
transformation $\bA \leftarrow \bA \bS$ or $\bX \leftarrow \bS \bX$.
However,  the three-factor NMF is not  equivalent to the
standard NMF if we apply special constraints  or conditions as
illustrated by the following special cases.

\subsubsection[Orthogonal Three-Factor NMF]{\bf Orthogonal Three-Factor NMF}
%\subsection{Orthogonal and Uniorthogonal 3--Factor NMF}

\inx{Orthogonal three-factor NMF} imposes additional constraints upon the two
matrices $\bA^T \bA =\bI_J$ and $\bX \bX^T = \bI_R$ while the matrix
$\bS$ can be an arbitrary unconstrained matrix  (i.e., it has both
positive and negative entries) \cite{Ding_li},\cite{Ding_tech}.

For uni-orthogonal three-factor NMF only one matrix $\bA$ or $\bX$ is orthogonal and all three matrices are usually nonnegative.
\begin{figure}
\centering
\psfrag{x}[bc][bc]{\color{black}\small $\times$}
\includegraphics[width=8.7cm,height=2.5cm]{TriNMF}
\caption{Illustration of three factor NMF (tri-NMF). The goal is
to estimate two matrices $\bA \in \Real_+^{I \times J}$ and $\bX \in
\Real_+^{R \times T}$, assuming that the matrix $\bS \in \Real^{J
\times R}$ is given, or to estimate all three factor matrices $\bA, \bS, \bX$ subject to
additional constraints such as orthogonality or sparsity.}
\label{Fig-thriNMF}
\end{figure}


\subsubsection[Non-Smooth NMF]{\bf Non-Smooth NMF}
%\subsection{Non-Smooth NMF}

Non-smooth NMF\inxx{NMF,non-smooth NMF (nsNMF)}  (nsNMF) was proposed by  Pascual-Montano {\it et al.}
\cite{Pascual_2006} and is a special case of the three-factor NMF
model in which  the matrix $\bS$ is fixed and known, and is used
for controlling the sparsity or smoothness of the matrix $\bX$ and/or
$\bA$. Typically, the smoothing matrix $\bS \in \Real^{J \times J}$
takes the form: \be \bS= (1- \Theta) \; \bI_J + \frac{\Theta}{J} \; \1_{J
\times J},
\ee
where $\bI_J$ is $J \times J$ identity matrix and $\1_{J \times J}$ is the matrix of all ones. The
scalar parameter $0 \leq \Theta \leq 1$ controls the smoothness of the
matrix operator $\bS$. For $\Theta=0$, $\bS=\bI_J$,  the model
reduces to  the standard NMF and for $\Theta \rightarrow 1$  strong
smoothing is imposed on $\bS$, causing increased sparseness of
both $\bA$ and $\bX$ in order to maintain the faithfulness of the
model.


\subsubsection[Filtering NMF]{\bf Filtering NMF}
%\subsection{Filtering NMF}

In many applications it is necessary to impose some kind of
filtering upon the rows of the matrix $\bX$ (representing source signals),\inxx{Filtering NMF}
 e.g., lowpass filtering to perform
smoothing or highpass filtering in order to remove slowly changing
trends from the estimated components (source signals). In such cases we can define the filtering NMF as
%
 \be
 \hlbox{
 \bY = \bA \bX \bF  +\bE,
 }
 \ee
 %
 where $\bF$ is a suitably designed (prescribed) filtering matrix. In the case of lowpass
filtering, we usually perform some kind of averaging in the sense
that every sample value $x_{jt}$ is replaced by a weighted average
of that value and the neighboring value, so that in the simplest scenario the smoothing lowpass
filtering matrix $\bF$ can take  the
following form:
\be
\bF = \left[
\begin{array}{rrrrrr}
1/2 & \;\; 1/3 & \;\;0 & & & 0 \\ 1/2 & \;\; 1/3 & \;\; 1/3& & & 0 \\ &\;\; 1/3& \;\; 1/3& \;\; 1/3 \\& & \ddots & \ddots & \ddots \\ 0 &  &
&1/3& \;\;1/3& \;\;1/2 \\
0 &  & &0& \;\;1/3& \;\;1/2
\end{array}
\right] \in  \Real^{T \times T}.
\ee
%\be
%\bF = \left[
%\begin{matrix}
%1/2 & &  1/3 & & 0 & & & & 0 \\ 1/2 && 1/3 && 1/3 &&&  & 0 \\ & & 1/3 & & 1/3 & & 1/3 \\& & & & \ddots && \ddots && \ddots \\
%0 &  & && & 1/3 & & 1/3 &&1/2 \\
%0  & && & &0 && 1/3 && 1/2
%\end{matrix}
%\right] \in  \Real^{T \times T}.
%\ee
A standard way of performing highpass filtering is equivalent to an
application of a first-order differential operator, which means (in
the simplest scenario) just  replacing  each sample value
by the difference between the value at that point and the value at
the preceding point. For example, a highpass filtering matrix can
take following  form (using the first order or second order discrete
difference forms):
%
%\begin{equation}
%%\bF^T = \left[
%%\begin{matrix}
%%1 &0& & &   0 \\ 0 & 1 & -1& &  0 \\ 0&  0& \ 1 &  -1 & &  \\ \vdots & & \ddots &
%%\ddots \\ 0 & &
%%& 0 & 1
%%\end{matrix}
%%\right] \in  \Real^{T \times T},
%%\quad \mbox{or}  \;\;\;
%\bF = \left[
%\begin{matrix}
%1&-1&0& & & 0 \\-1&2&-1& & & 0 \\ &-1&2&-1 \\& & \ddots & \ddots & \ddots \\ 0 & & &
%&-1&2&-1 \\
% 0 & & & & &-1&1
%\end{matrix}
%\right] \in  \Real^{T \times T} .
%\end{equation}
%
\be
\bF = \left[
\begin{array}{rrrrrr}
1&-1&0& & & 0 \\-1&\;\;2&-1& & & 0 \\ &-1&\;\;2&-1 \\& & \ddots & \ddots & \ddots \\ 0 & &
&-1&\;\;\;2&-1 \\
 0 & & & & -1&1

\end{array}
\right] \in  \Real^{T \times T} \,.
\ee
Note that since the matrix $\bS$  in the \inx{nsNMF} and the matrix  $\bF$ in filtering NMF
are known or designed  in advance, almost all
the algorithms known for the standard NMF can be straightforwardly
extended to the nsNMF and Filtering NMF, for example, by defining new
matrices $\bA \stackrel{\triangle}{=} \bA \bS$, $\bX \stackrel{\triangle}{=} \bS \bX$, or
$\bX \stackrel{\triangle}{=} \bX \bF$, respectively.

\subsubsection[CGR/CUR Decomposition]{\bf CGR/CUR Decomposition}
%\subsubsection{CUR Decomposition}

%
\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth,trim = 0cm .5cm 0cm .2cm,clip = true]{CUR}
\caption{Illustration of CUR decomposition. The objective is to select such rows and columns of data matrix $\bY \in \Real^{I \times T}$  which provide the best approximation. The matrix $\bU $ is
usually the pseudo-inverse of the matrix $\bZ \in \Real^{R \times
C}$, i.e., $\bU =\bZ^{\dag}$. For simplicity of graphical illustration, we have assumed that the joint $R$ rows and the joint $C$ columns of the matrix $\bY$ are selected.} \label{Fig-CUR}
\end{figure}
In the CGR,\inxx{CGR decomposition}  also recently called CUR\inxx{CUR decomposition} decomposition, a given data matrix $\bY \in \Real^{ I \times
T}$ is decomposed as follows \cite{Goreinov},\cite{Goreinov09},\cite{Drineas-CUR},\cite{CUR-tensor},\cite{Drineas-CUR2}:
%
\be
\hlbox{
\bY =  \bC \bU \bR
+\bE,
}
\ee
where $\bC \in \Real^{I \times C}$ is a matrix
constructed from $C$ selected columns of $\bY$, $\bR \in
\Real^{R \times T}$ consists of $R$ rows of $\bY$ and matrix $\bU \in
\Real^{C \times R}$ is chosen to minimize the error $\bE \in \Real^{I
\times T}$. The matrix $\bU $ is often the pseudo-inverse of a
matrix $\bZ \in \Real^{R \times C}$, i.e., $\bU =\bZ^{\dag}$, which is defined by the intersections of the selected rows and columns (see
Figure  \ref{Fig-CUR}).  Alternatively, we can compute a core matrix $\bU$ as $\bU = \bC^{\dag} \bY \bR^{\dag}$, but in this case knowledge of the whole data matrix $\bY$ is necessary.

Since typically, $C<<T$ and $R <<I$, our challenge is to find a matrix $\bU$ and select  rows
and columns of $\bY$ so that for the fixed number of columns and rows the error cost function $||\bE||_F^2$ is minimized.  It was proved by Goreinov {\it et al.} \cite{Goreinov09}  that for $R=C$ the following bounds can be theoretically achieved:
\begin{equationarray}{llll}
&||\bY - \bC \bU \bR||_{\max} &\leq& (R+1) \; \sigma_{R+1},\\[1ex]
&||\bY - \bC \bU \bR||_{F} &\leq&  \sqrt{1+ R(T-R)} \; \sigma_{R+1} ,
%||\bY - \bC \bU \bR||_{2} \leq \sigma_{R+1} (1+ 4 \sqrt{RI}),
%||\bY - \bC \bU \bR||_{F} \leq  (1+ \varepsilon)  ||\bY - \sum_{r=1}^T \; \sigma_{r} \bu-r \bv_r^T||_F,
\end{equationarray}
%
where $||\bY||_{\max}=\max_{it} \{|y_{it}|\}$ denotes max norm and $\sigma_{r}$ is the $r$-th singular value
%and $\bu_r, \bv_r$ are corresponding singular vectors
of $\bY$.

Without loss of generality,  let us assume that the first $C$ columns and the first $R$ rows of the matrix $\bY$ are selected so the matrix is partitioned as follows:
\begin{equation}
\bY = \left[
\begin{matrix}
\bY_{11} & \bY_{12} \\ \bY_{21} &  \bY_{22}
\end{matrix}
\right] \in  \Real^{I \times T},
\quad \mbox{and }  \;\;\; \bC = \left[
\begin{matrix}
\bY_{11} \\ \bY_{21}
\end{matrix}
\right] \in  \Real^{I \times C}, \quad  \bR = \left[\begin{matrix}
\bY_{11} & \bY_{12}
\end{matrix}
\right] \in  \Real^{R \times T},
\end{equation}
then the following bound is obtained \cite{Goreinov}
\be
||\bY - \bC \bY_{11}^{\dag} \bR||_{F} \leq  \gamma_R \;  \sigma_{R+1},
\ee
where $\gamma_R=\min \left\{ \sqrt{(1+||\bY_{21} \bY^{\dag}_{11}||_F^2)}, \sqrt{(1+||\bY^{\dag}_{11} \bY_{12}||_F^2)} \right\}$.
%\be
%\gamma_R=\min \left\{ \sqrt{(1+||\bY_{21} \bY^{\dag}_{11}||_F^2)}, \sqrt{(1+||\bY^{\dag}_{11} \bY_{12}||_F^2)} \right\}.
%\ee
This formula allows us to identify  optimal columns and rows in sequential manner \cite{Caiafa-Cichocki-CUR}.
In fact, there are
 several strategies for the selection of suitable columns and rows.
 The main principle is to select columns and rows that exhibit high ``statistical
leverage'' and  provide the best low-rank fit of the data matrix \cite{Goreinov09},\cite{Drineas-CUR}.

 In the special case, assuming that $\bU \bR =\bX$, we have  CX decomposition:
 \be
 \hlbox{
 \bY=\bC \bX +\bE.
 }
 \label{CX-factorization}
 \ee
 The CX and CUR (CGR) decompositions
are low-rank matrix decompositions that are explicitly
expressed in terms of a small number of actual columns and/or
actual rows of the data matrix and they have recently received  increasing attention in the data analysis community, especially for nonnegative data due to many potential applications \cite{Goreinov09,Drineas-CUR,Drineas-CUR2,CUR-tensor}.
The CUR decomposition has an  advantage that components (factor matrices $\bC$ and $\bR$)
are directly obtained from rows and
columns of data  matrix $\bY$,  preserving desired properties such as nonnegativity or sparsity.
Because they are constructed from
actual data elements, CUR decomposition is often  more easily interpretable by
practitioners of the field from which the data are drawn (to the extent that the original
data points and/or features are interpretable) \cite{Drineas-CUR2}.


\subsection{NMF with Offset (Affine NMF)}
\label{subsec:aNMF}
%\subsection{\bf Affine NMF }

In NMF with offset (also called  affine NMF, aNMF)\inxx{NMF,affine NMF}, our goal is to remove the base line or DC
bias from the matrix $\bY$ by
using a slightly modified NMF model:
%
\be
\hlbox{
\bY = \bA \bX + \ba_0 \1^T +\bE,
} \label{equ_affineNMF}
\ee
where $\1 \in \Real^T$ is a vector of all ones and  $\ba_0
\in \Real_+^{I}$ is a vector which is selected in such a way that
the matrix $\bX$ is zero-grounded, that is, with a possibly
large number of zero entries in each row (or for noisy data
close to zero entries). The term $\bY_0=\ba_0 \1^T$  denotes offset,
 which together with nonnegativity
constraint often ensures the sparseness of factored matrices.
The main role of the offset  is to absorb the constant  values of a
data matrix, thereby making the factorization sparser and therefore improving
(relaxing) conditions for the uniqueness of NMF (see next sections).
Chapter \ref{Ch3} will demonstrates the affine NMF with multiplicative algorithms.
However, in practice, the offsets are not the same and perfectly constant in all data sources.
For image data, due to illumination flicker, the intensities of offset regions
vary between images. Affine NMF with the model (\ref{equ_affineNMF}) fails to decompose such data.
The Block-Oriented Decomposition (BOD1) model presented in section (\ref{subsec:BOD}) will help us resolving this problem.

\subsection{Multi-layer NMF}

In multi-layer NMF\inxx{multi-layer NMF} the basic matrix $\bA$ is replaced by a set of
cascaded (factor) matrices. Thus, the model can be described as
(see Figure  \ref{Fig-multiNMF})
%
\be
\hlbox{
\bY =\bA^{(1)} \bA^{(2)} \cdots \bA^{(L)} \bX + \bE.
}
\ee
%
Since the model is linear, all the matrices can be
merged into a single matrix $\bA$ if no special constraints are
imposed upon the individual matrices $\bA^{(l)}, (l=1,2,\ldots,L)$.
However, multi-layer NMF can be used to considerably improve
the performance of standard NMF algorithms due to distributed structure and
alleviating the problem of local minima.

 \begin{figure}[ht]
\centering
%
%\includegraphics[width=3.7cm,height=5.9cm]{projNMF}\\
%\vspace{1cm}
\includegraphics[width=9.7cm,trim = 0 .2cm 0 0, clip = true]{multiNMF_c}
%
\caption{Multilayer NMF model. In this model the global factor matrix $\bA = \bA^{(1)}  \bA^{(2)}  \cdots \bA^{(L)}$ has distributed representation in which each matrix $\bA^{(l)}$ can be sparse.}
\label{Fig-multiNMF}
\end{figure}

To improve the performance of the  NMF algorithms
 (especially for ill-conditioned and badly-scaled
 data) and  to reduce the risk of  converging to local minima of a cost function due
  to nonconvex alternating minimization,
 we have developed a simple hierarchical
 multi-stage procedure \cite{CichZd_ICANNGA07},\cite{CichZd_ICASSP07},\cite{CichZd_ICAISC06},\cite{CichZd_ICA07}
 combined with a \inx{multi-start initialization}, in which we perform
 a sequential decomposition of nonnegative matrices as follows.
 In the first step, we perform the basic approximate decomposition
 $\bY \cong \bA^{(1)} \bX^{(1)} \in \Real^{I \times T}$
 using any available NMF algorithm.
 In the second stage, the results obtained from the first stage
 are used to   build up a new input data matrix $\bY \leftarrow \bX^{(1)}$, that is,
 in the next step, we
 perform a similar decomposition
$ \bX^{(1)} \cong \bA^{(2)}  \bX^{(2)} \in \Real^{J \times T}$,
 using the same or different update rules.
%
 We continue our decomposition taking into account only the last obtained components.
 The process can be repeated for an arbitrary number of times until some stopping criteria are satisfied.
 Thus, our multi-layer NMF  model has the form:
 \be
 \bY \cong \bA^{(1)}  \bA^{(2)}  \cdots \bA^{(L)}
 \bX^{(L)},  \ee
 with  the final results $\bA = \bA^{(1)}  \bA^{(2)}  \cdots \bA^{(L)}$ and $\bX =\bX^{(L)}.$
%
 Physically, this means that we build up a distributed system\inxx{distributed NMF system} that has many layers or cascade
 connections
 of $L$ mixing subsystems. The key point in this approach is that the learning (update)
 process to find parameters of matrices $\bX^{(l)}$   and  $\bA^{(l)}, (l=1,2,\ldots,L)$ is performed sequentially,
 layer-by-layer, where each layer is randomly initialized with different initial conditions.
 We have found that the hierarchical multi-layer approach
 can improve performance of most NMF algorithms discussed in this book
 \cite{CichZd_EL06},\cite{CichZd_ICAISC06},\cite{CichZd_ICASSP06}.


%\subsection{Three-factor NMF}
%
%Three-factor NMF (also called  tri-NMF) can  be considered as a special case of the
%multilayer NMF and can take the following
%general form \cite{Ding_li,Ding_tech}: \be \bY = \bA \bS \bX +\bE,
%\ee where nonnegativity constraints are usually imposed to all or the selected
%factor matrices: $\bA \in \Real^{I \times J}, \; \bS \in \Real^{J
%\times R}, \bX \in \Real^{R \times T} $.
%
%It should be noted that  if we do not impose any additional
%constraints to the factors (besides nonnegativity),  the
%three-factor NMF can be reduced to the standard (two-factor) NMF by
%transformation $\bA \leftarrow \bA \bS$ or $\bX \leftarrow \bS \bX$.
%However,  the three-factor NMF is not  equivalent to the
%standard NMF if we apply special constraints  or conditions as
%illustrated by the following special cases.
%
%\subsubsection{\bf Orthogonal and Uniorthogonal Three-Factor NMF }
%%\subsection{Orthogonal and Uniorthogonal 3--Factor NMF}
%
%Orthogonal three-factor NMF imposes additional constraints to two
%matrices $\bA^T \bA =\bI_J$ and $\bX \bX^T = \bI_J$ while the matrix
%$\bS$ can be an arbitrary unconstrained matrix  (i.e., it has both
%positive and negative entries) \cite{Ding_li,Ding_tech}.
%
%For uni-orthogonal 3-factor NMF only one matrix $\bA$ or $\bX$ is orthogonal and all three matrices are usually nonnegative.
%\begin{figure}
% \begin{center}
%%
%%\includegraphics[width=3.7cm,height=5.9cm]{projNMF}\\
%%\vspace{1cm}
%\includegraphics[width=8.7cm,height=2.5cm]{TriNMF}
%%
%\end{center}
%\caption{Illustration of three factor NMF (tri-NMF). The goal is
%to estimate two matrices $\bA \in \Real^{I \times J}$ and $\bX \in
%\Real^{R \times T}$, assuming that the matrix $\bS \in \Real^{J
%\times R}$ is given, or estimate all three matrices subject to
%additional constraints such as orthogonality or sparsity.}
%\label{Fig-thriNMF}
%\end{figure}
%
%
%\subsubsection{\bf Non-Smooth NMF}
%%\subsection{Non-Smooth NMF}
%
%Non-smooth NMF  (nsNMF) was proposed by  Pascual-Montano et al.
%\cite{Pascual_2006} and is a special case of the three-factor NMF
%model in which  the matrix $\bS$ is fixed and known, and is used
%for controlling the sparsity or smoothness of the matrix $\bX$ and/or
%$\bA$. Typically, the smoothing matrix $\bS \in \Real^{J \times J}$
%takes the form: \be \bS= (1- \Theta) \bI_J + \frac{\Theta}{J} \1_{J
%\times J},
%\ee
%where $\1_{J \times J}$ is the matrix of all ones. The
%parameter $0 \leq \Theta \leq 1$ controls the smoothness of the
%matrix operator $\bS$. For $\Theta=0$, $\bS=\bI_J$,  the model
%reduces to  the standard NMF and for $\Theta \rightarrow 1$  strong
%smoothing is imposed on $\bS$, causing increased sparseness of
%both $\bA$ and $\bX$ in order to maintain the faithfulness of the
%model.
%
%
%\subsubsection{\bf Filtering NMF }
%%\subsection{Filtering NMF}
%
%In many applications it is necessary to impose some kind of
%filtering for the rows of the matrix $\bX$ (representing source signals),
% e.g., lowpass filtering to perform
%smoothing or highpass filtering in order to remove slowly changing
%trends from the estimated components (source signals). In such cases we can define the filtering NMF as
%%
% \be \bY = \bA \bX \bF  +\bE, \ee where $\bF \in \Real^{T
%\times T}$ is a suitably designed (prescribed) filtering matrix. In the case of low-pas
%filtering, we usually perform some kind of averaging in the sense
%that every sample value $x_{jt}$ is replaced by a weighted average
%of that value and the neighboring value, so that in the simple scenario the smoothing lowpass
%filtering matrix $\bF$ can take  the
%following form:
%\begin{equation}
%\bF^T = \frac{1}{3}\left[
%\begin{matrix}
%1&1&1& & & 0 \\ &1&1&1 \\& & \ddots & \ddots & \ddots \\ 0 &  &
%&1&1&1
%\end{matrix}
%\right].
%\end{equation}
%A standard way of performing highpass filtering is equivalent to an
%application of a first-order differential operator, which means (in
%the simplest scenario) just  replacing  each sample value
%by the difference between the value at that point and the value at
%the preceding point. For example, a highpass filtering matrix can
%take following  form (using first order or a second discrete
%differentiation):
%%
%\begin{equation}
%\bF^T = \left[
%\begin{matrix}
%1 &-1& & & 0 \\ & \ 1 &  -1 \\ \vdots & & \ddots & \ddots \\ 0 & &
%& 1 & -1
%\end{matrix}
%\right],
%\qquad \mbox{or}  \;\;\; \bF^T = \left[
%\begin{matrix}
%-1&2&-1& & & 0 \\ &-1&2&-1 \\& & \ddots & \ddots & \ddots \\ 0 & &
%&-1&2&-1
%\end{matrix}
%\right].
%\end{equation}
%%
%Note, that since the matrix $\bS$  in the nsNMF and the matrix  $\bF$ in filtering NMF
%are known or designed  in advance, almost all
%the algorithms known for the standard NMF can be straightforwardly
%extended to the nsNMF and Filtering NMF, for example, by defining new
%matrices $\bA \triangleq \bA \bS$, $\bX \triangleq  \bS \bX$, or
%$\bX \triangleq  \bX \bF$, respectively.
%
%\subsubsection{\bf CGR Decomposition }
%%\subsubsection{CUR Decomposition}
%
%%
%\begin{figure}[ht]
% \begin{center}
%\includegraphics[width=13.4cm,height=4.3cm]{CUR}
%\end{center}
%\caption{Illustration of CGR (CUR) decomposition. The matrix $\bU $ is
%usually the pseudo-inverse of the matrix $\bZ \in \Real^{R \times
%C}$, i.e., $\bU =\bZ^{\dag}$.} \label{Fig-CUR}
%\end{figure}
%In the CGR called also sometimes CUR decomposition a given matrix $\bY \in \Real_+^{ I \times
%T}$ is decomposed as follows \cite{Goreinov,Goreinov09,Drineas-CUR,CUR-tensor}: \be \bY = \bC \bU \bR
%+\bE, \ee where $\bC \in \Real_+^{I \times C}$ is a matrix
%constructed from $C$ preselected columns of $\bY$, $\bR \in
%\Real_+^{R \times T}$ consists of $R$ rows of $\bY$ and matrix $\bU \in
%\Real^{C \times R}$ is chosen to minimize the error $\bE \in \Real^{I
%\times T}$. The matrix $\bU $ is often the pseudo-inverse of a
%matrix $\bZ \in \Real^{R \times C}$, i.e., $\bU =\bZ^{\dag}$ (see
%Figure  \ref{Fig-CUR}). Typically, $C<<T$ and $R <<I$. There are
% several strategies for the selection of suitable columns and rows \cite{Drineas-CUR}.
%%

%\subsubsection{\bf Simultaneous NMF }

\subsection{Simultaneous NMF}

In simultaneous NMF\inxx{NMF,simultaneous NMF} (\inx{siNMF}) we have available two or more linked input data matrices (say, $\bY_1$ and $\bY_2$) and the objective is to
decompose them into nonnegative factor matrices in such a way that one of a factor matrix is common,
for example, (which is a special form of the Nonnegative Tensor Factorization NTF2 model presented in Section \ref{sec:NTF2}),
\be
	\bY_1 &=& \bA_1  \bX +\bE_1, \nonumber \\
 	\bY_2 &=& \bA_2  \bX +\bE_2.
\ee
Such a problem arises, for example, in bio-informatics if we combine gene expression and transcription
factor regulation \cite{Badea}.
In this application the data matrix $\bY_1 \in \Real^{I_1 \times T}$ is the expression level of gene $t$ in a data sample $i_1$ (i.e., the index $i_1$ denotes samples, while $t$ stands for genes) and $\bY_2 \in \Real^{I_2 \times T}$ is a transcription matrix  (which is 1  whenever transcription factor $i_2$ regulates gene $t$).

\subsection{Projective and Convex NMF}

A projective NMF\inxx{NMF,projective NMF} model can be formulated as the
estimation of sparse and nonnegative matrix $\bW \in \Real_+^{I \times J}$, $I > J$,
 which satisfies the matrix equation
\be \bY = \bW \bW^T \bY +\bE. \ee
In a more general nonsymmetric
form the projective NMF involves estimation of two nonnegative
matrices: $\bA \in \Real_+^{I \times J}$ and $\bB \in \Real_+^{I
\times J}$ in the model (see Figure  \ref{Fig-convexNMF-1}):
\be \bY = \bA \bB^T \bY +\bE. \ee
This may lead to the following optimization
problem: \be \min_{\bA,\bB} ||\bY - \bA \bB^T \bY||_F^2, \qquad
\mbox{s.t.}
 \;\; \bA \geq \0, \;\; \bB \geq \0.
\ee The projective NMF is similar to the subspace PCA. However, it involves
nonnegativity constraints.
% \begin{figure}
% \begin{center}
%%
%\subfigure[tight][]{\includegraphics[width=8.7cm,height=3.1cm]{projNMF_c}\label{Fig-convexNMF-1}}\\
%\subfigure[tight][]{\includegraphics[width=8.7cm,height=3.1cm]{ConvexNMF}\label{Fig-convexNMF-2}}
%%
%\end{center}
%\caption{Illustration of (a) Projective NMF  (typically, $\bA=\bB^T =\bW$) and (b) Convex NMF.}
%\label{Fig-convexNMF}
%\end{figure}
%

% \begin{figure}[t]
%\centering
%\subcapraggedrighttrue
%\subcapnoonelinetrue
%\subfiguretopcaptrue
%\renewcommand*{\subcapsize}{\normalsize}
%\setlength{\templength}{(-\textwidth+6.7cm)/2}
%\subfigcapmargin \templength
%\subfigure[]{\includegraphics[width=6.7cm,height=2.1cm]{projNMF_c}
%\label{Fig-convexNMF-1}}
%\subfigure[]{\includegraphics[width=6.7cm,height=2.1cm]{ConvexNMF}
%\label{Fig-convexNMF-2}}
%\caption{\subref{Fig-convexNMF-1} Illustration of  Projective NMF  (typically, $\bA=\bB^T =\bW$) and \subref{Fig-convexNMF-2} Convex NMF.}
%\label{Fig-convexNMF}
%\end{figure}
 \begin{figure}[t]
\centering
\subfigure[]{\includegraphics[width=.45\textwidth,height=2.1cm]{projNMF_c}
\label{Fig-convexNMF-1}}
\hfill\subfigure[]{
\psfrag{t}[bl][tc]{\color{black}\scriptsize $T$}
\psfrag{x}[bc][bc]{\color{black}\scriptsize $\times$}
\includegraphics[width=.45\textwidth,height=2.1cm]{ConvexNMF_fix}
\label{Fig-convexNMF-2}}
\caption{\subref{Fig-convexNMF-1} Illustration of Projective NMF (typically, $\bA=\bB =\bW$) and
\subref{Fig-convexNMF-2} Convex NMF.}
\label{Fig-convexNMF}
\end{figure}
In the convex NMF\inxx{NMF,convex NMF} proposed  by Ding, Li and Jordan
\cite{Ding_tech}, we assume that the basis vectors $\bA =[\ba_1,
\ba_2, \ldots, \ba_J]$ are constrained to be convex combinations of
the data input matrix $\bY =[\by_1,\by_2, \ldots, \by_T]$. In other
words, we require that the vectors $\ba_j$ lie within the column
space of the data matrix $\bY$, i.e.: \be \ba_j=\sum_{t=1}^T w_{tj}
\by_t = \bY \bw_j \qquad \mbox{or} \qquad \bA=\bY \bW, \ee where
$\bW \in \Real_+^{T \times J}$ and $\bX=\bB^T \in \Real_+^{J \times
T}$. Usually each column in $\bW$ satisfies the sum-to-one
constraint, i.e., they are unit length in terms of the $\ell_1$-norm. % $\1 \; \bW = \1^T$.
 We restrict ourselves to convex
combinations of the columns of $\bY$. The convex NMF model can be
written in the matrix form as{\footnote{In general, the convex NMF
applies to both nonnegative data and mixed sign data which can be
written symbolically as $\bY_{\pm} = \bY_{\pm} \bW_+ \bX_+ + \bE$.}}
%
\be
\bY = \bY \bW \bX +  \bE
\ee
%
and we can apply the  transpose operator to give \be \bY^T = \bX^T \bW^T
\bY^T +  \bE^T. \ee
This illustrates that the convex NMF can be
represented in a similar way  to  the projective NMF (see Figure  \ref{Fig-convexNMF-2}).
%
The convex NMF usually implies that both nonnegative factors $\bA$
and $\bB=\bX^T$ tend to be very sparse.

The standard cost function (squared Euclidean distance) can be
expressed as \be || \bY  - \bY \, \bW \, \bB^T ||_F^2 = \tr (\bI-\bB \,
\bW^T) \, \bY^T \, \bY \,(\bI- \bW \, \bB^T) = \sum_{j=1}^J \lambda_j \; || \,\bv_j^T \,
(\bI - \bW \, \bB^T) \,||^2_2, \ee where $\lambda_j$ is the positive
$j$-th eigenvalue (a diagonal entry of diagonal matrix $\mbi
\Lambda$) and $\bv_j$ is the corresponding eigenvector for the
eigenvalue decomposition:\ $\bY^T \bY = \bV \mbi \Lambda \bV^T =
\sum_{j=1}^J \lambda_j \bv_j \bv_j^T$. This form of NMF can  also be
considered as a special form of the kernel NMF with a linear kernel
defined as $\bK=\bY^T \bY$.
%
%
\subsection{Kernel NMF}
%\subsection{Kernel NMF}

The convex NMF leads to a natural  extension of the kernel NMF
\cite{Li_du},\cite{Kulis},\cite{Rosipal01}. Consider a mapping $\by_t \rightarrow
\phi(\by_t)$ or $\bY \rightarrow \phi(\bY)=[\phi(\by_1),\phi(\by_2),
\ldots, \phi(\by_T)]$, then the kernel NMF can be defined as \be
\phi(\bY) \cong \phi(\bY) \; \bW \; \bB^T. \ee
%
This leads
to the minimization of  the cost function: \be ||\phi (\bY)  - \phi(\bY)
\bW \; \bB^T ||_F^2 = \tr (\bK) - 2 \tr (\bB^T \; \bK \; \bW) + \tr(\bW^T \; \bK \; \bW \; \bB^T
\bB), \ee which depends only on the kernel $\bK = \phi^T (\bY)\phi
(\bY)$.

%
\subsection{Convolutive NMF}
%
%
%\begin{figure}[ht]
% \begin{center}
%%
%%\includegraphics[width=3.7cm,height=5.9cm]{projNMF}\\
%%\vspace{1cm}
%\subfigure[tight][]{\includegraphics[height=3cm]{fig_CNMF_1}\label{fig_CNMF_buildmodel_1}}
%\subfigure[tight][]{\includegraphics[width=9.7cm,height=7.9cm]{fig_CNMF_AB_mix}\label{fig_CNMF_buildmodel_2}}
%%
%\end{center}
%\caption{Illustrative of extension of NMF model to convolutive NMF (a) 2 basis features (columns) in $\bA$ are activated by $\bX$, (b) building an NMF model with 2 2-D patterns $\bA^{(1)}$ and $\bA^{(2)}$ for $\bY$.}
%\label{fig_CNMF_buildmodel}
%\end{figure}
%
% a different view point of the NMF model and present  a sequence of extended models of this one. Let's consider a first toy example illustrated in Figure \ref{fig_CNMF_buildmodel_1}.
%Matrix $\bA$ contains 2 columns considered to as 2 features (patterns);
%while matrix $\bX$ with 2 rows indicates the time point at which the start event of each pattern occurs.
%It means that the feature 1 will be activated to present in the signal $\bY$
%if an element in the first row of $\bX$ gets positive value (black for 1, and white for 0).
%In this example, feature 1 occurs twice at time points 0 and 4;
%while feature 2 also occurs twice at time points 2 and 4.
%We can say that each $j$-th row of matrix $\bX$ expresses time information of each corresponding feature $\ba_i$
%in the observed signal.
%%Each feature could be repeated an arbitrary number of times depended on how many nonnegative time points.
%In practice, signals always contain basis patterns repeated an arbitrary number of times, such as acoustic signals with notes and rhythms,
%EEG spectra, images for character recognition ...
%
%
%This makes a different meaning for the NMF model which is often considered as a mixing model
%with mixing matrix and source signals, and also open some promised applications such as spectrum analysis, object recognition, monaural separation of music and voice...
%However, basis patterns (features) are not simply expressed as vectors but often as matrices.
%Figure \ref{fig_CNMF_buildmodel_2} expresses an image $\bY \in \Real^{I \times T}$ that contains 2 letter A and B at three time points.
%In such a case, this image could be considered as sum of two images $\bY_1$ and $\bY_2$ that contain only A or B.
%Using NMF model, image $\bY_1$ could be expressed by product of basis pattern $\bA^{(1)} \in \Real^{I \times P}$ and its activating matrix $\bX_1 \in \Real^{P \times T}$
%which is depicted with white for zero values, and gray for other values.
%Columns of matrix $\bA^{(1)}$ are activated sequentially, so matrix $\bX_1$ has identity elements on its diagonals (see Figure \ref{fig_CNMF_buildmodel_2}, row 3). This means that rows of matrix $\bX_1$ are right-shift versions of its first row
%\be
%    [\bX_1]_{p-1:} = \overset{p\rightarrow} {[\bX_1]_{1:}}, \quad p = 0,1,\ldots,P-1,
%\ee
%where  $\overset{p\rightarrow}\bX$ shifts the columns of $\bX$ by $p$ spots (columns) to the right, with the columns shifted in from outside the matrix set to zero.
%Analogously, $\overset{\leftarrow p}\bY$ shifts the columns of $
%\bY$ by $p$ spots to the left. These notations will also be used for
%the shifting operations of other matrices throughout this book. Note that,
%$\overset{0 \rightarrow }{\bX}=\overset{\leftarrow 0}{\bX}=\bX$.\\
%
%Therefore, we can build a new model for $\bY$ as follows
%\be
%    \bY &=& \bY_1 + \bY_2 = \bA^{(1)} \bX_1 + \bA^{(2)} \bX_2\\
%        &=& \sum\limits_{p=1}^{P}{\ba^{(1)}_p [\bX_1]_{p:} } +  \sum\limits_{p=1}^{P}{\ba^{(2)}_p [\bX_2]_{p:} } =
%        \sum\limits_{p=0}^{P-1}{\ba^{(1)}_{p+1}  \overset{p\rightarrow} {[\bX_1]_{p:} } +  \ba^{(2)}_{p+1} \overset{p \rightarrow} {[\bX_2]_{1:}}} \label{equa_CNMF_convexp}\\
%                &=&\sum\limits_{p=0}^{P-1}{\left(\left[ {\begin{array}{*{20}cc}
%                                           {\ba_{p+1}^{(1)} }  & {\ba_{p+1}^{(2)} }
%                                        \end{array}} \right]
%                                {\left[ {\begin{array}{*{20}c}
%                                           {\overset{p\rightarrow} {[\bX_1]_{1:} }}  \\
%                                           {\overset{p\rightarrow} {[\bX_2]_{1:} }}
%                                        \end{array}} \right]
%                                }\right)}
%                = \sum\limits_{p=0}^{J}{\bA_{::p+1}
%                                        {\overset{p\rightarrow} {\bX}}}
%\ee
%where tensor ${\underline \bA}$ is size of $I\times 2 \times P$ and formed by 2 feature matrices $\bA^{(1)}$ and $\bA^{(2)}$:
%$\bA_{:j:} = \bA^{(j)}, j = 1, 2$; and matrix $\bX$ is formed by two first rows of $\bX_1$ and $\bX_2$.
%Generally, if signals $\bY$ are composed by $J$ basis features
%$\bA^{(j)} \in \Real_+^{I\times P}$ and their activating matrix
%$\bX$, it can be expressed as form
%
The Convolutive NMF\inxx{Convolutive NMF} (CNMF\inxx{NMF,CNMF}) is a
natural extension and generalization of the standard NMF.  In  the
Convolutive NMF, we process a set of nonnegative matrices or
patterns which are horizontally shifted (or time delayed) versions
of the primary matrix $\bX$ \cite{Smaragdis04}.
%
\begin{figure}[t]
\centering
%
%\includegraphics[width=3.7cm,height=5.9cm]{projNMF}\\
%\vspace{1cm}
%\psfrag{Ts}[bc][bc]{\color{black}$\bS$}
%%\psfrag{s}[bl][br]{\color{black}
%%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-1pt}\hspace{1pt}$\Sigma$\end{tabular}}%
%\psfrag{JxT}[bl][bc]{\color{black}
%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-.8ex}\hspace{-1.5ex}\footnotesize$J \times T$\end{tabular}}%
%\psfrag{IxT}[bl][bc]{\color{black}
%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-.8ex}\hspace{-1.5ex}\footnotesize$I \times T$\end{tabular}}%
%%\psfrag{IxT}[tc][bc]{\color{black}\small$(I \times T)$}
%\psfrag{IxJ}[bl][bc]{\color{black}
%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-.8ex}\hspace{-1.3ex}\footnotesize$I \times J$\end{tabular}}%
%%\psfrag{IxJ}[bc][bc]{\color{black}\small$(I \times J)$}
%\includegraphics[width=7.7cm,height=7.5cm]{ConvNMF_c_fix}
\includegraphics[width=7.7cm,height=7.5cm]{Figure1.10.eps}
%
\caption{Illustration of Convolutive NMF. The goal is to  estimate
the input sources represented by nonnegative matrix $\bX \in \Real_+^{J
\times T}$ (typically, $T>>I$) and to identify the convoluting
system, i.e., to estimate a set of nonnegative matrices
$\{\bA_0,\bA_1, \ldots, \bA_{P-1}\}$ ($\bA_p \in \Real_+^{I \times J},
\;\ p=0,1,\ldots,P-1$) knowing only the input data matrix $\bY \in
\Real^{I \times T}$. Each operator $\bS_p = \bT_1 \; (p=1,2,\ldots,P-1)$
performs a horizontal shift of the columns in $\bX$ by one spot.}
\label{Fig-cNMF}
\end{figure}
In the simplest form the CNMF can be described as (see Figure  \ref{Fig-cNMF})
%
\be
\hlbox{
    \bY = \sum_{p=0}^{P-1} \bA_p \overset{p \rightarrow}\bX +\bE,
}
     \label{equ_CNMF_shift_form}
\ee where $\bY \in \Real_+^{I \times T}$ is a given input data
matrix, $\bA_p \in \Real_+^{I \times J}$ is a set of unknown
nonnegative basis matrices, $\bX = \overset{0 \rightarrow}\bX \in
\Real_+^{J \times T}$ is a matrix representing primary sources or
patterns, $\overset{p \rightarrow}\bX$ is a shifted by $p$ columns
version of $\bX$. In other words,   $\overset{p\rightarrow}\bX$
means that the columns of $\bX$ are shifted
to the right $p$ spots (columns), while the entries in the columns shifted into the matrix from
the outside  are set to zero. This shift (time-delay) is
performed by a basic operator illustrated in Figure  \ref{Fig-cNMF} as
$\bS_p=\bT_1$. Analogously, $\overset{\leftarrow p}\bY$ means that the
columns of $ \bY$  are shifted to the left $p$ spots. These
notations will also be used for the shift operations of other
matrices throughout this book (see Chapter \ref{Ch3}  for more detail). Note
that,
$\overset{0 \rightarrow }{\bX}=\overset{\leftarrow 0}{\bX}=\bX$.\\
%
The \inx{shift operator} is illustrated by the following example:
\begin{eqnarray*}
\bX = \left[ {\begin{array}{*{20}ccc}
                1 & 2 &3 \\
                4 & 5 & 6
                \end{array}}\right], &\quad
\overset{1\rightarrow} \bX = \left[ {\begin{array}{*{20}ccc}
                0 &1 & 2  \\
                0& 4 & 5
                \end{array}}\right], \;\; % & \quad
%\overset{1\rightarrow} \bX = \left[ {\begin{array}{*{20}ccc}
%                0 &1 & 2  \\
%                0& 4 & 5
%                \end{array}}\right],  &\quad
\overset{2\rightarrow} \bX = \left[ {\begin{array}{*{20}ccc}
                0 &0 & 1  \\
               0& 0 & 4
               \end{array}}\right],  &\quad
\overset{\leftarrow 1} \bX = \left[ {\begin{array}{*{10}ccc}
                2 &3 &0\\
                5 & 6&0
                \end{array}}\right]                .
\end{eqnarray*}
%
In the Convolutive NMF model,
temporal continuity exhibited by many audio signals can be expressed more
efficiently in the time-frequency domain, especially for  signals whose
frequencies vary with time.  We will present  several efficient
and extensively tested algorithms for the CNMF model in Chapter \ref{Ch3}.

\subsection{Overlapping NMF}

In Convolutive NMF we perform horizontal shift of the columns of
the matrix $\bX$. In some applications, such as in spectrogram
decomposition,  we need to perform different transformations by
shift vertically the rows of the matrix $\bX$. For example, the
observed data may be represented by a linear combination of horizontal
bars or features and modeled by transposing the CNMF\inxx{NMF,overlapping NMF}
model (\ref{equ_CNMF_shift_form}) as
%
 \be
    \bY \approx \sum_{p=0}^P (\overset{\rightarrow p} \bX)^T  \bA_p^T = \sum_{p=0}^P (\bX \bT_{\mathop p\limits_ \to} )^T  \bA_p^T
        = \sum_{p=0}^P \bT_{\mathop p\limits_ \to}^T \bX^T  \bA_p^T, \label{equ_CNMF_transpose1}
\ee
where $\bT_{\mathop p\limits_ \to} \stackrel{\triangle}{=} \bT_p$ is the horizontal-shift matrix operator such that  $\overset{\rightarrow p} \bX = \bX \, \bT_{\mathop p\limits_ \to}$ and $\overset{\leftarrow p} \bX = \bX \, \bT_{\mathop p\limits_ \leftarrow}$.
%$p>0$ for right shifting, and $p<0$ for left shifting.
%
\begin{figure}[t!]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+8.7cm)/2}
\subfigcapmargin \templength
%\subfigure[]{\includegraphics[width=8.7cm,height=4.2cm]{overMapNMFp_c}\label{Fig-overlapNMF_a}}
\subfigure[]{\includegraphics[width=8.7cm,height=4.2cm]{Figure1.11(a).eps}\label{Fig-overlapNMF_a}}
\setlength{\templength}{(-\textwidth+10.7cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{1}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-4.3pt}\fontsize{2}{2} \scriptsize 1\end{tabular}}%
\psfrag{2}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-4.3pt}\scriptsize2\end{tabular}}%
%\includegraphics[width=10.7cm,height=5.1cm]{multi-nonovelapNMFp_c}\label{Fig-overlapNMF_b}}
\includegraphics[width=10.7cm,height=5.1cm]{Figure1.11(b).eps}\label{Fig-overlapNMF_b}}
\caption{\subref{Fig-overlapNMF_a} Block diagram schema for overlapping NMF, \subref{Fig-overlapNMF_b} \protect\inx{Extended Multi-layer  NMF} model.}
\label{Fig-overlapNMF}
\end{figure}
%
For example, for the fourth-order identity matrix  this operator can
take the following form
\begin{eqnarray*}
\bT_{\mathop 1\limits_ \to} = \left[ {\begin{array}{*{30}ccc}
                0 & 1 &0 &0\\
                0 & 0 & 1&0 \\
                0 & 0 & 0 &1\\
                0 & 0 & 0 &0
                \end{array}}\right], &\quad
\bT_{\mathop 2\limits_ \to} = \bT_{\mathop 1\limits_ \to}\bT_{\mathop 1\limits_ \to}= \left[ {\begin{array}{*{30}ccc}
                0 & 0 &1 &0\\
                0 & 0 & 0&1 \\
                0 & 0 & 0 &0\\
                0 & 0 & 0 &0
                \end{array}}\right], &\quad
\bT_{\mathop 1\limits_ \leftarrow} = \left[ {\begin{array}{*{30}ccc}
                0 & 0 &0 &0\\
                1 & 0 & 0&0 \\
                0 & 1 & 0 &0\\
                0 & 0 & 1 &0
                \end{array}}\right]. \label{equ_shiftmatrix} %&\quad
\end{eqnarray*}
Transposing the horizontal shift operator $\bT_{\mathop p\limits_
\to}:= \bT_p$ gives us the vertical  shift operator $\bT_{\uparrow p} =
\bT_{\mathop p\limits_ \leftarrow}^T$ and $\bT_{\downarrow p} =
\bT_{\mathop p\limits_ \to}^T$, in fact, we have $\bT_{\uparrow p}
= \bT_{\mathop p\limits_ \to}$ and $\bT_{\downarrow p} =
\bT_{\mathop p\limits_ \leftarrow}$.

It is interesting to note that by  interchanging the role of
matrices $\bA$ and $\bX$,
%From (\ref{equ_CNMF_transpose1}), we permute rows of
that is, $\bA \stackrel{\triangle}{=} \bX$ and $\bX_p \stackrel{\triangle}{=} \bA_p$, we
obtain  the overlapping NMF introduced by Eggert {\it et al.}
 \cite{Eggert2004} and investigated by  Choi {\it et al.} \cite{Kim-Choi-overNMF}, which
can be described as  (see Fig. \ref{Fig-overlapNMF_a})
\be
\hlbox{
    \bY \cong \sum_{p=0}^P \bT_{\uparrow p} \bA  \bX_p.
}
     \label{equ_CNMF_transpose2}
\ee
%where $\bT_p:=\bT_{\uparrow p}$ (see Figure  \ref{Fig-overlapNMF_a}).

Figure  \ref{Fig-overlapNMF_b} illustrates the extended multi-layer  overlapping NMF
(by analogy to the standard  multi-layer NMF
 in order to improve the performance of the overlapping NMF).
The overlapping NMF  model can be considered as a modification or variation of the CNMF model,
where transform-invariant representations and
sparseness constraints are incorporated  \cite{Eggert2004},\cite{Kim-Choi-overNMF}.

%The non-overlapping NMF was first proposed by Eggert at al  and also investigated by S. Choi {\it et al.}
%%
%It can be
%formulated as the following problem:  Given nonnegative input data matrix $\bY \in \Real_+^{I \times T}$ estimate matrices $\{\bA \in  \Real_+^{I \times J},  \bX_k \in \Real_+^{J \times J}\}$  using a model
%\be \overline \bY =
%\sum_{k=1}^K \bT_k \bA \bX_k +\bE_k, \qquad k=1,2,\ldots,K,
%\ee where
%%$\bA=\bA^{(1)} \bA^{(2)} \cdots \bA^{(L)}$ is distributed
%%basis matrix and
%$\bT_k$ are known translation matrices [].
%This translation matrices can take different form depending on applications.
%Typically, we use shift-up or shift down of rows vectors of the identity matrix $\bI \in \Real^{I \times I}$, i.e.,
%\be
%T_k = \overset{(k-I) \rightarrow}\bI, \qquad (1 \leq k \leq 2I-1)
%\ee
%After shift-up or shift-down empty elements are zero padded.
%
% For example, for the case $I=4$  the translation matrices take the form:\\
%
%TODO\\
%
% Multiplying a vector by these translation matrices, lead to a set of vertically shifted vectors.

%\clearpage
%\newpage
%
%\begin{figure}
%%\includegraphics[scale = .5]{DAT1}
%\includegraphics[scale = .5]{DAT1_c}
%\caption{rysunek DAT1 i DAT1C}
%\label{fig1}
%\end{figure}
%


\section{Basic Approaches to Estimate Parameters of Standard NMF}

In order to estimate factor matrices $\bA$ and $\bX$ in the standard NMF, we need to consider the  similarity measure  to
quantify a difference between the data matrix $\bY$ and the approximative NMF model
matrix $\widehat \bY = \bA \bX$.
The choice of the  similarity  measure (also referred to as distance, divergence or
measure of dissimilarity) mostly depends on the probability
distribution of the estimated signals or components and on the
structure of data or a distribution of noise.
The simplest and most often used measure is based on Frobenius norm:
\be D_F(\bY || \bA \bX) = \frac{1}{2}||\bY - \bA \bX||_F^2, \ee which
is also referred to as the squared Euclidean distance. It should be
noted that the above cost function is convex with respect to either the
elements of the matrix $\bA$ or the matrix $\bX$, but not both.{\footnote{Although the NMF optimization problem is not convex, the objective functions are  separately convex in each of the two factors $\bA$ and $\bX$, which implies that finding the optimal factor matrix $\bA$ corresponding to a fixed matrix $\bX$ reduces to a convex optimization problem and vice versa. However, the convexity is lost as soon as we try to optimize factor matrices simultaneously \cite{Gillis2009}.}}
Alternating minimization  of such a cost leads to the
ALS\inxx{algorithm,ALS NMF} (Alternating Least Squares)\inxx{Alternating Least Squares} algorithm which can be described
 as follows:
\begin{enumerate}
\item Initialize $\bA$ randomly or by using a specific deterministic strategy.

\item Estimate $\bX$ from the matrix equation $\bA^T \bA \bX = \bA^T \bY$ by solving
\be
\min_{\bX} D_F(\bY || \bA \bX) = \frac{1}{2}||\bY - \bA \bX||_F^2, \qquad \mbox{with fixed} \qquad \bA.
\ee
\item Set all the negative elements of $\bX$ to zero or some small positive value.
\item Estimate $\bA$ from the matrix equation $\bX \bX^T \bA^T = \bX \bY^T$ by solving
\be
\min_{\bA} D_F(\bY || \bA \bX) = \frac{1}{2}||\bY^T -  \bX^T \bA^T||_F^2, \qquad \mbox{with fixed} \qquad \bX.
\ee
\item Set all negative elements of $\bA$ to zero or some small positive value $\varepsilon$.
\end{enumerate}
%
The above ALS algorithm can be written in the following form:{\footnote{Note that the  $\max$ operator is applied element-wise,
that is, each element of a matrix is compared with scalar parameter $\varepsilon$.}}
%As an illustrative example, we present a very efficient and simple algorithm that uses the square
%Euclidean distance, i.e. $$D(\bY||\bA\bX) = \tilde{D}(\bY||\bA\bX) = ||\bY -
%\bA\bX||_F^2.$$ Assuming $\nabla_{\bX} D(\bY||\bA\bY)$, $\nabla_{\bA}
%\tilde{D}(\bY||\bA\bX)$ for positive entries in $\bA$ and $\bX$, which occurs when a
%stationary point is reached, we have:
%\be
%\label{LS_pinv}
% \hlbox{
%\bX  \leftarrow \max  \, \left \{ \varepsilon, \;
%(\bA^T\bA)^{-1} \bA^T \bY \right\} =[\bA^\dagger \bY]_+,
%}
%\ee
%\be
% \hlbox{
%\bA  \leftarrow  \max  \, \left \{ \varepsilon, \; \bY \bX^T (\bX
%\bX^T)^{-1}  \right\} = [\bY \bX^\dagger]_+ ,
%}
%\ee
\minrowclearance 5pt
\arrayrulecolor{bordercolor}
\begin{equationarray}{| fF Nl
>{\columncolor{highlightcolor}[.2\tabcolsep][.8\tabcolsep]}rNl
>{\columncolor{highlightcolor}[.2\tabcolsep][.8\tabcolsep]}rk|}
\cline{1-5}
& \bX  &\leftarrow& \max  \, \left \{ \varepsilon, \;
(\bA^T\bA)^{-1} \bA^T \bY \right\} =[\bA^\dagger \bY]_+ \,, & \label{LS_pinv}\\
& \bA  &\leftarrow & \max  \, \left \{ \varepsilon, \; \bY \bX^T (\bX
\bX^T)^{-1}  \right\} = [\bY \bX^\dagger]_+ \,,&
\\[2ex]
\cline{1-5} \noalign {\vskip-16pt}  \arrayrulecolor{black}
\nonumber
\end{equationarray}
\minrowclearance 0pt
where $\bA^{\dagger}$ is the Moore-Penrose inverse of $\bA$, $\varepsilon$ is a small constant
(typically, $10^{-16}$) to enforce positive entries.
Various additional constraints on $\bA$ and $\bX$ can be imposed.

Today the ALS method is considered as a basic ``workhorse" approach,
however it is not guaranteed to converge to a global minimum
nor even a stationary point, but only to a solution where the cost functions
cease to decrease \cite{Kolda08},\cite{Berry}. Moreover, it is often not sufficiently accurate.
The  ALS method can
be dramatically improved and its computational complexity
reduced as it will be shown in Chapter \ref{Ch4}. % \cite{PHANCIA_08_NOLTA}.
%However, the standard ALS algorithm can be dramatically
%improved and computational complexity reduced as we show this in Chapter 4.

It is interesting to note that  the NMF problem can be
considered as a natural extension of a \inx{Nonnegative Least
Squares} (NLS) problem formulated as the following optimization problem:
given a matrix $\bA \in \Real^{I \times J}$ and a set of
observed values given by the vector $\by \in \Real^I$, find a
nonnegative vector $\bx \in \Real^J$ which minimizes the cost function
$J(\bx) = \frac{1}{2} ||\by -\bA\bx||_2^2$, i.e., \be \min_{{\bx} \geq
\bf 0} \; \frac{1}{2} ||\by -\bA \, \bx||_2^2, \ee subject to $\bx \geq
\0$.
%
There is a  large volume  of literature devoted
to the NLS
 problems which will be exploited and adopted in this book.

Another  frequently used cost function for NMF is the
generalized Kullback-Leibler divergence (also called the I-divergence)
\cite{Lee2001}: \be D_{KL}(\bY ||\bA\bX) =
  \sum_{it} \left( y_{it} \; \ln \frac{y_{it}}{[\bA\bX]_{it}}
  - y_{it} + [\bA\bX]_{it} \right). \label{KL1-ch1}
  \ee
%
Most existing approaches minimize only one kind of cost
function by alternately  switching between sets of parameters. In
this book we adopt a more general and flexible approach in which
instead of one cost function we rather exploit two or more cost
functions (with the same global minima); one of them is minimized
with respect to $\bA$ and the other one with respect to $\bX$. Such an
approach is fully justified as $\bA$ and $\bX$ may have
different distributions or different statistical properties and therefore
different cost functions can be optimal for them\inxx{algorithm,multi-layer NMF}\inxx{pseudo-code,multi-layer NMF}.
%
%\begin{algorithm}
%\caption{{Multi-layer NMF using alternating minimization of two cost functions}\label{alg_nmf}}
%\dontprintsemicolon
%\KwIn{$\bY \in \Real_+^{I \times T} $: input data, $J$: rank of approximation}
%\KwOut{$\bA \in \Real_+^{I \times J}$ and $\bX \in \Real_+^{J \times T}$
%    such that a given cost function is minimized.}
%\BlankLine
%\Begin{
%    Initialize randomly $\bA_{(l)}^{(0)}$ and $\bX_{(l)}^{(0)}$ \footnotemark\;
%    $\bX_{(0)} = \bX$\;
%    \For{$l=1$ to $L$}{
%        \For{$k=0$ to $K_{max}$}{
%        $\bA_{(l)}^{(k+1)} = \displaystyle{ \arg \min_{{\bf A} \geq 0} \left \{ D_1 \left ( \bY_{(l)} \; || \; \bA
%            \bX_{(l)}^{(k+1)} \right ) \right \} \big{|}_{\bA = \bA_{(l)}^{(k)} } }\;$ for fixed $\bX = \bX_{(l)}^{(k+1)}$ \;
%       $\bX_{(l)}^{(k+1)} = \displaystyle{ \arg \min_{{\bf X} \geq 0}
%            \left \{ D_2 \left (\bY_{(l)} \; || \; \bA_{(l)}^{(k)} \bX  \right ) \right \}
%             \big{|}_{\bX = \bX_{(l)}^{(k)} }} \;$ for fixed $\bA=\bA_{(l)}^{(k)}$\;
%       }
%         $\bX_{(l + 1)} = \bX_{(l)}^{(K_{max} + 1)}\;$
%    }
%}
%\end{algorithm}
%\footnotetext{Instead of random initialization, we can use alternatively ALS or SVD based initialization.
% See Section \ref{Sec_init}.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Corrected on 9/2, HUY PHAN
\begin{table}[ht]
\begin{minipage}[t!]{1\textwidth}
\begin{algorithm}[H]
\caption{\bf{Multi-layer NMF using alternating minimization of two cost functions}\protect\inxx{algorithm,Multi-layer NMF}\protect\inxx{pseudo-code,Multi-layer NMF}\label{alg_nmf}}
\dontprintsemicolon
\KwIn{$\bY \in \Real_+^{I \times T} $: input data, $J$: rank of approximation}
\KwOut{$\bA \in \Real_+^{I \times J}$ and $\bX \in \Real_+^{J \times T}$
    such that some given cost functions are minimized.}
\BlankLine
\Begin{
    $\bX = \bY$, \, $\bA = \bI$\;
    \For{$l=1$ to $L$}{
        Initialize randomly $\bA_{(l)}$ and $\bX_{(l)}$ $\qquad ^a$\label{alg_1note1}\;
        \Repeat{a stopping criterion is met}{
            $\bA_{(l)} = \displaystyle{ \arg \min_{{\bf A}_{(l)} \geq \bf 0} \left \{ D_1 \left ( \bX \; || \; \bA_{(l)} \bX_{(l)} \right ) \right \}  }\;$ for fixed $\bX_{(l)}$ \;
           $\bX_{(l)} = \displaystyle{ \arg \min_{{\bf X}_{(l)} \geq \bf 0}
                \left \{ D_2 \left (\bX \; || \; \bA_{(l)} \bX_{(l)} \right ) \right \}
                  } \;$ for fixed $\bA_{(l)}$\;
       }(\tcc*[f]{convergence condition})
%         $\bX_{(l + 1)} = \bX_{(l)}^{(K_{max} + 1)}\;$
         $\bX = \bX_{(l)}$\;
         $\bA \leftarrow \bA \bA_{(l)}$\;
    }
%    $\bX = \bX_{(l)}$
}
\end{algorithm}
\begin{tablenotes}
{$^a$} {Instead of random initialization, we can use
ALS or SVD based initialization, see Section \ref{Sec_init}.}
\end{tablenotes}
\end{minipage}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{algorithm}[t!]
%\caption{Multilayer NMF using alternating minimization of two cost functions}\label{alg_nmf}
%\begin{algorithmic}[1]
% \STATE  $\bX_{(0)} = \bX$
% \STATE Initialize randomly   $\bA_{(l)}^{(0)}$ and/or $\bX_{(l)}^{(0)}$ \footnotemark
% \FOR{$l=1$ to $L$}
%     \FOR{$k=0$ to $K_{max}$}
%         \STATE $\bA_{(l)}^{(k+1)} = \displaystyle{ \arg \min_{{\bf A} \geq 0} \left \{ D_2 \left ( \bY_{(l)} \; || \; \bA
%            \bX_{(l)}^{(k+1)} \right ) \right \} \big{|}_{\bA = \bA_{(l)}^{(k)} } }\;$ for fixed $\bX = \bX_{(l)}^{(k+1)}$
%        \STATE $\bA_{(l)}^{(k+1)} \leftarrow \displaystyle{ \left [\frac{a_{ij}}{\sum_{i = 1}^m a_{ij}} \right
%                ]_{(l)}^{(k+1)}}\;$
%        \STATE $\bX_{(l)}^{(k+1)} = \displaystyle{ \arg \min_{{\bf X} \geq 0}
%            \left \{ D_1 \left (\bY_{(l)} \; || \; \bA_{(l)}^{(k)} \bX  \right ) \right \}
%             \big{|}_{\bX = \bX_{(l)}^{(k)} }} \;$ for fixed $\bA=\bA_{(l)}^{(k)}$
%      \ENDFOR
%       \STATE $\bX_{(l + 1)} = \bX_{(l)}^{(K_{max} + 1)}\;$
% \ENDFOR
%\end{algorithmic}
%\end{algorithm}
%\footnotetext{Instead of random initialization, we can use alternatively ALS or  SVD based initialization. See Section\ref{Sec_init}.}


 Algorithm \ref{alg_nmf} illustrates such  a case, where
the cost functions $D_1(\bY ||\bA\bX)$ and $D_2(\bY||\bA\bX)$ can
take various forms, e.g.:  I-divergence and Euclidean distance
\cite{Sra_NIPS},\cite{CichZd_ICA06} (see Chapter \ref{Ch2}).

% Alternatively, as a cost function, we can use
% alpha divergence \cite{CichZd_ICA06}: \begin{eqnarray}
%D^{(\alpha)}_{A}(\bY||\bA\bX) & = & \sum_{ik} y_{ik} \frac
%{(y_{ik}/z_{ik})^{\alpha-1}-1}{\alpha(\alpha-1)} \nonumber \\ & + &
%\frac{z_{ik}-x_{ik}}{\alpha}, \label{CR1} \end{eqnarray} where $x_{ik}=[\bX]_{ik}$ and
%$z_{ik}=[\bA\bX]_{ik}$. Using the Algorithm \ref{alg1} we derived a new family of NMF
%algorithms for $\alpha \not = 0$:
% \begin{eqnarray}  \label{Amari_alg_1} a_{ij} & \leftarrow & a_{ij} \left( \sum_{k =1}^T
% \left(x_{ik}/ [\bA \, \bX]_{it} \right)^{\alpha}
%s_{jk} \,\right)^{1/\alpha}, \\
%a_{ij} & \leftarrow& \frac {a_{ij}} {(\sum_i  a_{ij})}, \\
%\bX & \leftarrow & \max \left \{ \varepsilon, \bA^+ \bY \right \}.  \label{Amari_alg_2}
%\end{eqnarray}

We can generalize this concept  by using not one or two cost
functions but rather a set of cost functions to be  minimized
sequentially or simultaneously.
%
For $\bA= [\ba_1,\ba_1, \ldots,\ba_J]$ and $\bB=\bX^T= [\bb_1,\bb_2,
\ldots,\bb_J]$, we can express the squared Euclidean  cost function
as
%
 \be J(\ba_1,\ba_1, \ldots,\ba_J,\bb_1,\bb_2, \ldots,\bb_J)
&=&\frac{1}{2}||\bY -\bA \bB^T||_F^2 \nonumber \\
&=& \frac{1}{2}||\bY - \sum_{j=1}^J \ba_j \bb_j^T||_F^2. \ee
%
 An underlying
idea is to define a residual (rank-one approximated) matrix (see Chapter \ref{Ch4} for more detail and
explanation) \be \bY^{(j)} \stackrel{\triangle}{=} \bY - \sum_{p \neq j} \ba_p \bb_p^T
\ee and alternately minimize the set of cost functions  with respect to the unknown variables $\ba_j, \bb_j$:
\begin{subequations}
\begin{alignat}{4}
&\quad\quad\quad\quad\quad\quad\quad D_A^{(j)}(\ba) &&= \frac{1}{2} ||\bY^{(j)} -  \ba \, \bb_j^T ||_F^2, \quad\quad&& \text{for a fixed} \; \bb_j, \\
&\quad\quad\quad\quad\quad\quad\quad D_B^{(j)}(\bb) &&= \frac{1}{2} ||\bY^{(j)} -  \ba_j \; \bb^T||_F^2,  && \text{for a fixed} \; \ba_j,
\end{alignat}
\end{subequations}
for $j=1,2,\ldots, J$ subject to $\ba \geq \0$ and $\bb \geq \0$, respectively.




\subsection{Large-Scale NMF}
\label{subsec-LNMF}

In many applications, especially in dimension reduction applications
the data matrix $\bY \in \Real^{I \times T}$ can be very large
(with millions of entries), but it can be approximately factorized using a rather smaller
number of nonnegative components
($J$), that is, $J<< I$ and $J << T$\inxx{Large-scale NMF}.
%
Then the problem $\bY \approx \bA \bX$ becomes highly redundant and we do not need to use information
 about all entries of $\bY$ in order to estimate precisely the
 factor matrices $\bA \in \Real^{I \times J}$ and $\bX \in \Real^{J \times T}$.
 In other words, to solve the large-scale NMF problem we do not need to know the whole
 data matrix but only a small
 random part of it. As we will show later, such an approach can outperform  considerably
 the standard NMF methods, especially for extremely overdetermined systems.

 In this approach, instead of performing large-scale factorization
 \be
 \bY =\bA \bX + \bE, \notag
 \ee
 we can consider a two set of linked factorizations using much smaller matrices, given by
 \be
 \bY_r &=& \bA_r \bX + \bE_r,  \qquad  \mbox{for fixed (known)} \qquad \bA_r,\\
 \bY_c &=& \bA \bX_c + \bE_c,  \qquad  \mbox{for fixed (known)} \qquad \bX_c,
\label{LSNMF}
 \ee
 where $\bY_r \in \Real_+^{R \times T}$  and $\bY_c \in \Real_+^{I \times C}$   are the
 matrices constructed from the selected rows and columns  of the matrix $\bY$, respectively.
  Analogously, we can construct the reduced matrices:  $\bA_r \in \Real^{R \times J}$ and  $\bX_c \in \Real^{J \times C}$
  by using the same indices for the columns and rows as those used for the
  construction of the data sub-matrices $\bY_c$ and $\bY_r$\inxx{Block-wise NMF}.
   In practice, it is usually sufficient to choose: $ J < R \leq 4 J$ and $ J < C \leq 4 J$.
%
\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth,trim = 0 .5cm 0 .1cm,clip = true]{LSNMF}
\caption{ Conceptual illustration of block-wise data processing
for large-scale NMF.  Instead of processing the whole matrix $\bY
\in \Real^{I \times T}$, we can process much smaller block matrices
$\bY_c \in \Real^{I \times C}$ and $\bY_r \in \Real^{R \times T}$
and corresponding factor matrices $\bX_c \in \Real^{J \times C}$ and
$\bA_r \in \Real^{R \times J}$ with $C << T$ and $R << I$. For
simplicity of graphical illustration, we have assumed that the first $R$ rows and the first
$C$ columns of the matrices $\bY, \; \bA$ and  $\bX$ are selected.} \label{Fig-LSNMF}
\end{figure}

In the special case, for the squared Euclidean distance (Frobenius norm),
instead of alternately minimizing the cost function
%
 \be D_F(\bY
\; ||\; \bA \bX) = \frac{1}{2} \left \Vert \bY - \bA \bX \right
\Vert_F^2, \ee we can minimize sequentially the two cost functions: \be
D_F(\bY_r \; ||\; \bA_r \bX) &=& \frac{1}{2} \left \Vert \bY_r - \bA_r \bX \right \Vert_F^2, \qquad \mbox{for fixed} \qquad \bA_r, \\
D_F(\bY_c \; ||\; \bA \bX_c) &=& \frac{1}{2} \left \Vert \bY_c - \bA
\bX_c \right \Vert_F^2, \qquad \mbox{for fixed} \qquad \bX_c. \ee
The minimization of these cost functions with respect to $\bX$ and
$\bA$, subject to nonnegativity constraints, leads to  the simple
ALS update formulas for the large-scale NMF:
%%
% \be
%\hlbox{
%\bA \leftarrow \left[ \bY_c \bX_c^{\dag} \right]_+ = \left[ \bY_c \bX_c^{T} (\bX_c \bX_c^{T})^{-1} \right]_+ , \qquad
%\bX \leftarrow \left[\bA_r^{\dag}  \bY_r \right]_+ =
%\left[(\bA^T_r \bA_r)^{-1} \bA_r \bY_r \right]_+.
%} \nonumber \\
%\ee
%
 \be
\hlbox{
\bX \leftarrow \left[\bA_r^{\dag}  \bY_r \right]_+ =
\left[(\bA^T_r \bA_r)^{-1} \bA_r \bY_r \right]_+ \,, \qquad
\bA \leftarrow \left[ \bY_c \bX_c^{\dag} \right]_+ = \left[ \bY_c \bX_c^{T} (\bX_c \bX_c^{T})^{-1} \right]_+ \,.
}
\ee
A similar strategy can be applied for other cost functions and details will
be given in Chapter \ref{Ch3} and Chapter \ref{Ch4}.

 There are several strategies to choose the columns and rows\inxx{selection of row and columns} of the input data matrix \cite{Boutsidis-CUR},\cite{Drineas-CUR2},\cite{Caiafa-Cichocki-CUR},\cite{Hasan07},\cite{Hasan08}.
  The simplest scenario is to choose  the first $R$ rows and the first $C$ columns of the data matrix $\bY$ (see Figure  \ref{Fig-LSNMF}) or select them randomly using a uniform distribution.
 An optional strategy is to select randomly rows and columns from the set of all rows and columns with probability proportional to their relevance, e.g., with probability proportional to square of Euclidean $\ell_2$-norm of rows and columns, i.e., $||\underline \by_{\,i}||^2_2$ and $||\by_t||^2_2$, respectively.
  Another heuristic option is to choose those rows and columns that provide the largest $\ell_p$-norm.
  For noisy data with uncorrelated noise, we can construct new columns and rows as a local average
  (mean values) of some specific numbers of
  the columns and rows of raw data. For example,  the first selected column is created as an average of the first $M$ columns, the second column is an average of the next $M$ columns, and so on; the same procedure applies for rows.
  Another  strategy is to select optimal rows and columns using optimal CUR decomposition \cite{Caiafa-Cichocki-CUR}.


\subsection{Non-uniqueness of NMF and Techniques to Alleviate the  Ambiguity Problem}
\label{Sec_ambg}

 Usually, we perform NMF using the alternating minimization scheme (see
Algorithm \ref{alg_nmf}) of a set given objective functions. However, in general, such
minimization  does not guarantee a unique solution
(neglecting unavoidable scaling and permutation ambiguities)\inxx{Non-uniqueness of NMF}. Even
the quadratic function with respect to both sets of arguments $\{
\bA\}$ and  $\{ \bX\}$ may have many local minima,  which makes NMF algorithms suffer from rotational
indeterminacy (ambiguity). For example,  consider the
 quadratic function: \be \label{fun_Gaussian}
D_F(\bY||\bA\bX) = ||\bY - \bA\bX||_F^2 = ||\bY - \bA\bR^{-1}
\bR\bX||_F^2 = ||\bY - \tilde{\bA} \tilde{\bX}||_F^2. \ee There are
many ways to select  a  rotational matrix $\bR$ which is not necessarily
nonnegative or not necessarily a generalized permutation matrix,{\footnote{Generalized permutation matrix is a matrix with only one nonzero positive element  in each row and each column.}} so that
the transformed (rotated) $\tilde{\bA} \not = \bA$ and
$\tilde{\bX} \not = \bX$ are nonnegative.
Here, it is important to
note that the inverse of a nonnegative matrix is nonnegative if and only if it is
a generalized permutation matrix  \cite{Plemmons72}.
%Here, it is important to
%note that if the square nonsingular matrix $\bR$ is
%nonnegative (written as $\bR \geq 0$), then its inverse matrix
%$\bP^{-1}$ is not necessarily nonnegative. In addition, the matrix $\bP$
%does not need to be nonnegative to satisfy the nonnegativity constraints
%of the transformed matrices $\bA \bR^{-1}$ and $\bR \bX$. However, it
If we assume that $\bR \geq 0$ and $\bR^{-1} \geq 0$
(element-wise) which are sufficient conditions for the nonnegativity of
the transform matrices $\bA \bR^{-1}$ and $\bR \bX$,
then $\bR$ must be a generalized permutation (also called monomial) matrix, i.e.,
$\bR$ can be expressed as a product of a nonsingular positive definite
diagonal matrix and a permutation matrix.
It is intuitively easy to understand that if the original matrices
$\bX$ and $\bA$ are sufficiently sparse only a generalized
permutation matrix  $\bP=\bR$ can satisfy the nonnegativity constraints of any
transform matrices and NMF is unique.
%

To illustrate \inx{rotational indeterminacy} consider
the following mixing and source matrices:
 \be \label{A_mix_2_2_a} \bA = \left [ \begin{array}{cc} 3 & 2 \\
7 & 2 \end{array} \right ], \quad \bX = \left [ \begin{array}{c} \bx_1(t) \\ \bx_2(t) \end{array} \right ], \ee
which give the output
%
\be \label{mix_2_2_b} \bY = \left [ \begin{array}{c} \by_1(t) \\ \by_2(t) \end{array} \right ] = \bA \bX = \left [
\begin{array}{c} 3 \bx_1(t) + 2 \bx_2(t) \\ 7 \bx_1(t) + 2 \bx_2(t) \end{array} \right ]. \ee
%
It is clear that there exists another nonnegative decomposition which gives us the following components: \be
\label{mix_2_2_c} \bY = \left [\begin{array}{c} 3 \bx_1(t) + 2 \bx_2(t) \\ 7 \bx_1(t) + 2 \bx_2(t) \end{array} \right ]
= \tilde{\bA} \tilde{\bX} = \left [\begin{array}{cc} 0 & 1 \\ 4 & 1 \end{array} \right ] \left [\begin{array}{c}
\bx_1(t) \\ 3 \bx_1(t) + 2 \bx_2(t) \end{array} \right ], \ee where
\be \label{A_mix_2_2_d} \tilde{\bA} = \left [ \begin{array}{cc} 0 & 1 \\
4 & 1 \end{array} \right ], \quad \tilde{\bX} = \left [ \begin{array}{c} \bx_1(t) \\ 3 \bx_1(t) + 2 \bx_2(t)
\end{array} \right ] \ee are new nonnegative components which do not come from the permutation or scaling
\inx{indeterminacies}.

 However, incorporating some sparsity or smoothness measures to the objective
function is sufficient to solve the NMF problem uniquely (up
to unavoidable  scale and permutation indeterminacies). The issues
related to sparsity measures for NMF have been widely discussed
 \cite{Hoyer04},\cite{Donoho},\cite{Heiler},\cite{CichZd_ICASSP06},\cite{CichZd_ICANNGA07},\cite{ZdCich_SP07},
 and are addressed in almost all chapters in this book.

 When no prior information is available, we should perform  normalization of
  the columns in $\bA$ and/or the rows in $\bX$  to help mitigate the effects of rotation indeterminacies. Such normalization is usually performed by scaling the columns $\ba_j$ of $\bA = [\ba_1, \ldots, \ba_J]$ as follows: \be
\label{norm_A} \bA \leftarrow \bA \bD_A, \quad {\rm where} \quad \bD_A = \diag (||\ba_1||_p^{-1}, ||\ba_2||_p^{-1},
\ldots, ||\ba_J||_p^{-1} ), \qquad p \in [0, \infty).\ee
%
Heuristics based on extensive experimentations show that  best results can be obtained for $p =
1$, i.e., when the columns of $\bA$  are normalized to unit  $\ell_1$-norm. This may be justified by the fact that the mixing
matrix  should contain only a few dominant entries in each column, which is emphasized by the normalization to the unit
$\ell_1$-norms.{\footnote{In the case when the columns of $\bA$ and rows of $\bX$ are both normalized, the standard NMF model $\bY \approx \bA \bX$ is converted to a three-factor NMF model $\bY \approx \bA \bD \bX$, where $\bD =\bD_A \bD_X$ is a diagonal scaling matrix.}}
%
The normalization  (\ref{norm_A}) for the alternating minimization scheme (Algorithm \ref{alg_nmf})
 helps to
alleviate  many numerical difficulties, like numerical instabilities or ill-conditioning, however,
it makes searching for the global minimum  more complicated.
%From the normalized mixing matrix we cannot recover the true matrix,
%but in many applications the information on directions of the basis vectors in $\Real^I$ is %fully sufficient.
%In order to achieve uniqueness of NMF, we usually normalize
%column vectors of $\bA$ and/or rows vectors of $\bX$ to unit length.

Moreover, to avoid rotational ambiguity of NMF,  the rows of $\bX$  should be sparse
or zero-grounded. To achieve this we may apply some preprocessing,
sparsification, or filtering of the input data. For example, we may
 remove the baseline from the input data $\bY$ by applying the
affine NMF instead of the regular NMF, that is, \be \bY = \bA \bX + \ba_0
\1_T^T +\bE, \label{affineNMF} \ee where $\ba_0 \in \Real_+^{I}$ is
a vector selected in such a way that the unbiased matrix
$\hat \bY = \bY - \ba_0 \1_T^T \in \Real_+^{ I \times T}$ has
 many zeros or close to zero entries (see Chapter \ref{Ch3} for algorithms).

In summary, in order to obtain a unique NMF solution
(neglecting unavoidable permutation and scaling indeterminacies), we
need to enforce at least one of the following techniques:
%
\begin{enumerate}
  \setlength{\itemsep}{6pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
\item  Normalize or filter the input data $\bY$, especially by applying
the affine NMF model (\ref{affineNMF}),
in order to make the factorized matrices zero-grounded.

\item Normalize  the columns of $\bA$ and/or the rows of $\bX$ to unit length.

\item Impose sparsity and/or smoothness constraints to the factorized matrices.

\end{enumerate}

\subsection{Initialization of NMF}
\label{Sec_init}

The solution and convergence provided by NMF algorithms usually
  highly depends on initial conditions, i.e., its starting guess values,
especially in a multivariate context. Thus, it is important to have
efficient and consistent ways for initializing  matrices $\bA$ and/or
$\bX$.
%
In other words, the efficiency of many NMF strategies is affected by
the selection of the starting matrices. Poor initializations often
result in slow convergence, and in certain instances may lead even
to an incorrect or irrelevant solution. The problem of selecting
appropriate starting initialization matrices becomes even more
complicated for large-scale NMF problems and when certain structures or
constraints are imposed on the factorized matrices involved. As a good
initialization for one data set may be poor for another data set, to evaluate the
efficiency of an initialization strategy and the algorithm we should
perform uncertainty analysis such as Monte Carlo simulations.
Initialization in NMF plays a key role since the objective function
to be minimized may have many local minima, and the intrinsic alternating
minimization in NMF is nonconvex, even though the objective function is
strictly convex with respect to one set of variables. For example,
the quadratic function:
$$D_F(\bY||\bA\bX) = ||\bY - \bA\bX||_F^2$$ is strictly convex in one set of variables, either $\bA$ or $\bX$,
 but not in both.
%Moreover, due to computational complexity  most NMF algorithms are based on the gradient descent approach which
%assures only a local minimization. Thus, in many practical cases, especially when the prior knowledge about the solution
%is not very strong, the initialization considerably affects the estimated solution.
%
The issues of initialization in NMF have been widely discussed in the literature
\cite{ALS-Meyer},\cite{Kim_Choi_icassp_07},\cite{Langville06},\cite{Boutsidis08}.\\
%
As a rule of thumb, we can obtain  a \inx{robust  initialization} using the following steps:
%
\begin{enumerate}
  \setlength{\itemsep}{6pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}

    \item First, we built up a search method for generating $R$ initial matrices  $\bA$ and $\bX$.
    This could be based on random starts or the output from a simple ALS   NMF algorithm.  The parameter $R$ depends on
     the number of required iterations (typically, 10-20 is sufficient).
%
    \item Run  a specific  NMF algorithm for each set of initial matrices and with a fixed  but small
    number of iterations (typically, 10-20).
    As a result, the NMF algorithm provides $R$ initial estimates of the matrices $\bA^{(r)}$ and $\bX^{(r)}$.
%
   \item Select the estimates (denoted  by $\bA^{(r_{\min})}$ and $\bX^{(r_{\min})}$) corresponding to the lowest value of the cost function (the best likelihood) among the $R$ trials as initial values for the final factorization.
\end{enumerate}
%
In other words, the main idea  is to find good initial estimates (``candidates") with the following
\inx{multi-start initialization} algorithm:
%\vspace{0.8cm}

%\begin{algorithm}[H]
%\SetLine
%\caption{\bf Multi-start initialization}
%\KwData{$R$ number of restarts}
%\KwData{$K_{init}$ number of initial alternating steps}
%\KwData{$K_{fin}$ number of final alternating steps}
%\For{$r = 1, \ldots, R$} {
%    Initialize randomly  $\bA^{(0)}$ or $\bX^{(0)}$ \;
%    $\{ \bA^{(r)}, \bX^{(r)} \} \leftarrow {\rm nmf_{-}algorithm}(\bY,\bA^{(0)}, \bX^{(0)}, K_{init} )$\;
%    Compute $d_r = D(\bY||\bA^{(r)}\bX^{(r)})$ \;
%}
%$r_{min} = \arg \min_{1 \leq r \leq R} d_r$\;
%$\{ \bA, \bX \} \leftarrow {\rm nmf_{-}algorithm}(\bY,\bA^{(r_{min})}, \bX^{(r_{min})}, K_{fin})$\;
%\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Corrected on 9/2/2009 Huy Phan
\begin{algorithm}[Ht]
\caption{\bf{Multi-start initialization}\protect\inxx{algorithm,multi-start initialization}\protect\inxx{pseudo-code,multi-start initialization} \label{alg_nmf_multistart}}
\dontprintsemicolon
\KwIn{$\bY \in \Real_+^{I \times T} $: input data,
    \linebreak $J$: rank of approximation ,     $R$: number of restarts, \linebreak
    $K_{init}$, $K_{fin}$:  number of alternating steps for initialization and completion}
\KwOut{$\bA \in \Real_+^{I \times J}$ and $\bX \in \Real_+^{J \times T}$
    such that a given cost function is minimized.}
\SetKwFor{Parfor}{parfor}{do}{endfor}
\BlankLine
\Begin{
   \Parfor({\tcc*[f]{process in parallel mode}}){$r = 1$ to $R$}{
        Initialize randomly  $\bA^{(0)}$ or $\bX^{(0)}$ \;
        $\{ \bA^{(r)}, \bX^{(r)} \} \leftarrow {\rm nmf_{-}algorithm}(\bY,\bA^{(0)}, \bX^{(0)}, K_{init} )$\;
        $d_r = D(\bY||\bA^{(r)}\bX^{(r)})$ {\tcc*[f]{compute the cost value}}\;
    }
    $r_{min} = \arg \min_{1 \leq r \leq R} d_r$\;
    $\{ \bA, \bX \} \leftarrow {\rm nmf_{-}algorithm}(\bY,\bA^{(r_{min})}, \bX^{(r_{min})}, K_{fin})$\;
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\vspace{0.6cm}
%
%\  $\{ \bA^{(n)}, \bX^{(n)} \} \leftarrow {\rm nmf_{-}algorithm}(\bA^{(0)},
% \bX^{(0)}, K_{i} )$, \\
%{\rm  {\bf Compute}} $d_n = D(\bY||\bA^{(n)}\bX^{(n)} )$, \\
%{\rm  {\bf End} } \\
%\ {\rm  {\bf Find} } $n_{min} = \arg \min_{1 \leq n \leq N} d_n$, \\
%\ {\rm  {\bf Do} }  $\{ \bA, \bX \} \leftarrow {\rm nmf_{-}algorithm}(\bA^{(n_{min})}, \bX^{(n_{min})}, K_f)$, \\
%\end{algorithm}
%
%\begin{algorithm}
%\caption({\bf Multi-start initialization})
%\begin{algorithmic}[1]
%\State  $N$ {\rm (number of restarts)}, \\
%                   $K_{i}$ {\rm (number of initial alternating steps)},  $K_{f}$ {\rm (number of final alternating steps)}  \\
%{\rm  {\bf For} }  $n = 1, \ldots, N$ {\rm  {\bf do} }  \\
%\ {\rm  {\bf Initialize randomly} } $\bA^{(0)}$, $\bX^{(0)}$, \\
%\                    $\{ \bA^{(n)}, \bX^{(n)} \} \leftarrow {\rm nmf_{-}algorithm}(\bA^{(0)}, \bX^{(0)}, K_{i} )$, \\
%{\rm  {\bf Compute}} $d_n = D(\bY||\bA^{(n)}\bX^{(n)} )$, \\
%{\rm  {\bf End} } \\
%\ {\rm  {\bf Find} } $n_{min} = \arg \min_{1 \leq n \leq N} d_n$, \\
%\ {\rm  {\bf Do} }  $\{ \bA, \bX \} \leftarrow {\rm nmf_{-}algorithm}(\bA^{(n_{min})}, \bX^{(n_{min})}, K_f)$, \\
%\end{algorithm}
%
%\begin{algorithm}
%({\bf Multi-start initialization})
%{\rm  {\bf Set} }  $N$ {\rm (number of restarts)}, \\
%                   $K_{i}$ {\rm (number of initial alternating steps)},  $K_{f}$ {\rm (number of final alternating steps)}  \\
%{\rm  {\bf For} }  $n = 1, \ldots, N$ {\rm  {\bf do} }  \\
%\ {\rm  {\bf Initialize randomly} } $\bA^{(0)}$, $\bX^{(0)}$, \\
%\                    $\{ \bA^{(n)}, \bX^{(n)} \} \leftarrow {\rm nmf_{-}algorithm}(\bA^{(0)}, \bX^{(0)}, K_{i} )$, \\
%{\rm  {\bf Compute}} $d_n = D(\bY||\bA^{(n)}\bX^{(n)} )$, \\
%{\rm  {\bf End} } \\
%\ {\rm  {\bf Find} } $n_{min} = \arg \min_{1 \leq n \leq N} d_n$, \\
%\ {\rm  {\bf Do} }  $\{ \bA, \bX \} \leftarrow {\rm nmf_{-}algorithm}(\bA^{(n_{min})}, \bX^{(n_{min})}, K_f)$, \\
%\end{algorithm}
%

Thus, the multi-start initialization selects the initial estimates for $\bA$ and $\bX$ which give the steepest
decrease in the assumed objective function $D(\bY||\bA\bX)$ via alternating steps. Usually,
we choose the generalized Kullback-Leibler divergence $D_{KL}(\bY||\bA\bX)$ for checking the
convergence results after $K_{init}$  initial alternating steps.
The initial estimates $\bA^{(0)}$ and $\bX^{(0)}$ which give the lowest values of $D_{KL}(\bY||\bA\bX)$
after $K_{init}$
alternating steps are expected to be the most suitable candidates for continuing the alternating minimization.
In practice, for $K_{init} \geq  10$, the algorithm works quite efficiently.

Throughout this book, we shall  explore various alternative methods for the
efficient initialization of the iterative NMF  algorithms  and provide supporting
 pseudo-source codes and MATLAB codes; for example, we
 use extensively  the \inx{ALS-based initialization} technique as illustrated by the following MATLAB code:
%The ALS initialization is based on the NMF ALS algorithm with only one or two iteration.
%While the SVD initialization exploits the leading singular vectors of data matrix.
 %mentioned by {\tt lsv\_NMF} function.
%
\begin{lstlisting}[caption = {Basic initializations for NMF algorithms.}]
function [Ainit,Xinit] = NMFinitialization(Y,J,inittype)
% Y     :      nonnegative matrix
% J     :      number of components
% inittype     1 {random}, 2 {ALS}, 3 {SVD}
[I,T] = size(Y);
Ainit = rand(I,J);
Xinit = rand(J,T);

switch inittype
    case 2 % ALS
        Ainit = max(eps,(Y*Xinit')*pinv(Xinit*Xinit'));
        Xinit = max(eps,pinv(Ainit'*Ainit)*(Ainit'*Y));
    case 3  %SVD
        [Ainit,Xinit] = lsvNMF(Y,J);
end
Ainit = Ainit*bsxfun(@rdivide,Ainit,sum(Ainit));
end
\end{lstlisting}

\subsection{Stopping Criteria}

There are several possible stopping criteria for the iterative
algorithms used in NMF\inxx{stopping criteria}:

\begin{itemize}

 \item The cost function achieves a zero-value or a value below a given threshold $\varepsilon$, for example,
%
  \be
  D^{(k)}_{F}(\bY \; || \; \hat {\bf Y}^{(k)} )  -
  \left\| {\bf Y} - \hat {\bf Y}^{(k)}  \right\|_F^2  \le \varepsilon \,.
  \ee

\item There is little or no improvement between successive iterations in the minimization of a
cost function, for example,
%
 \be
 D^{(k+1)}_F (\hat {\bY}^{(k+1)} \;||\; \hat {\bf Y}^{(k)}) = \left\| \hat {\bf Y}^{(k)} - \hat {\bf Y}^{(k+1)}  \right\|_F^2  \le \varepsilon,
       \ee
       or
        \be
 \frac{|D^{(k)}_F  - D^{(k-1)|}_F|} {D^{(k)}_F}  \le  \varepsilon \,.
        \ee


\item There is little or no change in the updates for factor matrices $\bA$ and $\bX$.

\item The number of iterations achieves
or exceeds a predefined maximum number of iterations.

\end {itemize}
%
In practice,  the iterations usually continue until some combinations
of stopping conditions are satisfied. Some more advanced stopping criteria are discussed in Chapter \ref{Ch5}.

%In our practical implementations we applied the following stopping criteria.\\
%
%TODO
%
%Convergence evaluation for NMF algorithms is often based on the difference between two recent estimated matrices such as
%\begin{itemize}
%    \item Frobenius norm of cost function
%       \be
%            J^{F}_{{\hat {\bf Y}}^{(k)}}  = \left\| {\bf Y} - {\hat {\bf Y}}^{(k)}  \right\|_F^2  \le \varepsilon
%       \ee
%
%     \item Kullback-Leibler distance of cost function
%       \be
%       J^{KL}_{{\hat {\bf Y}}^{(k)}}  = \sum {\sum {{\bf Y}\log \left( \frac{\bf Y}{{\bf \hat Y}^{(k)}}\right)} }  - {\bf Y} + {\bf \hat Y}^{(k)}
%       \ee
%
%
%    \item Frobenius norm of estimated matrices
%       \be
%            J_{{\hat {\bf Y}}^{(k)}} = \left\| {\hat {\bf Y}}^{(k)} - {\hat {\bf Y}}^{(k+1)}  \right\|_F^2  \le \varepsilon
%       \ee
%    \item Ratio of distance
%        \be
%            J_{\hat {\bf Y}}  &= \frac{{J_{{\hat {\bf Y}}^{(k-1)} }  - J_{{\hat {\bf Y}}^{(k)} } }}{{J_{{\hat {\bf Y}}^{(k)} } }}  \le \varepsilon
%        \ee
%
%    \item Projected Gradient stopping criteria\\
%        These above stopping conditions do not reveal whether a solution is close to a stationary point or not. Chih-Jen Lin in \cite{Lin2007} pointed these disadvantages, and also suggested the new stopping condition to overcome them described as follows
%        \be
%            \left\| {\nabla ^P f\left( {{\bf A}^k ,{\bf X}^k } \right)} \right\|_F^2  \le \varepsilon \left\| {\nabla ^P f\left( {{\bf A}^1 ,{\bf X}^1 } \right)} \right\|_F^2 \label{equ_PG_criteria}
%        \ee
%        where the gradient function and the projected gradient function are defined as
%        \be
%           \nabla f\left( {{\bf A}^{(k)},{\bf X}^{(k)}} \right) = \left[ {\begin{array}{cc}
%           {\nabla _{{\bf A}^{(k)}} f\left( {{\bf A}^{(k)},{\bf X}^{(k)}} \right)}  \\
%           {\nabla _{{\bf X}^{(k)}} f\left( {{\bf A}^{(k)},{\bf X}^{(k)}} \right)}  \\
%           \end{array}} \right]= \left[ {\begin{array}{*{20}c}
%           {\nabla _{{\bf A}^{(k)} }}  \\
%           {\nabla _{{\bf X}^{(k)} }}  \\
%            \end{array}} \right]= \left[{\begin{array}{*{20}c}
%           \bA^{(k)}(\bX^{(k)} \bX^{(k)T}) - \bY \bX^{(k)T} \\
%           \bX^{(k)T} (\bA^{(k)T}  \bA^{(k)} )  - \bY^T  \bA^{(k)}  \\
%            \end{array}} \right] ,
%        \ee
%
%        \begin{numcases}{\nabla ^P f\left( x \right) = }
%        \nabla f\left( x \right)  & if $x > 0$ , \\
%        \min \left( {\nabla f\left( x \right),0} \right) & if $x = 0$.
%        \end{numcases}
%        Note that the projected gradient matrix has size of $(I+T) \times J$.
%        \item Geometric Mean of the Projected Gradient stopping criteria\\
%
%        This criteria is modified based on the PG criteria and allows stopping more quickly.
%        Apply Cauchy's inequality on every columns of the projected gradient matrices
%        $\nabla _{{\bf A}^{(k)} }$ and $\nabla _{{\bf X}^{(k)} }$, we obtain
%        \begin{align}
%        \hspace{-8mm}
%            \left\| {\nabla ^P f\left( {{\bf A}^k ,{\bf X}^k } \right)} \right\|_F^2 &=
%            \left\|  {\begin{array}{*{20}c}
%           {\nabla^P_{{\bf A}^{(k)} }}  \\
%           {\nabla^P_{{\bf X}^{(k)} }}  \\
%            \end{array}} \right\|^2_F
%            = \sum\limits_{j=1}^{J}{\left\| [\nabla^P_{{\bf A}^{(k)}} ]_j  \right\|_F^2
%             + \left\| [\nabla^P_{{\bf X}^{(k)} }]_j \right\|_F^2 } \notag\\
%            &\geq \sum\limits_{j=1}^{J}{2 \left\| [\nabla^P_{{\bf A}^{(k)}} ]_j  \right\|_F
%              \left\| [\nabla^P_{{\bf X}^{(k)} }]_j \right\|_F } \notag\\
%           & = 2
%           \left[ \begin{array}{ccc}
%             {\left\| {\left[ {\nabla _{{\bf A}^{(k)} }^P } \right]_1 } \right\|_F } &  \ldots  & {\left\| {\left[ {\nabla _{{\bf A}^{(k)} }^P } \right]_J } \right\|_F }
%           \end{array}
%          \right]
%          \left[ \begin{array}{ccc}
%                {\left\| {\left[ {\nabla _{{\bf X}^{(k)} }^P } \right]_1 } \right\|_F } &  \ldots  & {\left\| {\left[ {\nabla _{{\bf X}^{(k)} }^P } \right]_J } \right\|_F }  \\
%           \end{array}
%          \right]^T \notag\\
%          &=2 \;\; {}_{{\bf A}^k }\nabla _F^P  \;\; {}_{{\bf X}^k }\nabla _F^P.
%        \end{align}
%        where two vectors ${}_{{\bf A}^k }\nabla _F^P $ and ${}_{{\bf X}^k }\nabla _F^P$ contain Frobenius norms of all columns vectors of $\nabla _{{\bf A}^{(k)} }^P$ and $\nabla _{{\bf X}^{(k)} }^P$.
%        The equality holds if and only if
%        \be
%            \left\|\left[ \nabla _{{\bf A}^{(k)} }^P  \right]_j  \right\|_F   = \left\|\left[ \nabla_{{\bf X}^{(k)} }^P  \right]_j  \right\|_F \label{equ_NMFcriter_equal}.
%        \ee
%        Note that cause of scaling ambiguity in NMF, we are always able to select a diagonal matrix $\bD$
%        such that $\bA_d  = \bA \bD$ and $\bX_d  = \bD^{-1} \bX$
%
%        Calculate gradients of $\bA_d$ and $\bX_d$ , we have
%            \be
%                \nabla_{{\bA}_{d}} = {\bA}_d (\bX_d \bX_d^T) - \bY \bX_d^T = \bD^{-1} \left[ {\bA}_d (\bX \bX^T) - \bY  \bX^T \right] = \bD^{-1}\nabla_{\bA}
%            \ee
%        and
%            \be
%                \nabla_{{\bX}_{d}} = {\bX}^T_d (\bA_d^T \bA_d) - \bY^T \bA_d = \bD \left[{\bX}^T (\bA^T \bA) - \bY^T \bA \right] = \bD \;\nabla_{\bX}.
%            \ee
%        Basing on the definition of the projected gradient, it is easy to have
%        \be
%            \left\|\left[ \nabla _{{\bf A}_d} ^P  \right]_j  \right\|_F = 1/d_{jj} \left\|\left[ \nabla _{{\bf A} }^P  \right]_j  \right\|_F
%        \ee
%        and
%        \be
%            \left\|\left[ \nabla _{{\bf X}_d} ^P  \right]_j  \right\|_F = d_{jj} \left\|\left[ \nabla _{{\bf X} }^P  \right]_j  \right\|_F.
%        \ee
%        Scaling coefficients $d_{jj}$ in the main diagonal of matrix $\bD$ could be selected as follows
%        %        \ref{equ_NMFcriter_equal}
%        \be
%            1/d_{jj} \left\|\left[ \nabla _{{\bf A} }^P  \right]_j  \right\|_F = d_{jj} \left\|\left[ \nabla _{{\bf X} }^P  \right]_j  \right\|_F
%        \ee
%        or
%        \be
%            d_{jj} = \frac{\left\|\left[ \nabla _{{\bf A} }^P  \right]_j  \right\|_F }{\left\|\left[ \nabla _{{\bf X} }^P  \right]_j  \right\|_F}.
%        \ee
%        It means that diagonal of the scaling matrix $\bD$ is formed by component-wise division of two Frobenius norm vectors.
%        \be
%            \bD = diag\{{}_{{\bf A}^k }\nabla _F^P  \oslash {}_{{\bf X}^k }\nabla _F^P\}. \label{equ_GMPG_scalingvector}
%        \ee
%        Therefore, with every couple of matrices $\bA$ and $\bX$, we are always able to choose matrices $\bA_d$ and $\bX_d$ which minimize the projected gradient \ref{equ_PG_criteria} in sense of Frobenius norm.
%        Finally, the new stopping criteria referred to as the {\emph Geometric Mean of Projected Gradient} is stated as follows
%            \begin{enumerate}
%            \item Update matrices \bA and \bX by specified NMF algorithm.
%            \item Choose the scaling matrix $\bD$ (\ref{equ_GMPG_scalingvector}) and scale matrices $\bA$ and $\bX$ to get new ones $\bA_d$ and $\bX_d$.
%            \item Continue until satisfying the stopping condition (\ref{equ_PG_criteria}).
%            \end{enumerate}
%\end{itemize}
%
%%%
%{\bf Illustrative examples and MATLAB source code will be inserted here}
%
%%\be
%% {\nabla _{\bf A} } =  \bA(\bX \bX^T) - \bY \bX^T\\
%% {\nabla _{\bf X} } =  (\bA^T \bA) \bX - \bA^T \bY\\
%%\ee

\section{Tensor Properties and Basis of Tensor Algebra}

Matrix factorization models discussed in the previous sections
can be naturally extended and generalized to multi-way arrays,
also called  multi-dimensional matrices  or simply tensor
decompositions.{\footnote{The notion of tensors used in this
book should not  be confused with  field tensors used in physics and
differential geometry, which are generally referred to as tensor
fields (i.e., tensor-valued functions on manifolds) in mathematics
\cite{Kolda08}. Examples include, stress tensor, moment-of inertia
tensor, Einstein tensor, metric tensor, curvature tensor, Ricci
tensor.}}

\subsection{Tensors (Multi-way Arrays) -- Preliminaries}

A tensor is a \inx{multi-way array} or \inx{multi-dimensional matrix}.
The order of a tensor is the number of dimensions, also
known as ways or modes.
%
Tensor can be formally defined as\inxx{tensor definition}
%
\begin{definition}
{\bf (Tensor)} Let $I_1,I_2,\ldots,I_N \in \mathbb{N}$  denote index upper bounds. A tensor
$\underline \bY \in \Real^{I_1 \times I_2 \times \cdots \times I_N}$ of order $N$ is an $N$-way array  where elements $y_{i_1 i_2 \cdots i_n}$ are indexed by $i_n \in \{1,2,\ldots,I_n\}$   for $1 \leq  n \leq N$.
%
\end{definition}
%
%{\bf Definition 2.1 (Tensor)}. A tensor is a multi-way (multidimensional) array.
%The order of a tensor is the number of dimensions, also
%known as ways or modes.\\
%
Tensors are obviously generalizations of vectors and matrixes, for example, a third-order tensor (or three-way array) has three
modes\inxx{tensor modes} (or indices  or  dimensions) as shown in Figure  \ref{Fig3waytensor}.
 \begin{figure}
\centering
\psfrag{y}{\color{black}$y$}
\includegraphics[width=4.7cm,height=4.9cm]{cube_core1_c}
\caption{A three-way array (third-order tensor) $\underline \bY \in \Real^{7 \times 5 \times 8}$ with  elements $y_{itq}$.}
\label{Fig3waytensor}
\end{figure}
%
A zero-order tensor is a scalar, a first-order tensor is a vector, a
second-order tensor is a matrix, and tensors of order three and
higher are called higher-order tensors (see
Figure  \ref{Fig0-5waytensors}).

\begin{figure}[th]
%\includegraphics[scale = .5]{tensorDline}
\centering
\psfrag{y}[r][r]{\color{black}$y$}
\psfrag{I}[bc][bl]{\color{black}\footnotesize$I$}
\psfrag{T}{\color{black}\footnotesize$T$}
\psfrag{Q}{\color{black}\footnotesize$Q$}
\psfrag{N}{\color{black}\footnotesize$N$}
\psfrag{M}{\color{black}\footnotesize$M$}
\psfrag{1}{\color{black}\footnotesize1}
\psfrag{1N}{\color{black}\footnotesize1$N$}
\psfrag{m1}{\color{black}\footnotesize1}
\psfrag{L}{\color{black}\footnotesize1$L$}

\includegraphics[width=12.7cm,height=10.2cm]{mode1bis_c_fix3}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=3.9cm]{tensorD}
%
\caption{Illustration of multi-way data: zero-way  tensor = scalar,
     1-way tensor = row or column vector, 2-way tensor = matrix,
      $N$-way  tensor = higher-order tensors. The 4-way and 5-way tensors are represented here as a
      set of the three-way tensors.}
\label{Fig0-5waytensors}
\end{figure}

%
%A higher order tensor is a multidimensional or multi-way-way array.


%\subsection{Tensor and Matrix Notations}

%
% \begin{figure}
% \begin{center}
%\includegraphics[width=4.7cm,height=4.9cm]{cube_core1_c}\\
%\vspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{cube_fiber3w_c}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{cube_fiber2w_c}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{cube_fiber1w_c}\\
%\vspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{cube_slice3w_c}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{cube_slice2w_c}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{cube_slice1w_c}\\
%\end{center}
%\caption{Three-way array - third order tensor $\underline \bY =[y_{itq}] \in \Real^{I \times T \times Q}$ and illustration of fibers: Mode-1  fibers (columns), mode-2   fibers (rows), mode-3 fibers (tubes) and horizontal slices, lateral (vertical) slices and frontal slices.}
%\label{fig11-chap}
%\end{figure}

Generally, tensors  are denoted by an
underlined capital boldface letters, e.g., $\underline \bY \in
\Real^{I_{1} \times I_{2} \times \cdots \times I_{N}}$.  In contrast, matrices are denoted by boldface
capital letters, e.g., $\bY$; vectors are denoted by boldface
lowercase letters, e.g., columns of the matrix $\bA$ by $\ba_j$ and
scalars are denoted by lowercase letters, e.g., $a_{ij}$. The $i$-th
entry of a vector $\ba$  is denoted by $a_{i}$, and  the $(i, j)$-th
element of a matrix $\bA$ by $a_{ij}$. Analogously, the element $(i, t,
q)$  of a third-order tensor $\bY \in \Real^{I \times T \times Q}$
is denoted by $y_{itq}$. The values of indices are typically ranging from 1 to their
capital version, e.g.,
$i = 1,2, \ldots, I; \; t=1,2,\ldots, T; q=1,2, \ldots, Q$. %

\subsection{Subarrays, Tubes and Slices}

Subtensors\inxx{subtensors} or \inx{subarrays} are formed when a subset of the indices is fixed. For matrices, these
are the rows and columns. A colon is used to indicate all elements of a mode in the style of MATLAB. Thus,
the $j$-th column of a matrix $\bA = [\ba_1,\ba_2, \ldots, \ba_J]$ is  formally denoted by $\ba_{:\,j}$; likewise, the $j$-th row of  $\bX$ is denoted by $\underline {\bx}_{\,j}= \bx_{j:}$\inxx{tensor fibers}.
%
\begin{definition}
{\bf (Tensor Fiber)} A tensor fiber is a one-dimensional fragment
of a tensor, obtained by fixing all indices except for one.
\end{definition}
%
A matrix column is a mode-1 fiber and a
matrix row is a mode-2 fiber. Third-order tensors have column, row, and tube fibers,
denoted by  $\by_{:\,t\,q}, \by_{i\,:\,q}$, and $\by_{i\,t\,:}$, respectively (see Figure  \ref{Fig-fibers}).
Note that fibers are always assumed to be oriented as column vectors \cite{Kolda08}.
%
\begin{definition}
{\bf (Tensor Slice)} A tensor slice\inxx{tensor slices} is a two-dimensional section
(fragment) of a tensor, obtained by fixing all indices except for
two indices.
\end{definition}
%
%{\bf Definition 2.4 (Tensor Slice)}. A tensor slice is a
%two-dimensional section (fragment) of a tensor, obtained by fixing all
%indices but two indices.
%%Fig,\ref{Fig-slices} shows the horizontal, lateral, and frontal slices
%%of a third-order tensor
%%$\bY \in \Real^{I, \times T, \times Q}$, denoted by $\bY_{i::}, \; \bY_{:t:}$, and $\bY_{::q}$,
%%respectively.\\
%
\begin{figure}[hp]
\centering
%
\psfrag{y}{\color{black}$\by$}
\hfill\includegraphics[width=3.4cm]{cube_fiber3w_c}
\hfill\includegraphics[width=3.4cm]{cube_fiber2w_c}
\hfill
\psfrag{y13:}{\color{black}$\by_{13:}$}
\includegraphics[width=3.4cm,height=5.6cm]{cube_fiber1w_c_fix}
\caption{Fibers: for a third-order tensor $\underline \bY =[y_{itq}] \in \Real^{I \times T \times Q}$
(all fibers are treated as column vectors).}
\label{Fig-fibers}
\end{figure}
%
 \begin{figure}[hp]
\centering
%
\hfill\includegraphics[width=3.4cm]{cube_slice3w_c}
\hfill\includegraphics[width=3.4cm]{cube_slice2w_c}
\hfill\includegraphics[width=3.4cm]{cube_slice1w_c}
\caption{Slices for a third-order tensor $\underline \bY =[y_{itq}] \in \Real^{I \times T \times Q}$.}
\label{Fig-slices}
\end{figure}
%
\begin{figure}[hp]
\centering
%\includegraphics[scale = .5]{tensor1}
\hfill\includegraphics[width=4.0cm,height=4.6cm]{tensor2_c}
\hfill\includegraphics[width=4.0cm,height=4.6cm]{tensor04_c}
\psfrag{j}[bc][bc]{\color{black}\scriptsize $i$}
\psfrag{s}[bc][bc]{\color{black}\scriptsize $t$}
\psfrag{p}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-4pt}\hspace{3pt}\scriptsize $q$\end{tabular}}
\hfill\includegraphics[width=4.0cm,height=4.6cm]{tensor3_c_fix}
\caption{Illustration of subsets (subarrays) of a three-way tensor and basic tensor notations of tubes and slices.}
\label{Fig-tubslices}
\end{figure}
%
Figure  \ref{Fig-slices} shows the horizontal, lateral, and frontal slices
of a third-order tensor $\underline \bY \in \Real^{I \times T \times Q}$, denoted  respectively by
$\bY_{i\,:\,:} \, , \; \bY_{:\,t\,:}$  and $\bY_{:\,:\,q}$ (see also Figure  \ref{Fig-tubslices}).
%Generally, a colon is used to indicate all elements of a specific mode.
%
%
Two special subarrays have more compact representations: the $j$-th
column of matrix $\bA$, $\ba_{:\,j}$, may also be denoted as
$\ba_j$, whereas the $q$-th frontal slice of a third-order tensor,
$\bY_{:\,:\,q}$ may also be denoted as $\bY_q$, $(q=1,2, \ldots,Q)$.
 %
%
%\begin{figure}
%\begin{center}
%\includegraphics[scale = .5] {slice1}
%\includegraphics[scale = .5]{slice2}
%\includegraphics[scale = .5]{slice3}\\
%\includegraphics[scale = .4] {fiber1w}
%\includegraphics[scale = .4]{fiber2w}
%\includegraphics[scale = .4]{fiber3w}
%\end{center}
%\caption{slices: Frontal, Lateral, Horizontal}
%\label{fig1}
%\end{figure}
%
%
%\begin{figure}
%\begin{center}
%\includegraphics[scale = .4] {fiber1w}
%\includegraphics[scale = .4]{fiber2w}
%\includegraphics[scale = .4]{fiber3w}
%\caption{Fibers: Rows, Columns, tubes}
%\end{center}
%\label{fig2}
%\end{figure}
%
%%\begin{figure}
%%\includegraphics[scale = .4]{fiber2C}
%%\includegraphics[scale = .4]{fiber3C}
%%\caption{fiber1 i fiber2C i fiber3C}
%%\label{fig2}
%%\end{figure}
%
%  \begin{figure}[t]
%\begin{center}
%\includegraphics[width=3.7cm,height=5.9cm]{fiber1w}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{fiber2w}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{fiber3w}
%%
%\end{center}
%%\vspace{-0.5cm}
% \caption{Fibers of 3D tensor} \label{Fig3-chap1}
%\end{figure}
%
% \begin{figure}%[ht]
%\begin{center}
%\includegraphics[width=3.7cm,height=5.9cm]{slice1w}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{slice2w}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=5.9cm]{slice3w}
%%
%\end{center}
%%\vspace{-0.5cm}
% \caption{Slices (matrices) of 3D tensor} \label{Fig3-chap1}
%\end{figure}
%
%
% \clearpage

%\section{Basic Tensor Operations}


%
\subsection{Unfolding -- Matricization}

It is often very convenient to represent tensors as matrices or
to represent multi-way relationships and a tensor decomposition in
their matrix forms. Unfolding\inxx{tensor unfolding}, also known as matricization\inxx{tensor matricization} or flattening,
 is a process of reordering the elements of an $N$-th order tensor into
a matrix. There are various ways to order the fibers of tensors\inxx{tensor fibers}, therefore,
the unfolding process is not unique.
   Since the concept is easy to understand
by  examples, Figures  \ref{Fig-rowcolunf}, \ref{Fig-3modeunfold} and \ref{Fig-unfold-ex} illustrate the  various unfolding processes of a three-way array.
 For example, for a third-order tensor we can arrange frontal, horizontal and lateral  slices\inxx{tensor slices} in row-wise and column-wise ways.
 Generally speaking, the unfolding of an $N$-th order tensor can be understood
 as the process of the  construction of a matrix containing all the mode-$n$
vectors of the tensor. The order of the columns is not unique and in
this book it is chosen in accordance with \cite{Kolda08} and based on
 the following definition\inxx{unfolding}:
%
\begin{definition}{\bf (Unfolding)}
The mode-$n$ unfolding of tensor $\underline \bY \in
\Real^{I_{1} \times I_{2} \times \cdots \times I_{N}}$ is
denoted by\footnote{We use the Kolda - Bader notations \cite{Kolda08}.} $\bY_{(n)}$ and
arranges the mode-$n$ fibers into columns of a matrix.
More specifically, a tensor element $(i_1, i_2, \ldots, i_N)$ maps onto a
matrix element $(i_n, j)$, where
\be j=1+ \sum_{p \neq n}(i_p-1) J_p, \quad \mbox{with} \quad J_p= \begin{cases} 1,  & \mbox{if} \;\; p=1 \;\; \mbox{or  if} \;\; p=2 \;\; \mbox{and} \;\; n=1, \cr  \displaystyle \prod_{m \neq
n}^{p-1} I_m, & \mbox{otherwise.} \end{cases} \ee
\end{definition}
\begin{figure}[t!]
\includegraphics[width=13.1cm,height=6.1cm]{2way_c}
\caption{Illustration of  row-wise and column-wise unfolding (flattening, matricizing) of a third-order tensor.}
\label{Fig-rowcolunf}
\end{figure}
%
\begin{figure}[h]
\centering
\includegraphics[width=13.1cm,height=10.1cm]{unfoldAbis_c_fix}
\caption{Unfolding (matricizing) of a third-order tensor. The tensor can be unfolded in three ways to obtain matrices comprising its mode-1, mode-2 and mode-3 vectors.}
\label{Fig-3modeunfold}
\end{figure}
%
\begin{figure}[ht]
\centering
\psfrag{1}{\color{black}\tiny 1}
\psfrag{2}{\color{black}\tiny 2}
\psfrag{3}{\color{black}\tiny 3}
\psfrag{121}{\color{black} \tiny 121}
\psfrag{131}{\color{black} \tiny 131}
\psfrag{221}{\color{black} \tiny 221}
\psfrag{231}{\color{black} \tiny 231}
\psfrag{321}{\color{black} \tiny 321}
\psfrag{331}{\color{black} \tiny 331}
\psfrag{332}{\color{black} \tiny 332}
\psfrag{122}{\color{black} \tiny 122}
\psfrag{212}{\color{black} \tiny 212}
\psfrag{312}{\color{black} \tiny 312}
\psfrag{132}{\color{black} \tiny 132}
\psfrag{222}{\color{black} \tiny 222}
\psfrag{232}{\color{black} \tiny 232}
\psfrag{322}{\color{black} \tiny 322}
\psfrag{141}{\color{black} \tiny 141}
\psfrag{142}{\color{black} \tiny 142}
\psfrag{241}{\color{black} \tiny 241}
\psfrag{242}{\color{black} \tiny 242}
\psfrag{341}{\color{black} \tiny 341}
\psfrag{342}{\color{black} \tiny 342}

\psfrag{12}{\color{black} \tiny 12}
\psfrag{13}{\color{black} \tiny 13}
\psfrag{14}{\color{black} \tiny 14}
\psfrag{21}{\color{black} \tiny 21}
\psfrag{22}{\color{black} \tiny 22}
\psfrag{23}{\color{black} \tiny 23}
\psfrag{24}{\color{black} \tiny 24}
\psfrag{31}{\color{black} \tiny 31}
\psfrag{32}{\color{black} \tiny 32}
\psfrag{33}{\color{black} \tiny 33}
\psfrag{34}{\color{black} \tiny 34}

\psfrag{\0501\051}{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\hspace{-6pt}\scriptsize (1)\end{tabular}}
\psfrag{\0502\051}{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\hspace{-6pt}\scriptsize (2)\end{tabular}}
\psfrag{\0503\051}{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\hspace{-6pt}\scriptsize (3)\end{tabular}}
\psfrag{A}{\color{black} \large$\bA$}%
\psfrag{B}{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\hspace{-9pt}\large$\bA$\end{tabular}}

\includegraphics[width=13.1cm,height=11.1cm]{unfolding3_01_fix1}
\caption{Example of unfolding the third-order tensor in mode-1, mode-2 and mode-3.}
\label{Fig-unfold-ex}
\end{figure}
%
%
Observe that in the mode-$n$ unfolding the mode-$n$ fibers are
rearranged to be the columns of the matrix $\bY_{(n)}$.

More generally, a subtensor of the tensor $\underline \bY \in \Real^{I_{1} \times I_{2} \times \cdots \times I_{N}}$, denoted by $\underline \bY_{(i_n=j)}$, is obtained by
fixing the $n$-th index to some value $j$.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{\bf Examples and MATLAB source code need to be put here.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%{\bf Definition 1.3 (UNFOLDING)}.
%The mode-$n$ unfolding of the tensor $\underline \bY \in \Real^{I_{1} \times I_{2} \times \cdots \times I_{N}}$
%is denoted by $\bY_{n)}$  and arranges the mode-$n$ fibers to be the
%columns of the matrix. Specifically, tensor element $(i_1, i_2, \ldots, i_N)$
%maps to matrix element $(i_n, j)$, where
%\be
%j=1+ \sum_{p=1,p \neq n}(i_p-1)J_p, \qquad \mbox{with} \qquad J_p=\prod_{m=1,m \neq n}^{p-1} I_m.
%\ee
For example,  a third-order tensor $\underline \bY \in \Real^{ I_1 \times I_2 \times I_3}$ with entries
 $y_{i_1,i_2,i_3}$ and indices  $(i_1,i_2,i_3)$
has a corresponding position ($i_n,j$)  in the mode-$n$ unfolded matrix $\bY_{(n)}$ ($n = 1,2,3$)
as follows
\begin{itemize}
\item mode-1:  $j = i_2 + (i_3 -1 ) I_2$,

\item mode-2:  $j = i_1 + (i_3 -1 ) I_1$,

\item mode-3:  $j = i_1 + (i_2 -1 ) I_1$.
\end{itemize}
%
Note that mode-$n$ unfolding of a tensor $\underline \bY \in \Real^{I_1 \times I_2 \cdots \times I_N}$ also represents mode-1 unfolding of its permuted tensor $\underline {\tilde \bY} \in \Real^{I_n \times I_1 \cdots \times I_{n-1} \times I_{n+1} \cdots \times I_N}$ obtained by permuting its modes to obtain the  mode-1 be $I_n$.


\subsection{Vectorization}

It is often convenient to represent tensors and matrices as vectors, whereby
%
\inx{vectorization} of matrix $\bY =[\by_1,\by_2,\ldots, \by_T] \in \Real^{I \times T}$ is defined as
\be
\by=\mbox{vec} (\bY) = \left[\by_1^T,\by_2^T, \ldots, \by_T^T \right]^T \in \Real^{IT}.
\ee
%which is coded in MATLAB as
%\begin{lstlisting}
%% vectorize tensor as vector
%y = Y(:);
%\end{lstlisting}
The vec-operator applied on a matrix $\bY$ stacks its columns into a vector.
The \inx{reshape} is a reverse function to vectorization which converts a
vector to a matrix. For example, $\mbox{reshape}(\by, I,T) \in
\Real^{I \times T}$ is defined as (using MATLAB notations and
similar to the reshape MATLAB function): \be
\mbox{reshape}(\by,I,T)=\left[ \by(1:I),\by(I+1:2I), \ldots,
\by((T-1)I:IT) \right] \in \Real^{I \times T}. \ee
%
Analogously, we
define the vectorization of a tensor $\underline \bY$ as a
vectorization of the associated mode-1 unfolded matrix $\bY_{(1)}$.
For example, the vectorization of the third-order tensor $\underline
\bY \in \Real^{I \times T \times Q}$ can be written in the following
form \be \mbox{vec} (\underline \bY) = \mbox{vec} (\bY_{(1)}) =
\left[\mbox{vec}(\bY_{:\,:\,1})^T,\mbox{vec}(\bY_{:\,:\,2})^T,
\ldots, \mbox{vec}(\bY_{:\,:\,Q})^T \right]^T \in \Real^{ITQ}. \ee
%
Basic properties of the vec-operators include (assuming that matrices  are appropriate sizes):
\be
\mbox{vec}( c\; \bA) &=& c \; \mbox{vec}(\bA), \\
\mbox{vec}(\bA+\bB) &=& \mbox{vec}(\bA) + \mbox{vec}(\bB),\\
\mbox{vec}(\bA)^T \; \mbox{vec}(\bB) &=& \mbox{trace} (\bA^T \bB), \\
\mbox{vec}(\bA \bB \bC) &=& (\bC^T \otimes \bA) \mbox{vec} (\bB).
\ee


\subsection{Outer, Kronecker, Khatri-Rao and Hadamard Products}

Several special matrix products are important  for
representation of tensor factorizations and decompositions. % \cite{Kolda08}.


\subsubsection[Outer Product]{\bf Outer Product}

The outer product\inxx{outer product} of the tensors $\underline \bY \in \Real^{I_1 \times I_2 \times \cdots \times I_N}$  and
$\underline \bX \in \Real^{J_1 \times J_2 \times \cdots \times J_M}$
is given by
\be
\underline \bZ = \underline \bY \circ \underline \bX \in \Real^{I_1 \times I_2 \times \cdots \times I_N \times J_1 \times J_2 \times  \cdots \times J_M},
\ee
where
\be
z_{i_1,i_2,\ldots,i_N,j_1,j_2, \ldots,j_M}=y_{i_1,i_2,\ldots,i_N} \; x_{j_1,j_2, \ldots,j_M}.
\ee
Observe that, the tensor $\underline \bZ$ contains all the possible combinations
of pair-wise products between the elements  of $\underline \bY$ and $\underline \bX$.

% This operator is very closely related to the Kronecker
%product defined for matrices.

As special cases, the outer product of two vectors $\ba \in
\Real^I$ and $\bb \in \Real^J$ yields a rank-one matrix \be \bA=\ba
\circ \bb = \ba \bb^T \in \Real^{I \times J} \ee and the outer
product of  three vectors: $\ba \in \Real^I, \; \bb \in \Real^J$ and $
\bc \in \Real^Q$ yields a third-order rank-one tensor: \be
\underline \bZ = \ba \circ \bb \circ \bc \in \Real^{I \times J
\times Q}, \ee where \be z_{ijq} =a_i \; b_j \; c_q . \ee

\subsubsection[Kronecker Product]{\bf Kronecker Product }

The Kronecker product\inxx{Kronecker product}  of two matrices $\bA \in \Real^{I \times J}$ and $\bB \in \Real^{T \times R}$ is a matrix denoted as $\bA \otimes \bB \in \Real^{IT \times JR}$ and defined as  (see the  MATLAB function {\tt kron}):
\be
\bA \otimes \bB &= &\left[ {\begin{array}{*{20}c}
                       {a_{11} \; {\bf B}} & {a_{12} \; {\bf B}} & {\cdots} & {a_{1J} \; {\bf B}}  \\
                       {a_{21} \; {\bf B}} & {a_{22} \; {\bf B}} & {\cdots} & {a_{2J} \; {\bf B}}  \\
                       {\vdots} & {\vdots} & {\ddots} & {\vdots}  \\
                       {a_{I1} \; {\bf B}} & {a_{I2} \; {\bf B}} & {\cdots} & {a_{IJ} \;{\bf B}}  \\
                    \end{array}} \right]\\
                &=&   \left[ {\begin{array}{*{20}c}
                       {{\ba}_1 \otimes {\bb}_1} & {{\ba}_1 \otimes {\bb}_2} & {{\ba}_1 \otimes {\bb}_3} & {\cdots} & {{\ba}_J \otimes {\bb}_{R-1}} & {{\ba}_J \otimes {\bb}_R}  \\
                    \end{array}} \right].
\ee
 For any given three matrices $\bA, \bB$, and $\bC$ of (appropriate size), where $\bB$ and $\bC$ have the same size, the following properties hold:
\be
(\bA \otimes \bB)^T &=& \bA^T \otimes \bB^T, \\
(\bA \otimes \bB)^{\dag} &=& \bA^{\dag} \otimes \bB^{\dag} , \\
\bA \otimes(\bB +\bC) &=& (\bA \otimes \bB) + (\bA \otimes \bC),\\
(\bB +\bC)\otimes \bA  &=& (\bB \otimes \bA) + (\bC \otimes \bA),\\
(\bA \otimes \bB) (\bC \otimes \bD)  &=& \bA \bC  \otimes \bB  \bD, \\
c \; (\bA \otimes \bB) &=& (c \; \bA) \otimes \bB = \bA \otimes (c \; \bB).
\ee
%
It should be mentioned that, in general, the outer product  of vectors yields
  a tensor whereas the Kronecker product gives a vector. For
example, for the three vectors $\ba \in \Real^J, \; \bb \in \Real^T, \;
\bc \in \Real^Q$ their three-way outer product $\underline \bY = \ba \circ
\bb \circ \bc \in \Real^{I \times T \times Q}$ is a third-order
tensor with the entries $y_{itq} =a_j b_t c_q$, while the three-way
Kronecker product of the same vectors is a vector
$\mbox{vec}(\underline \bY) = \bc \otimes \bb \otimes \ba \in
\Real^{ITQ}$.

\subsubsection[Hadamard Product]{\bf Hadamard Product }

The Hadamard product\inxx{Hadamard product} of two equal-size matrices is the element-wise product denoted by $\*$ (or $.*$ for MATLAB notation)
 and defined as
\be
\bA \* \bB &= &\left[ {\begin{array}{*{20}c}
                       {a_{11} \; b_{11}} & \;\;{a_{12} \; b_{12}} &  \;\; {\cdots} &  \;\;{a_{1J}\; b_{1J} }  \\
                       {a_{21} \; b_{21} } & \;\;{a_{22} \; b_{22}} &  \;\; {\cdots} &  \;\;{a_{2J}\; b_{2J}}  \\
                       {\vdots} & \;\;{\vdots} &  \;\; {\ddots} &  \;\; {\vdots}  \\
                       {a_{I1} \; b_{I1}} & \;\; {a_{I2} \; b_{I2}} &  \;\; {\cdots} &  \;\;{a_{IJ} \; b_{IJ}}  \\
                    \end{array}} \right].
\ee

\subsubsection[Khatri-Rao Product]{\bf Khatri-Rao Product }

For two matrices $\bA =[\ba_1, \ba_2, \ldots, \ba_J] \in \Real^{I \times J}$ and $\bB =[\bb_1,\bb_2, \ldots, \bb_J] \in \Real^{T \times J}$ with the same number of columns $J$, their Khatri-Rao product, denoted by $\odot$, performs the following operation\inxx{Khatri-Rao product}:
\be
\bA \odot \bB &=& \left[\ba_1 \otimes \bb_1 \;\; \ba_2 \otimes \bb_2 \;  \cdots \; \ba_J \otimes \bb_J \right]\\[1ex]
&=& \left[\mbox{vec} (\bb_1 \ba^T_1) \;\; \mbox{vec} (\bb_2 \ba^T_2) \; \cdots \; \mbox{vec} (\bb_J \ba^T_J) \right] \in \Real^{IT \times J}.
\ee
%
The Khatri-Rao product is:
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
  \abovedisplayskip=-0ex
%\belowdisplayskip=3pt
\item associative \be \bA \odot (\bB \odot \bC)= (\bA \odot \bB)
\odot \bC, \ee

\item distributive \be (\bA + \bB) \odot \bC= \bA \odot \bC
+ \bB \odot \bC, \ee

\item non-commutative \be \bA \odot \bB \neq \bB
\odot \bA, \ee

\item its cross-product simplifies  into
  \abovedisplayskip=8pt
\be (\bA \odot \bB)^T \;
(\bA \odot \bB)= \bA^T \bA  \; \* \; \bB^T \bB, \ee
\item and the
Moore-Penrose pseudo-inverse  can be expressed as
%
  \abovedisplayskip=8pt
\begin{equationarray}{rcl@{\qquad}}
(\bA \odot \bB)^{\dag} &=& [(\bA \odot \bB)^T (\bA \odot \bB)]^{-1} (\bA \odot \bB)^T
= [(\bA^T  \bA )\; \* (\bB^T  \bB)]^{-1} (\bA \odot \bB)^T, \\[1ex]
( (\bA \odot \bB)^T )^{\dag} &=& (\bA \odot \bB) [(\bA^T  \bA) \; \* (\bB^T  \bB)]^{-1}.
%
\end{equationarray}

\end{itemize}
\subsection{Mode-$n$ Multiplication of Tensor by  Matrix and Tensor by Vector, Contracted Tensors Product}

To multiply\inxx{mode-$n$ multiplication} a tensor by a matrix\inxx{product of tensor and matrix}, we need to specify which mode
of the tensor is multiplied by the columns (or rows) of a matrix
(see Figure  \ref{Fig-n-mode-mul} and Table \ref{table-n-mode-mult}).
%
\begin{definition}
{\bf(mode-$n$ tensor matrix product)}
The mode-$n$  product $\underline \bY=\underline \bG \times_n \bA$ of a tensor
$\underline \bG \in \Real^{J_{1} \times J_{2} \times \cdots \times J_{N}}$ and
a matrix $\bA \in \Real^{I_n \times J_n}$ is  a tensor
$\underline \bY \in \Real^{J_1 \times \cdots \times J_{n-1} \times I_n \times J_{n+1} \times \cdots \times J_N}$, with elements
\be
%\nonumber \\
y_{j_1,j_2,\ldots,j_{n-1},i_n,j_{n+1},\ldots, j_{N}} =\sum_{j_n=1}^{J_n} g_{j_1,j_2,\ldots,J_N} \; a_{i_n,j_n}.
\ee
%
%
%\be
%\underline \bY = \underline \bG \times_n \bA \in \Real^{J_1 \times \cdots \times J_{n-1} \times I_n \times J_{n+1} \times \cdots \times J_N}.
%\ee
\end{definition}
%
\begin{figure}[p]%[ht!]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+11.7cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{X}[c][c]{\color{black} $\times$}
\psfrag{1}[c][r]{\color{black} \tiny 1}
\includegraphics[width=11.7cm,height=5cm, trim = 0 -.8cm 0 0 , clip = true]{AX1G}\label{Fig-n-mode-mul_a}}
\subfigure[]{
\psfrag{T}{\color{black}\tiny $T$}
\psfrag{X}[c][c]{\color{black} $\times$}
\psfrag{2}[c][r]{\color{black} \tiny 2}
\includegraphics[width=11.7cm,height=5.5cm,trim = 0 -.8cm 0 0 , clip = true]{BX2Gcor}\label{Fig-n-mode-mul_b}}
\setlength{\templength}{(-\textwidth+8.7cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{X}[c][c]{\color{black} $\times$}
\psfrag{3}[c][r]{\color{black} \tiny 3}
\includegraphics[width=8.7cm,height=5.5cm]{CX3Gcor}\label{Fig-n-mode-mul_c}}
\caption{Illustration of the mode-$n$ multiplications of a third-order tensor by matrices. \subref{Fig-n-mode-mul_a} mode-1 multiplication $\underline \bY_1= \underline \bG \, \times_1 \, \bA$, \subref{Fig-n-mode-mul_b} mode-2 multiplication $\underline \bY_2= \underline \bG \, \times_2 \, \bB$, \subref{Fig-n-mode-mul_c} mode-3 multiplication $\underline \bY_3= \underline \bG \, \times_3 \, \bC$.}
\label{Fig-n-mode-mul}
\end{figure}
%
The tensor-matrix product\inxx{product of tensor and matrix}
can be applied successively along several modes,
and it is commutative, that is
\be
(\underline \bG \times_n \bA) \times_m \bB = (\underline \bG \; \times_m \; \bB) \times_n \bA =
\underline \bG \times_n \bA \times_m \bB, \qquad (m \neq n).
\ee
%for $ m \neq n$.
%If matrices have the same dimension, then
The repeated (iterated) mode-$n$ tensor-matrix product for matrices $\bA$ and $\bB$ of appropriate dimensions can be simplified as
\be
(\underline \bG \times_n \bA) \times_n \bB = \underline \bG \; \times_n \; (\bB \bA).
\ee
%
For $\underline \bG \in \Real^{J_1 \times J_2 \times \cdots \times
J_N}$ and  a set of matrices $\bA^{(n)} \in \Real^{I_n \times J_n}$,  their
multiplication  in all possible modes
$(n=1,2,\ldots,N)$ is denoted as \be \underline \bG \times
\{\bA\} = \underline \bG \times_1 \bA^{(1)} \times_2 \bA^{(2)}
\cdots \times_N \bA^{(N)}, \ee and the  resulting tensor has dimension
$I_1 \times I_2 \times \cdots \times I_N$. Multiplication of
a tensor with all but one mode is denoted as \be \underline \bG
\times_{-n} \{\bA\} = \underline \bG \times_1 \bA^{(1)} \cdots
\times_{n-1} \bA ^{(n-1)} \times_{n+1} \bA^{(n+1)} \cdots \times_N
\bA^{(N)} \ee giving  a tensor of dimension $I_1 \times
\cdots \times I_{n-1} \; \times \; J_n \; \times I_{n+1} \; \times
\cdots \times I_N$.  The above notation is adopted from \cite{Kolda08}.

%\begin{lstlisting}
%function Y = prodtenmat(G,A,n)
%% The $n$-mode product of tensor and matrix: Y = G $\times_n$ A or Y$_{(n)}$ = A G$_{(n)}$
%
%Ia = size(A); Ig = size(G);
%Gn = unfolding(G,n);
%Y = permute(reshape(A*Gn,[Ia(1) Ig([1:n-1 n+1:end])]),[2:n 1 n+1:numel(Ig)]);
%\end{lstlisting}
%
It is not difficult to verify that these
 operations  satisfy the
following properties
%
 \be \left[
\underline \bG \times \{\bA\}\right]_{(n)} =
%\underline \bG \times_1 \bA^{(1)} \times_2 \bA^{(2)} \cdots \times_N \bA^{(N)},
\bA^{(n)} \bG_{(n)} \left[ \bA^{(N)} \otimes \bA^{(N-1)} \cdots \otimes \bA^{(n+1)} \otimes \bA^{(n-1)} \cdots \otimes \bA^{(1)} \right]^{T}.
\ee
%where $\otimes$ denotes Kronecker product.
\begin{definition}
{\bf (mode-$n$ tensor-vector product)}
 The mode-$n$ multiplication of a tensor \\
$\underline \bY \in \Real^{I_{1} \times I_{2} \times \cdots \times
I_{N}}$ by a vector $\ba \in \Real^{I_n}$ is denoted
by{\footnote{A bar over the operator $\times$ indicates a contracted
product.}} \be \underline \bY \; \bar{\times}_n \; \ba \ee  and has dimension
 $I_1 \times \cdots \times I_{n-1} \times  I_{n+1} \times
\cdots \times I_N$, that is,
%
 \be \underline \bZ = \underline \bY \;
\bar \times_n \; \ba \in \Real^{I_1 \times \cdots \times I_{n-1}
\times  I_{n+1} \times \cdots \times I_N}, \ee
%
Element-wise, we have \be
    z_{i_1,i_2,\ldots,i_{n-1},i_{n+1},\ldots,i_N} =\sum_{i_n=1}^{I_n} y_{i_1,i_2,\ldots,i_N} \; a_{i_n}.
\ee
\end{definition}
%Multiplication of tensor with sequence of vectors.\\
%
%Note that the dimension of the result is reduced by one. For example, multiplying
%a 3-way  (a third-order) tensor by a vector in mode-1 results
%in a 2-way tensor (a matrix)\inxx{product of tensor and vector}.

\begin{figure}[t]
\centering
\psfrag{G}[bc][bc]{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-0.5ex}
\hspace{-1ex} $\bG$\end{tabular}}
\psfrag{X}[c][c]{\color{black} $\bar{\times}$}
\psfrag{a}[bc][bc]{\color{black} $\ba$}
\psfrag{b}[bc][bc]{\color{black} $\bb$}
\psfrag{c}[bc][bc]{\color{black} $\bc$}
\psfrag{%}[c][c]{}
\psfrag{9}[l][r]{\color{black} \scriptsize 1}
\psfrag{2}[bc][tc]{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-2.5ex} \scriptsize 2\end{tabular}}
\psfrag{3}[bc][tc]{\color{black}\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-2.5ex} \scriptsize 3\end{tabular}}
\includegraphics[width=12.2cm,height=3.9cm]{GXabc}
\caption{Illustration of mode-$n$ multiplication of a third-order tensor $\underline \bG$ by vectors, yielding scalar
  $y = \underline \bG \; \bar \times_1 \; \ba \; \bar \times_2 \; \bb  \; \bar \times_3  \; \bc$. Note that the dimension of the result is reduced by one. For example, multiplying
a three-way  (a third-order) tensor by a vector in mode-1 results
in a 2-way tensor (a matrix).}
\label{Gxabc}
\end{figure}
%\begin{figure}[t]
%\centering
%\includegraphics[width=12.2cm,height=3.9cm]{GXabc}
%\caption{Illustration of mode-$n$ multiplication of a third-order tensor $\underline \bG$ by vectors, yielding scalar
%  $y = \underline \bG \; \bar \times_1 \; \ba \; \bar \times_2 \; \bb  \; \bar \times_3  \; \bc$.}
%\label{Gxabc}
%\end{figure}
%\vspace{-0.3cm}
%
\begin{table} [t]
\caption{Rules for the mode-$n$ multiplication of tensor $\underline \bG \in \Real^{J \times R \times P}$ with matrices $\bA \in \Real^{I \times J}, \;  \bB \in \Real^{T \times R}$, and  $ \bC \in \Real^{Q \times P}$ and with vectors: $\ba \in \Real^{J}, \; \bb \in \Real^{R}$ and $\bc \in \Real^{P}$.}
\renewcommand{\arraystretch}{2.0}
\setlength{\tabcolsep}{0.2mm}
    \begin{center}
    \shadingbox{
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}l@{\ \ \ \ \  }l@{\ \ \ \ \  }l} \hline
 Mode-$n$ product &
   Matricized version & Vectorized version \\ \hline
    $\underline \bY = \underline \bG \times_1 \bA \in \Real^{I \times R \times P} $ & $\bY_{(1)} = \bA \bG_{(1)}$
    & $\text{vec}(\bY_{(1)}) = (\bI \otimes \bA)
     \text{vec}(\bG_{(1)})$ \\
    $y_{irp} = \displaystyle\sum\limits_{j=1}^{J}{g_{jrp} \; a_{ij}}$ && \\ \hline
    $\underline \bY = \underline \bG \times_2 \bB \in \Real^{J \times T \times P}$
     & $\bY_{(2)} = \bB \bG_{(2)}$
     & $\text{vec}(\bY_{(2)}) = (\bI \otimes \bB) \text{vec}(\bG_{(2)})$ \\
    $y_{jtp} = \displaystyle\sum\limits_{r=1}^{R}{g_{jrp} \; b_{tr}}$ && \\ \hline
    $\underline \bY = \underline \bG \times_3 \bC \in \Real^{J \times R \times Q}$
    & $\bY_{(3)} = \bC \bG_{(3)}$
    & $\text{vec}(\bY_{(3)}) = (\bI \otimes \bC) \text{vec}(\bG_{(3)})$ \\
    $y_{jrq} = \displaystyle\sum\limits_{p=1}^{P}{g_{jrp} \; c_{qp}}$ && \\ \hline
    $\underline \bY = \underline \bG \;\bar \times_1 \;\ba \in \Real^{R \times P}$
    & $\bY_{(1)} = \ba^T \bG_{(1)}$
     & $\text{vec}(\bY_{(1)}) = (\bI \otimes \ba^T) \text{vec}(\bG_{(1)})$ \\
    $y_{rp} = \displaystyle\sum\limits_{j=1}^{J}{g_{jrp} \; a_j}$    &
    & $\text{vec}(\bY_{(1)}) = \bG^T_{(1)} \; \ba$ \\\hline
    %
     $\underline \bY = \underline \bG \;\bar \times_2 \;\bb \in \Real^{J \times P}$
     & $\bY_{(2)} = \bb^T \bG_{(2)}$
      & $\text{vec}(\bY_{(2)}) = (\bI \otimes \bb^T) \text{vec}(\bG_{(2)})$ \\

    $y_{jp} = \displaystyle\sum\limits_{r=1}^{R}{g_{jrp} \; b_r}$    &
    & $\text{vec}(\bY_{(2)}) = \bG^T_{(2)} \; \bb$ \\ \hline
    %
    $\underline \bY = \underline \bG \;\bar \times_3 \;\bc \in \Real^{J \times R}$
    & $\bY_{(3)} = \bc^T \bG_{(3)}$
    & $\text{vec}(\bY_{(3)}) = (\bI \otimes \bc^T) \text{vec}(\bG_{(3)})$ \\
    $y_{jp} = \displaystyle\sum\limits_{p=1}^{P}{g_{jrp} \; c_p}$    &
     & $\text{vec}(\bY_{(3)}) = \bG^T_{(3)} \; \bc$ \\ \hline
    \end{tabular*}
    }
    \end{center}
    \label{table-n-mode-mult}
\end{table}

\vspace{-0.3cm}
\noindent
 It is also possible to multiply a
tensor by a vector in more than one mode. Multiplying
a three-way tensor by vectors in the two modes results in a
1-way tensor (a vector); multiplying it in all modes results
in a scalar.
We can exchange the order of multiplication by the following rule:
\be
\underline \bY \; \bar \times_m \; \ba \; \bar \times_n \;\bb
= (\underline \bY \; \bar \times_m \; \ba)  \; \bar \times_{n} \; \bb %\nonumber \\
=  (\underline \bY \; \bar \times_n \; \bb)  \; \bar \times_{m} \;
\ba, \qquad \mbox{for} \;\; m<n. \ee For example,  the mode-$n$
multiplication of a tensor $\underline \bG \in \Real^{J \times R
\times P}$ by  vectors $\ba \in \Real^J, \; \bb \in \Real^R$ and $\bc
\in \Real^P$ can be expressed as (see Figure  \ref{Gxabc} and Table \ref{table-n-mode-mult})
%
\be z= \underline \bG \;\bar
\times_1 \; \ba  \;\bar \times_2 \; \bb \; \bar \times_3 \; \bc =
\sum_{j=1}^J \sum_{r=1}^R \sum_{p=1}^P g_{jrp} \; a_j \; b_r \; c_p.
\nonumber \ee
%
More generally,  for $\underline \bG \in \Real^{J_1 \times J_2
\times \cdots \times J_N}$ and $\ba^{(n)} \in \Real^{J_n}$, the
multiplication by all vectors in all modes $(n=1,2,\ldots,N)$
gives a scalar:
%
\be y=  \underline \bG \; \bar \times_1 \;
\ba^{(1)} \; \bar \times_2 \; \ba^{(2)} \; \cdots \bar \times_N \;
\ba^{(N)} = \underline \bG \; \bar \times \; \{\ba\} \in \Real \ee
whereas multiplication in every mode except mode-$n$  results in a vector
$\bx$ of length $J_n$:
\be \bx &=&  \underline \bG \; \bar \times_1 \;
\ba^{(1)}\; \cdots \bar \times_{n-1} \; \ba^{(n-1)} \; \bar
\times_{n+1} \; \ba^{(n+1)} \; \cdots \; \bar \times_N \; \ba^{(N)} \nonumber \\
&=& \bG_{(n)} \left(\ba^{(N)} \otimes \cdots \otimes
\ba^{(n+1)} \otimes \ba^{(n-1)}
  \otimes \cdots  \otimes  \ba^{(1)} \right)
= \underline \bG \; \bar \times_{-n} \; \{\ba\} \in \Real^{J_n}.\ee
%
Also note that multiplication in every mode except mode-$n$ and mode-$m$, results in a matrix of size $J_n \times J_m$.

%% HUY PHAN 14042009
A matrix $\bG \, (I \times J)$ can be considered as a third-order tensor $\underline \bG$
in which the 3rd dimension is 1 $(I \times J \times 1)$, and its matricized versions in each mode are given by
\be
	\left[ \underline \bG\right]_{(1)} &=& \left[ \underline \bG\right]_{(2)}^T = \bG, \\
	\left[ \underline \bG\right]_{(3)} &=& \vtr{\bG}^T.
\ee
The mode-3 product of the tensor $\underline \bG$ with a vector $\ba$
is exactly the outer product of $\bG$ and $\ba$.
\be
	 \underline \bG  \times_3  \ba =   \underline \bG \circ \ba.
\ee
%%
%% HUY PHAN 14042009

\begin{definition}{\bf The scalar product} (or inner product) of two tensors
 $\underline \bA, \underline \bB \in \Real^{I_1 \times I_2, \times \cdots \times I_N}$ of the same order
is denoted by  $\left< \underline \bA, \underline \bB \right>$ and is computed as a sum of  element-wise products over all the indices, that is,
\be
c = \left<\underline \bA, \underline \bB \right> =
 \sum_{i_1}^{I_1} \sum_{i_2}^{I_2} \cdots \sum_{i_N}^{I_N}b_{i_1,i_2, \ldots, i_N} a_{i_1,i_2, \ldots, i_N} \in \Real .
\ee
\end{definition}
%
The \inx{scalar product} allows us to define the higher-order
Frobenius norm of a tensor $\underline \bA$ as
\be
||\underline \bA||_F = \sqrt{\left< \underline \bA,\underline \bA \right>}
= \sqrt{\sum_{i_1}^{I_1} \sum_{i_2}^{I_2} \cdots \sum_{i_N}^{I_N} a^2_{i_1,i_2, \ldots, i_N}},
\ee
whereas the $\ell_1$-norm of a tensor is defined as
\be
||\underline \bA||_1
=  \sum_{i_1}^{I_1} \sum_{i_2}^{I_2} \cdots \sum_{i_N}^{I_N} |a_{i_1,i_2, \ldots, i_N}|.
\ee


\begin{definition}{\bf The contracted product} of two tensors
 $\underline \bA \in \Real^{I_1 \times \cdots \times I_{M} \times J_1 \times \cdots \times J_N}$ and \linebreak
  $\underline \bB \in \Real^{I_1 \times \cdots \times I_{M} \times K_1 \times \cdots \times K_P}$  along the first $M$ modes is a tensor of size $J_1 \times \cdots \times J_N \times K_1 \times \cdots \times K_P$, given by
 \be
{\langle \underline \bA, \underline\bB \rangle}_{1,\ldots,M;1,\ldots,M} ( j_1, \ldots, j_N, k_1,\ldots , k_P )
= \sum_{i_1=1}^{I_1} \cdots \sum_{i_M=1}^{I_M}
a_{i_1, \ldots , i_M, j_1, \ldots, j_N}\; b_{i_1, \ldots , i_M, k_1, \ldots, k_P }.
 \ee
\end{definition}
The remaining modes are ordered such that
those from $\underline \bA$ come before $\underline \bB$.
The arguments specifying the modes of $\underline \bA$ and those of $\underline \bB$ for contraction need not be consecutive. However,
the sizes of the corresponding dimensions must be equal. For example, the contracted tensor product
along the mode-2 of a tensor $\underline \bA \in \Real^{3 \times 4 \times 5}$,
and the mode-3 of a tensor $\underline \bB \in \Real^{7 \times 8 \times 4}$ returns a tensor
$	\underline \bC = {\langle \underline \bA, \underline\bB \rangle}_{2;3} \in \Real^{3 \times 5 \times 7 \times 8}$.

The contracted tensor product of $\underline \bA$ and $\underline \bB$ along  the same $M$ modes simplifies to
\be
	{\langle \underline \bA, \underline\bB \rangle}_{1,\ldots,M;1,\ldots,M} = {\langle \underline \bA, \underline\bB \rangle}_{1,\ldots,M}\, ,
\ee
whereas the contracted product of tensors $\underline \bA \in \Real^{I_1 \times \cdots \times I_N}$ and $\underline \bB \in \Real^{J_1 \times \cdots \times J_N}$ along  all modes except the mode-$n$ is denoted as
\be
	 {\langle \underline \bA, \underline\bB \rangle}_{-n} = \bA_{(n)} \, \bB_{(n)}^T \in \Real^{I_n \times J_n}, \qquad (I_k = J_k, \;\; \forall k\neq n ).
\ee
%
The tensor-vector, tensor-matrix and scalar multiplications can be expressed in a form of  contracted product. For example, the contracted product along the mode-$n$ of the tensor $\underline \bA$ and the mode-2 of matrix $\bC \in \Real^{J \times I_n}$  can be obtained by permuting  the dimensions of the mode-$n$ product of $\underline \bA$ and $\bC$
\be
	 {\langle \underline \bA, \bC \rangle}_{n;2} = {\langle \underline \bA, \bC^T \rangle}_{n;1} &=& {\tt{permute}}(\underline \bA \times_n \bC, [1,\ldots,n-1, n+1, \ldots,N, n ] ).
\ee
We also have
\be
	 {\langle \bC, \underline \bA \rangle}_{2;n} = {\langle \bC^T, \underline \bA \rangle}_{1;n} &=& {\tt{permute}}(\underline \bA \times_n \bC, [n , 1,\ldots,n-1, n+1, \ldots,N] ) .
\ee
For two tensors of the same dimension, their contracted product along all their modes is their inner product
\be
	 {\langle \underline \bA, \underline\bB \rangle}_{1,\ldots,N} &=&  {\langle \underline \bA, \underline\bB \rangle}.
\ee
In a special case of $M = 0$, the contracted product becomes the outer product of two tensors.

%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%\includegraphics[width=3.7cm,height=2.9cm]{mode1}
%\hspace{0.5cm}
%\includegraphics[width=3.7cm,height=2.9cm]{mode2}
%\hspace{0.5cm}
%\includegraphics[width=3.7cm,height=2.9cm]{mode3}
%\end{center}
%\caption{Multiplication of tensor by the matrix}
%\label{fig1}
%\end{figure}
%
%We use tensor notations and operations that are  consistent with the following references \cite{Kolda08}.\\
%
%{\bf The $n$-mode product} of a tensor $\underline \bG \in \Real^{J_1,J_2, \ldots, J_M} $ and
%a matrix $\bA \in \Real^{I_n \times J_n}$ along the $n$th mode is denoted as
%
%It may be visualized by multiplying all $n$-mode vectors of
%from the left-hand side by the matrix.
%In MATLAB, {\tt norm} function helps us to compute these two norms.
%\begin{lstlisting}
%% vectorize tensor as vector
%norm1 = norm(Y(:),1);
%norm2 = norm(Y(:),2);
%\end{lstlisting}



\subsection{Special Forms of Tensors}

Tensors can take special forms or structures.  For instance, often a tensor is sparse or symmetric.

\subsubsection[Rank-One Tensor]{\bf Rank-One Tensor }

Using the outer product, the rank of tensor can be defined as follows\inxx{rank-one tensor} (see Figure 1.23)
%
\begin{definition} {\bf(Rank-one tensor)} A tensor $\underline \bY \in \Real^{I_1 \times I_2 \times \cdots \times I_N}$ of order $N$ has rank-one if it can be written as an outer product of $N$ vectors  i.e.,
\be
\underline \bY = \ba^{(1)} \circ \ba^{(2)} \circ \cdots \circ \ba^{(N)},
\ee
where $\ba^{(n)} \in \Real^{I_n}$ and $y_{i_1,i_2,\ldots,i_N}= a_{i_1}^{(1)}  a_{i_2}^{(2)} \cdots a_{i_N}^{(N)}$. The rank of a tensor $\underline \bY \in \Real^{I_1 \times I_2 \times \cdots \times I_N}$
is defined as the minimal number of rank-one tensors $\underline \bY_{\; 1}, \ldots, \underline \bY_{\; R}$  such that $\underline \bY =\sum_{r=1}^R \underline \bY_{\; r}$.
\end{definition}

\begin{figure}[t!]
%\includegraphics[scale = .5]{tensorDline}
\centering
\psfrag{\)}{}
\psfrag{\(}{}
\psfrag{x}{}
\psfrag{Y}{\color{black}\large$\underline \bY$}
\psfrag{I}{}
\psfrag{T}[bc][bc]{\color{black}\footnotesize$(I \times T \times Q)$}
\psfrag{Q}{}
\psfrag{a}{\color{black}$\ba$}
\psfrag{b}{\color{black}$\bb$}
\psfrag{c}{\color{black}$\bc$}

\includegraphics[width=6.7cm,height=3.2cm]{Rank1_c_fix}
\caption{Rank-one third-order tensor: $\underline \bY = \ba \circ \bb \circ \bc \in  \Real^{I \times T \times Q}, \; \ba \in \Real^I,\; \bb \in \Real^T,\;\bc \in \Real^Q$.}
\label{fig-rank1}
\end{figure}

This outer product is often computed via the Khatri-Rao product or the Kronecker product based on  the following relation
\be
    \text{vec}(\underline \bY) = \text{vec}(\bY_{(1)}) =
    \text{vec}(\ba^{(1)}  (\ba^{(N)} \odot \cdots \odot \ba^{(2)})^T)
        = \ba^{(N)} \odot \cdots \odot \ba^{(2)}\odot \ba^{(1)}.
\ee
Rank-one tensors have many interesting properties and play an
important role in multi-way analysis
\cite{Zhang01},\cite{Wang04},\cite{Wang-Ahuja-08},\cite{CIAPHAN_08_MLSP_NTF},\cite{Hazan},\cite{Lim-Comon},\cite{Phan-HALS-IEICE},\cite{PHANCIA_08_ISNN}. In general, rank of a higher-order
tensor is defined as the minimal number of rank-one tensors whose linear combination
 yields $\underline \bY$. Such a
representation of a tensor by a linear combination of rank-one
tensors is just a CANonical DECOMPposition (CANDECOMP)\inxx{CANDECOMP} or PARAFAC\inxx{PARAFAC}
(PARAllel FACtor decomposition) which preserves the uniqueness under some
mild conditions \cite{Krus1989a}.

\subsubsection[Symmetric and Super-Symmetric Tensors]{\bf Symmetric and Super-Symmetric Tensors }

For the  particular case when all the $N$ vectors $\ba^{(j)}$ are  equal to a vector $\bg$,
 their outer product is called  a supersymmetric rank-one
tensor.{\footnote{In general, by analogy to symmetric matrices a
higher-order tensor is called supersymmetric if its entries are
invariant under any permutation of their indices.}} A
super-symmetric tensor has  the
same dimension\inxx{super-symmetric tensor} in every mode.
%
%%CCC
%\begin{figure}
%%\includegraphics[scale = .5]{tensorDline}
%\begin{center}
%\includegraphics[width=3.7cm,height=4.4cm]{tensorI_c}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=3.9cm]{tensorline_c}
%\hspace{1cm}
%\includegraphics[width=3.7cm,height=3.9cm]{tensorD}
%%
%\end{center}
%\caption{Special form of third-order tensors: Super-Identity cube
%tensor, diagonal tensor with diagonally positioned nonzero matrix,
%block diagonal tensor.}
%\label{Fig-diagtensors}
%\end{figure}
%
\begin{figure}
%\begin{center}
\hfill\subfigure[tight][]{\includegraphics[width=2.4cm,height=2.4cm]{tensorI_c}\label{spectensor1}}
\hfill\subfigure[tight][]{\includegraphics[width=2.4cm,height=2.2cm] {tensorline_c}\label{spectensor2}}
\hfill\subfigure[tight][]{\includegraphics[width=2.4cm,height=2.2cm] {tensorD}\label{spectensor3}}
%\end{center}
\caption{Special forms of third-order tensors: \subref{spectensor1} Super-Identity cube
tensor $\underline \bI$, \subref{spectensor2} sparse tensor with diagonal frontal slices, which can be mathematically expressed as $\underline \bI \times_3 \bC$,
and \subref{spectensor3} block diagonal tensor.}
\label{Fig-diagtensors}
\end{figure}

Tensors can also only be (partially) symmetric in two or more modes. For example,
a three-way tensor $\underline \bY \in \Real^{I \times I \times Q}$ is symmetric in modes one and two if all its frontal
slices are symmetric, i.e., $\bY_q=\bY_q^T, \forall q=1,2, \ldots,Q.$\\

\subsubsection[Diagonal Tensors]{\bf Diagonal Tensors }

An $N$-th order cubical tensor $\underline \bY \in \Real^{I_1 \times I_2 \times \cdots \times I_N}$ is diagonal if its elements
$y_{i_1,i_2, \ldots, i_N} \neq 0$ only if $i_1 = i_2 = \cdots = i_N$ (see Figure  \ref{spectensor1}).
We use $\underline \bI$ to denote the  cubical identity tensor with ones on the superdiagonal and zeros elsewhere. This concept can be generalized or extended as illustrated in  Figures  \ref{spectensor2} and \ref{spectensor3}.


\section{Tensor Decompositions and Factorizations}

Many modern applications generate large amounts of data with

multiple aspects and high dimensionality for which tensors (i.e., multi-way arrays) provide a
natural representation. These include text mining, clustering, Internet
traffic, telecommunication records, and large-scale social
networks.

Tensor decompositions  and factorizations were initiated by
Hitchcock in 1927 \cite{Hitchcock1927},
and later developed by Cattelin in 1944 \cite{Kolda08} and by Tucker in 1966 \cite{tucker64extension},\cite{Tucker66}.
These concepts and approaches received more attention after Carroll and
Chang \cite{Carroll_Chang},\cite{Carr1989} proposed the
Canonical Decomposition (CANDECOMP) and  independently Harshman \cite{Harshman},\cite{Harsman72},\cite{Harsman03}
proposed an equivalent model called the PARAFAC (Parallel Factor Analysis) in 1970.

M\"{o}ck  rediscovered the PARAFAC when tackling a
neuroscience problem of  event related potentials (ERP) in the context of
brain imaging \cite{Moecks}. These  foundations for tensor factorizations and decompositions   also
include results on the  uniqueness  of tensor factorizations and
some recommendations on how to choose the number of components. The
subsequent contributions  put  M\"{o}ck's results in the framework
proposed by Harshman \cite{Harshman}, Kruskal  \cite{kruskal77} and Carroll and Chang
\cite{Carroll_Chang}.

Most of the early results devoted to tensor factorizations and decompositions
 appeared in the  psychometrics literature.
Appellof, Davidson   and Bro  are  credited as being the first to
use tensor decompositions (in 1981-1998) in chemometrics, which have
since become extremely popular in that field (see, e.g.,
\cite{Appellof},\cite{bro_book},\cite{Bro1997},\cite{Bro_Andersson},\cite{Acar2008},\cite{Kolda08}).
 In parallel with the developments in psychometrics
and chemometrics, there was a great deal of interest in decompositions of bilinear
forms in the field of algebraic complexity \cite{Kolda08},\cite{Kim-Elden-Park}.

Although some tensor decomposition models have been proposed long time ago, they have recently
attracted the interest  of researchers working
in mathematics, signal processing, data mining,  and
neuroscience. This probably explains why  available mathematical theory
seldom deals  with the computational  and algorithmic aspects of
tensor decompositions, together with many still  unsolved fundamental problems.
%PARAFAC and Tucker are often considered as a generalizations of SVD (Singular Value
%Decomposition) to higher order tensors. When n = 2, PARAFAC = Tucker = SVD. When n > 2,
%PARAFAC and Tucker are two different ways to generalize SVD to higher order tensors.

%Decompositions of higher-order tensors (i.e., $N$-way arrays with $N
%\geq3$) have applications in psychometrics, chemometrics, signal
%processing, numerical linear algebra, computer vision, numerical
%analysis, data mining, neuroscience, graph analysis, etc.


Higher-order tensor decompositions  are nowadays frequently used  in a
variety of fields including psychometrics, chemometrics, image
analysis, graph analysis, and signal processing. Two of the
most commonly used decompositions are the Tucker decomposition and PARAFAC (also
known as CANDECOMP or simply CP\inxx{CP model}) which are often considered  (thought of)  as
higher-order generalizations of the matrix singular value
decomposition (SVD) or principal component analysis (PCA). In this book, we superimpose
different constraints such as
nonnegativity, sparsity or smoothness,  and generally such an analogy is
no longer valid.


%Both CANDECOMP/PARAFAC (CP)  and Tucker  tensor decompositions are
%usually  considered as higher-order generalizations of the matrix
%singular value decomposition (SVD) and principal component analysis
%(PCA).

%In the last ten years, the interest in tensor decompositions has been
%expanded to other fields. Examples include signal processing,
%numerical linear algebra, computer vision, numerical analysis, data
%mining, graph analysis, neuroscience and more. Several surveys have
%already been written in other fields.  Moreover, there are several
%software packages for working with tensors.

 In this chapter we formulate the models and problems for three-way arrays.
 Extension for arbitrary $N$-th order tensors will be given in Chapter \ref{Ch7}.


\subsection{Why Multi-way Array Decompositions and Factorizations?}

Standard matrix factorizations, such as PCA/SVD, ICA,  NMF, and their
variants, are invaluable tools for feature selection, dimensionality
reduction, noise reduction, and data mining \cite{CiAm02}. However, they have
only two modes or 2-way representations (say, space and time),
and their use is therefore  limited.
In many applications the data structures often contain
higher-order ways (modes) such as trials, task conditions, subjects,
and groups together with  the intrinsic dimensions of space, time,
and frequency. For instance, a sequence of trials  may lead to a large
 stream of data encompassing many dimensions: space,
time-frequency, subjects, trials, and conditions \cite{Andersen},\cite{ICDM08}.

%While multivariate bilinear methods such as PCA/SVD, ICA, and NMF
%have been used successfully for extracting the information about spatial
%and temporal features for real-world data, the need to unfold
%higher-order multi-way array (tensor) data sets into bilinear
%arrays, which leads decompositions that are often non-unique and to
%the loss of multi-way linkages and interactions present in the data.
%In other words, if the 2-way decomposition approaches are to be used
%for multi-way array data, tensors have to be first converted into
%matrices by unfolding several modalities. However, such unfolding
%may cause loss of some information specific to the unfolded modalities and
%make interpretation of  estimated components more difficult.



 Clearly the ``flat-world view" provided by 2-way matrix factorizations (ICA, NMF, SCA)
 may be insufficient and it is
natural to use   tensor decomposition
approaches. This way  all dimensions or modes are retained by virtue of
  multi-linear models which  often produce  unique and
 physically meaningful  components.
For example,
 studies  in neuroscience often involve multiple subjects (people or animals) and trials
leading to experimental data structures conveniently represented
by  multiway arrays or  blocks of three-way data.
 If the data for every subject were analyzed separately by
 extracting a matrix or slice from a data
 block we would lose the covariance information among subjects. To discover hidden components within the data
 and retain the integrative information, the analysis tools should reflect the multi-dimensional structure of the data.
%
% Thus, a key factor in analysis of complex data is the structure and the form (organization) of
% the data and access to efficient multi-way analysis tools to discover hidden components and
% get more quantitative information form integrative measurements.

The multi-way analysis\inxx{Why multi-way array decompositions} (tensor factorizations and decompositions) is a natural choice, for instance, in EEG studies as it provides convenient
 multi-channel and multi-subject time-frequency-space sparse representations,
 artifacts rejection in the time-frequency domain,
feature extraction, multi-way clustering and coherence tracking. Our
main objective here is to decompose the multichannel time-varying EEG
signals into multiple components with distinct modalities in the
space, time, and frequency domains in order to identify among them the
components common across these different domains, which at the same
time  are discriminative across different conditions (see Figure  \ref{Fig1-EEGtensors}).
\begin{figure}[t]
\subfigure[tight][]{\includegraphics[width=6.5cm,height=3.4cm]{tensor_FChT_c1}\label{Fig-FChT}}
\hfill\subfigure[tight][]{\includegraphics[width=6.5cm,height=3.4cm] {tensor_FCoT_c1}\label{Fig-FCoT}}
\subfigure[tight][]{\includegraphics[width=6.1cm,height=3.2cm]{tensor_TFChSc_c2}\label{Fig-TFChSc}}
\hfill\subfigure[tight][]{\includegraphics[width=6.9cm,height=3.9cm] {FTChTCoS_c}\label{Fig-TFCCoS}}
%
\caption{Illustration of various possible arrangements (organization)
of three-way and  multi-way multichannel EEG/MEG data.}
\label{Fig1-EEGtensors}
\end{figure}
The two most popular  decomposition/factorization models for $N$-th
order tensors are the Tucker model and the more restricted PARAFAC
model.
%
 Especially, NMF and NTF in conjunction with sparse
coding, have recently been given much attention due to their easy
interpretation and meaningful representation.
NTF  has been used in numerous applications in environmental analysis, food studies, pharmaceutical analysis and in chemistry in general (see \cite{Bro_2006},\cite{Acar2008},\cite{Kolda08} for review).


As a result of such tensor decompositions,  the inherent structures
of the recorded brain signals  usually  become enhanced and better
exposed.  Further operations performed on these components can
remove redundancy and achieve compact sparse representations. There
are at least two possible operations we can perform. First, the
extracted factors or hidden latent components can be grouped
(clustered) together and represented collectively in a lower
dimensional space to extract features and remove redundancy. Second,
the components can be simply pruned if they are correlated with a
specific mental task.  With the addition of extra dimensions it is
possible to investigate topography and time and frequency patterns
in one analysis.
%
The resulting components can be described  not only by the
topography and the time-frequency signature but also by the
relative contribution from different subjects or conditions.
%
Regarding an application to brain signal analysis,
 various oscillatory activities within the EEG
may overlap, however, the sparse and nonnegative tensor representation by means of
 the time-frequency-space transformation makes it possible
in many cases to isolate each oscillatory
behavior well, even when these activities are not well-separated in
the space-time (2-way) domain.
%
%Tensor decompositions have been recently used for

% neuroimage and video processing, which  allows us to better
%exploit spatial, temporal and chromatic correlations between pixels
%of image sequences. This leads also to extraction of some important
%hidden components and an decrease in a model size.

Recent  development in  high spatial  density arrays of EEG
signals involve multi-dimensional signal processing techniques
(referred to as multi-way analysis (MWA), multi-way-array (tensor)
factorization/decomposition,  dynamic tensor analysis (DTA), or
window-based tensor analysis (WTA)). These  can be employed  to
analyze multi-modal and multichannel experimental EEG/MEG and fMRI data \cite{Andersen},\cite{Valdes08},\cite{Liang08}.

%Standard matrix factorizations, like PCA, SVD, ICA, NMF, and their
%variants, are invaluable tools for feature selection, dimensionality
%reduction,   noise reduction, and mining []. However, they have only
%two modes or two-way representations (e.g., channels and time) and
%therefore have severe intrinsic limitations.

%In comprehensive neuroscience studies  the brain data structures
%often contain higher-order ways (modes) such as trials, task
%conditions, subjects, and groups in addition to the intrinsic
%dimensions of space, time and frequency. In fact, specific mental
%tasks or stimuli are often presented repeatedly in a sequence of
%trials leading to a large volume stream of data encompassing many
%dimensions: channels (space), time-frequency, trials, and
%conditions.

%Two-way matrix factorizations (ICA, NMF) or "flat-world view" may be
%insufficient for analysis of multi-modal and multi-channel brain
%data. In order to obtain more natural representations of the
%original multidimensional data structure, it is necessary to use
%tensor decomposition approaches since additional dimensions or modes
%can be retained only in multi-linear models to produce structures
%that are unique and which admit interpretations that are
%neurophysiologically meaningful [].

%The advantages of the sparse NTF/NMF based feature extraction approach lies in its capability to yield components which are common across the space, time and/or frequency domains and at the same time discriminative across different conditions without prior knowledge of the discriminative frequency bands and temporal windows for a specific subject.

%Note, that in general, tensor decompositions allow multi-channel and multi-subject time-frequency-space sparse representation, artifacts rejection in the time-frequency domain, feature extraction, multi-way clustering and coherence tracking. Our main objective is to decompose the multichannel time-varying EEG into multiple components with distinct modalities in the space, time and frequency domains to identify among them the components common across these different domains and at the same time discriminative across different conditions. Further operations performed on these components can remove redundancy and achieve compact sparse representation. There are at least two possible operations we can perform. First, extracted factors or hidden latent components can be grouped (clustered) together and represented collectively in a lower dimensional space to extract features and remove redundancy. Second, a component can be simply pruned if it is not uncorrelated with specific mental task.  Note that by the addition of extra dimensions it is possible to investigate topography and time-and frequency pattern (e.g., Morlet wavelets) in one analysis [21].
%
%The resulting components can be described, not only by the topography and the time-frequency signature but also by the relative contribution from the different users or conditions (see Figure  4). In practice, various oscillatory activities might overlap, but the sparse and nonnegative representations of the tensor data given e.g., by the time-frequency-space transformation enables the decompositions to isolate each oscillatory behavior well even when these activities are not well-separated in the space-time domain alone.



 \subsection{PARAFAC and Nonnegative Tensor  Factorization}
 \label{sec_PARAFAC}


 The PARAFAC\inxx{PARAFAC}{\footnote{Also called the CANDECOMP (Canonical Decomposition) or CP decomposition or simply CPD.}} can be formulated as follows (see Figures \ref{Parafac21} and \ref{Fig-varCP} for graphical representations).
 %\begin{quotation}
%

 Given a data tensor $\underline \bY \in \Real^{I \times T \times Q}$  and the positive index $J$, find three-component matrices, also called loading matrices or factors, $\bA=[\ba_1,\ba_2, \ldots, \ba_J] \in \Real^{I \times J}, \bB=[\bb_1,\bb_2, \ldots, \bb_J] \in \Real^{T \times J}$ and $ \bC=[\bc_1,\bc_2, \ldots, \bc_J] \in \Real^{Q \times J}$ which perform the following approximate factorization:
 \be
  \hlbox{
\underline \bY = \sum_{j=1}^J \ba_j \circ \bb_j \circ \bc_j + \underline \bE =  \llbracket \bA,\bB,\bC \rrbracket + \underline \bE,
}
\label{CP-outer}
 \ee
 or equivalently in the element-wise form
 \be
  \hlbox{
 y_{itq} =\sum_{j=1}^J a_{ij} b_{tj} c_{qj} + e_{itq}.
 }
 \label{CP-scalar}
 \ee
 \begin{figure}[t]
\centering
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}
    \includegraphics[width=\textwidth]{AY-matri-CPbis-b_c}
   \caption{A graphical representation of the  third-order PARAFAC  as a sum of rank-one tensors. All the vectors $\{\ba_j, \bb_j, \bc_j \}$ are treated as column vectors of factor matrices and are linked for each index $j$ via the outer product operator, that is, $\underline \bY = \sum_{j=1}^J \ba_j \circ \bb_j \circ \bc_j + \bE$ or equivalently in a compact form $\underline \bY = \underline \bI \times_1 \bA \times_2 \bB \times_3 \bC  + \bE$. (In this model not all vectors are normalized to unit length).}
    \label{Parafac21}
\end{figure}
%
The symbol  $\widehat {\underline \bY} = \llbracket \bA,\bB, \bC \rrbracket$ is a shorthand notation for the PARAFAC factorization, and $ \ba_j =[a_{ij}] \in \Real^I$,   $\bb_j =[b_{tj}] \in \Real^T$, and $\bc_j =[c_{qj}] \in \Real^Q$ are respectively the constituent vectors of the corresponding factor matrices $\bA,\bB$ and $\bC$.
 %Here, $a_{ij}$ stands for the $i,j$ element of  $I \times J$ matrix $\bA$,
% similarly, $b_{tj}$ and $c_{qj}$ stand for $t,j$th and $q,j$-th elements
%of matrices $\bB$ and $\bC$, respectively.
 %\end{quotation}
%


\begin{figure}[p]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+11.2cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{S}[bc][bc]{\color{black}\footnotesize$T$}
\psfrag{x}[bc][bc]{\color{black}\footnotesize$\times$}
\includegraphics[width=11.2cm,height=3.2cm]{parafac03b_c_fix} \label{Fig-varCP_a}}
\setlength{\templength}{(-\textwidth+10.2cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{1}[tc][tc]{\color{black}\footnotesize$1$}
\psfrag{J}[tc][tc]{\color{black}\footnotesize$J$}
\psfrag{T}[bc][bc]{\color{black}\footnotesize$T$}
\psfrag{I}[bc][bl]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-4pt}\footnotesize ($I$\end{tabular}}%
\psfrag{Q}[bc][bc]{\color{black}\footnotesize$Q$)}
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}
\psfrag{e}{}
\psfrag{f}{}
\includegraphics[width=10.2cm,height=5.1cm]{parafac_mat_colorb_fix2}\label{Fig-varCP_b}}
\setlength{\templength}{(-\textwidth+11.2cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{r}[bc][bc]{\color{black} $r$}
\psfrag{q}[bc][bc]{\color{black} $q$}
%\psfrag{Q}[bc][bc]{\color{black} \small $Q$}
\psfrag{q= 1,2,...,Q}[tc][bc]{\color{black} $(q=1,2,\ldots,Q)$}
\psfrag{ccong}[c][c]{\color{black}\Large$\cong$}
\psfrag{ctriangleq}[c][c]{\color{black}\Large $\triangleq$}
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}
\includegraphics[width=11.2cm,height=3.1cm]{parafac06b_c_fix2}\label{Fig-varCP_c}}
%
%
\setlength{\templength}{(-\textwidth+9.2cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{ccong}[c][c]{\color{black}\Large$\cong$}
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}
\psfrag{q = 1,2,...,Q}[bc][bc]{\color{black} $(q=1,2,\ldots,Q)$}
\psfrag{q}[bc][bc]{\color{black} $q$}
\includegraphics[width=9.2cm,height=4.5cm]{parafac08bbis_c_fix}\label{Fig-varCP_d}}
%
 \caption{The alternative representations of the third-order PARAFAC model:
 \subref{Fig-varCP_a} as a set of three  matrices using a scalar representation (see Eq. (\ref{CP-scalar})), \subref{Fig-varCP_b} as a set of vectors using summation of rank-one tensors expressed by the outer products of the vectors (see Eq. (\ref{CP-outer})), \subref{Fig-varCP_c} decomposition into two matrices using row-wise unfolding and \subref{Fig-varCP_d} representation by frontal slices (see Eq. (\ref{CP-JAD11})) The tensor $\underline \bD \in \Real^{J \times J \times Q}$ has diagonal frontal slices $\bD_q \in \Real^{J \times J}$, so we can write $\underline \bY \approx \underline \bD \times_1 \bA \times_2 \bB$.}
 \label{Fig-varCP}
\end{figure}

 The PARAFAC algorithms decompose a given tensor into a sum of multi-linear terms (in this case tri-linear), in a way analogous  to the bilinear matrix decomposition.
 As discussed before, unlike SVD, PARAFAC usually does not impose any orthogonality constraints.
 % Moreover, in our applications we impose nonnegativity and sparsity constraints.
 A model which imposes nonnegativity on factor matrices is called the NTF\inxx{NTF model} (\inx{Nonnegative Tensor Factorization}) or Nonnegative PARAFAC. A nonnegative version of PARAFAC was first introduced by Carroll {\it et al.} \cite{Carr1989}. Later, more efficient approaches were developed by Bro \cite{Bro1997},\cite{Nwaytoolbox},\cite{Kim-Elden-Park}, based on the modified NLS  and Paatero \cite{Paattero97},\cite{Paatero}  who generalized his earlier 2-way positive matrix factorization (PMF) method to the three-way PARAFAC model, referring to the result as PMF3 (three-way positive matrix factorization).
 %
 Although such constrained nonnegativity based model may  not match perfectly the input data (i.e., it may have  larger residual errors $\bE$ than the standard PARAFAC without any constraints) such decompositions are often very meaningful and have physical interpretation \cite{Phan-HALS-IEICE},\cite{PHANCIA_08_ISNN},\cite{PHANCIA_08_NOLTA}.

It is often convenient to assume that all vectors  have  unit length so that
we can use the modified \inx{Harshman's PARAFAC model} given by \cite{Harshman}, \cite{Harsman72}
 \be
\hlbox{
\underline \bY = \sum_{j=1}^J \lambda_j \; \ba_j \circ \bb_j \circ \bc_j + \underline \bE  \cong \llbracket \mbi \lambda,\bA,\bB,\bC \rrbracket,
}
 \label{CPlambda}
 \ee
 or in equivalent element-wise form
 \be
\hlbox{
 y_{itq}=\sum_{j=1}^J \lambda_j \; a_{ij} \; b_{tj} \; c_{qj} + e_{itq},
 }
 \ee
 where $\lambda_j$ are scaling factors and $\mbi \lambda =[\lambda_1, \lambda_2, \ldots, \lambda_J]^T$.
 %
\begin{figure}[t]
\centering
%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSparafac07.eps}\\
%\vspace{1.5cm}
\psfrag{ccong}[c][c]{\color{black}\Large$\cong$}
\psfrag{x}[bc][bc]{\color{black}$\times$}
\psfrag{lda1}[c][c]{\color{black}$\lambda_1$}
\psfrag{ldaJ}[c][c]{\color{black}$\lambda_J$}
\includegraphics[width=13.1 cm,height=8.1cm]{ALSPARAFAC07bisb_c_fix.eps}
\caption{Harshman's PARAFAC model with a superdiagonal core tensor ${\underline \bG} =
\underline {\mbi \Lambda} =
\diag(\lambda_1,\lambda_2, \ldots, \lambda_J)  \in \Real^{J \times J \times J}$
for the third-order tensor $\underline \bY \cong \underline \bLambda \times_1 \bA \times_2 \bB \times_3 \bC =\sum_{j=1}^J \lambda_j \; \ba_j \circ \bb_j \circ \bc_j$. (In this model all vectors are normalized to unit length).}
\label{Fig-Kruskal}
\end{figure}
 Figure  \ref{Fig-Kruskal}  illustrates the above model and its alternative equivalent representations.
 %There are  several graphical and mathematical interpretations of the
% PARAFAC model as illustrated in Figure  \ref{Fig-varCP}.
 The basic PARAFAC model can be represented in compact matrix forms upon applying unfolding representations of the tensor $\underline \bY$:
\minrowclearance 4pt
\arrayrulecolor{bordercolor}
\begin{equationarray}
{|f
>{\columncolor{highlightcolor}[1.19ex][.5\tabcolsep]}l Nl
>{\columncolor{highlightcolor}[.4\tabcolsep][.8\tabcolsep]}rNl
>{\columncolor{highlightcolor}[.4\tabcolsep][1.2ex]}rk|}
\cline{1-5}
 &\bY_{(1)} &\cong& \bA \; \mbi \Lambda \; (\bC \odot \bB)^T,&\\
 &\bY_{(2)} &\cong& \bB  \; \mbi \Lambda \; (\bC \odot \bA)^T,&\\
 &\bY_{(3)} &\cong& \bC \; \mbi \Lambda \; (\bB \odot \bA)^T,&
\\[1.01ex]
\cline{1-5} \noalign {\vskip-16pt}  \arrayrulecolor{black}
\nonumber
\end{equationarray}
\minrowclearance 0pt
%\boxedeqnt{
% \be
% \bY_{(1)} &\cong& \bA \; \mbi \Lambda \; (\bC \odot \bB)^T,\\
% \bY_{(2)} &\cong& \bB  \; \mbi \Lambda \; (\bC \odot \bA)^T,\\
% \bY_{(3)} &\cong& \bC \; \mbi \Lambda \; (\bB \odot \bA)^T,
% \ee
%}
where $\mbi \Lambda =\diag(\mbi \lambda)$ and $\odot$ means the Khatri-Rao  product.

Using the mode-$n$ multiplication of a tensor by a matrix, we have
 \be
  \hlbox{
 \underline \bY = \underline \bLambda \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE,
 }
 \label{CP-JAD}
 \ee
where $\underline \bLambda \in \Real^{J \times J \times J}$ is diagonal cubical tensor with nonzero elements $\lambda_j$ on the superdiagonal. In other words, within Harshman's model for the core tensor all but  the superdiagonal elements vanish
(see Figure  \ref{Fig-Kruskal}).
This also means that PARAFAC can be considered as a special case of the Tucker3 model in which the core tensor is a cubical superdiagonal or super-identity tensor,
i.e., $\underline \bG =\underline \bLambda \in \Real^{J \times J \times J}$ with $g_{jjj} \neq 0$.

Another form of the PARAFAC model is the vectorized form given by
%
\be
 \hlbox{
\text{vec} \; (\underline \bY) \cong (\bC \odot \bB \odot \bA)
{\mbi \lambda}.
}
 \ee
%
  The three-way PARAFAC model can be also described by using frontal,  lateral
  and horizontal slices as follows{\footnote{Such a representation does not exist for higher-order tensors where $N > 3$.}}
 %\be
% \bY_{:\,:\,q} &=& \bA \diag(\bc_{q\,:} )\bB^T, \\
% \bY_{i\,:\,:} &=& \bB \diag(\ba_{i\,:} ) \bC^T,\\
% \bY_{:\,t\,:} &=& \bA \diag(\bb_{t\,:} ) \bC^T,
% \ee
 \be
 \bY_{:\,:\,q} &\cong& \bA \; \bD_q( \bc_{q\,:}) \; \bB^T, \\
 \bY_{:\,t\,:} &\cong& \bA \; \bD_t(\bb_{t\,:}) \; \bC^T, \\
 \bY_{i\,:\,:} &\cong& \bB \; \bD_i (\ba_{i\,:}) \; \bC^T,
 \ee
 where $\bD_i(\ba_{i\,:} )$, $\bD_t(\bb_{t \,:})$ and $\bD_q(\bc_{q\;:} )$ are diagonal matrices which take the $i$-th, $t$-th and $q$-th row of
 the matrices $\bA,\bB$, and $\bC$, respectively, and produce diagonal matrices by placing the  corresponding row on the main diagonal.

 In particular, it  is convenient to represent the three-way
 PARAFAC model in terms of the frontal slices, as
 $\bY_q=\bY_{:\,:\,q}$ of $\underline \bY$
 \be
  \hlbox{
 \bY_q \cong \bA \bD_q \bB^T,
 }
 \label{CP-JAD11}
 \ee
 where a matrix $\bD_q := \bD_q(\bc_{q\,:})$ is the diagonal matrix based on the $q$-th row of $\bC$.

%\begin{table}[p]
%\caption{Mathematical formulations of the standard PARAFAC
%model for a third order tensor $\underline \bY \in \Real^{I \times T
%\times Q}$ with factor matrices: $\bA=[\ba_1,\ba_2, \ldots,\ba_J] \in \Real^{I \times J}, \; \bB
%=[\bb_1,\bb_2, \ldots,\bb_J] \in \Real^{T \times J}$ and $\bC =[\bc_1,\bc_2, \ldots, \bc_J] \in \Real^{Q \times J}$. There are two optional and equivalent representations: one in which we have linear combination of rank-one tensors with unit  length vectors and second in which  the vectors $\ba_j \in \Real^{I} $ and $\bb_j \in \Real^{T}$  have a unit $\ell_2$-norm and scaling factors $\lambda_j$ are absorbed by the non-normalized vectors $\bc_j  \in \Real^{Q}$.} \centering
%%\renewcommand{\arraystretch}{1.}
%%\setlength{\tabcolsep}{0.8cm}
% %   \begin{center}
%   \shadingbox{
%    \begin{tabular*}{\textwidth}[t]{@{\extracolsep{\fill}}ll} \hline  &  \\
%{ \raisebox{0mm}[0mm][4mm]{Operator}}
%&  Formulation  \\ \hline
% &   \\
% Outer products   &
% $  \underline \bY = \sum\limits_{j=1}^{J} {\ba_{j} \circ \bb_{j} \circ \bc_{j}} + \underline \bE, \quad (\mbox{s.t.} \;\; ||\ba_j||_2=||\bb_j||_2=1, \; \forall j )$ \\
%    &     $\underline \bY = \sum\limits_{j=1}^{J} {\lambda_j \; \ba_{j} \circ \bb_{j} \circ \bc_{j}} + \underline \bE, \quad (\mbox{s.t.} \;\; ||\ba_j||_2=||\bb_j||_2=||\bc_j||_2 =1, \; \forall j )  $ \\  &  \\ \hline & \\
%     Scalar  & $y_{itq} = \sum\limits_{j=1}^{J} {a_{ij} \; b_{tj} \; c_{qj}} + e_{itq}$ \\
%        &             $ y_{itq} = \sum\limits_{j=1}^{J} {\lambda_j \;a_{ij} \; b_{tj} \; c_{qj}} + e_{itq} $
%                   \\  &   \\ \hline &  \\
%     &   \\
%   mode-$n$ multiplications &
%                      $  \underline \bY = \underline \bI \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE $ \\
%    &   $ \underline \bY = \underline {\bf \Lambda} \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE $ \\  &   \\ \hline
%     &   \\
%    \multirow{3}{*}{Slice representations}
%    & {$\bY_{::q} = \bA \; \diag(\bc_{q:}) \; \bB^T  \cong \bA  \; \bD_q \,(\bc_{q:}) \; \bB^T + \bE_{::q}$}\\
%    & {$\bY_{i::} = \bB \; \diag(\ba_{i:}) \;\bC^T \cong \bB \; \bD_i \,(\ba_{i:}) \; \bC^T+ \bE_{i::}$}\\
%     & {$\bY_{:t:} = \bA \; \diag(\bb_{t:}) \; \bC^T \cong \bA \; \bD_t \,(\bb_{t:}) \; \bC^T  + \bE_{:t:}$}
%                                          \\  &  \\ \hline &  \\
%    Vectors & {$\text{vec}(\underline \bY) = \text{vec}(\bY_{(1)}) = (\bC \odot \bB \odot \bA) \;{\boldsymbol \lambda} + \text{vec}(\bE_{(1)}$} \\  &   \\ \hline
%     &   \\
%    Kronecker products & {$\bY_{(1)} = \bA \; \bD_{(1)} (\bc_q) \;(\bI \otimes \bB)^T + \bE_{(1)}$}\\  &
%    \\ \hline  &  \\
%    \multirow{3}{*}{Khatri-Rao products} & {$\bY_{(1)} \cong \bA \; (\bC \odot \bB)^T + \bE_{(1)}$}\\
%                                        & {$\bY_{(2)} \cong \bB \; (\bC \odot \bA)^T + \bE_{(2)}$} \\
%                                        & {$\bY_{(3)} \cong \bC \; (\bB\odot \bA)^T + \bE_{(3)}$}\\ & \\ \hline
%    \end{tabular*}
%    }
%%    \end{center}
%\label{table-Parafac}
%\end{table}
\minrowclearance 2ex
\begin{table}[t!]
\caption{Mathematical formulations of the standard PARAFAC
model for a third-order tensor $\underline \bY \in \Real^{I \times T
\times Q}$ with factor matrices: $\bA=[\ba_1,\ba_2, \ldots,\ba_J] \in \Real^{I \times J}, \; \bB
=[\bb_1,\bb_2, \ldots,\bb_J] \in \Real^{T \times J}$ and $\bC =[\bc_1,\bc_2, \ldots, \bc_J] \in \Real^{Q \times J}$. There are two optional and equivalent representations: one in which we have a linear combination of rank-one tensors with unit  length vectors and a second in which  the vectors $\ba_j \in \Real^{I} $ and $\bb_j \in \Real^{T}$  have a unit $\ell_2$-norm and the scaling factors $\lambda_j$ are absorbed by the non-normalized vectors $\bc_j  \in \Real^{Q}$.} \centering
%\renewcommand{\arraystretch}{1.}
%\setlength{\tabcolsep}{0.8cm}
 %   \begin{center}
   \shadingbox{
    \begin{tabular*}{\textwidth}[t]{@{\extracolsep{\fill}}ll} \hline
\multicolumn{1}{c}{Operator} &\multicolumn{1}{c}{Formulation} \\[1ex] \hline
 Outer products   &
            \begin{minipage}[c]{.75\textwidth}
                           \abovedisplayskip=-1.3ex
                           \belowdisplayskip=0pt
                           \minrowclearance .5ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                                $  \underline \bY$ &=& \sum\limits_{j=1}^{J} {\ba_{j} \circ \bb_{j} \circ \bc_{j}} + \underline \bE, \quad (\mbox{s.t.} \;\; ||\ba_j||_2=||\bb_j||_2=1, \; \forall j ) & \\
                                $\underline \bY$ &=& \sum\limits_{j=1}^{J} {\lambda_j \; \ba_{j} \circ \bb_{j} \circ \bc_{j}} + \underline \bE, \quad (\mbox{s.t.} \;\; ||\ba_j||_2=||\bb_j||_2=||\bc_j||_2 =1, \; \forall j )
                            \end{equationarray*}
                        \end{minipage}
            \\[6.5ex] \hline
 Scalar  &
            \begin{minipage}[c]{.75\textwidth}
               \abovedisplayskip=-1.3ex
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \begin{equationarray*}{p{3ex}llp{8cm}}
                    $y_{itq}$ &=& \sum\limits_{j=1}^{J} {a_{ij} \; b_{tj} \; c_{qj}} + e_{itq} & \\
                    $y_{itq}$ &=& \sum\limits_{j=1}^{J} {\lambda_j \;a_{ij} \; b_{tj} \; c_{qj}} + e_{itq}
                \end{equationarray*}
            \end{minipage}
            \\[6.5ex] \hline
   Mode-$n$ multiplications &
               \begin{minipage}[c]{.75\textwidth}
               \abovedisplayskip=-1.3ex
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \begin{equationarray*}{p{3ex}llp{8cm}}
                    $\underline \bY$ &=& \underline \bI \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE &\\
                    $\underline \bY$ &=& \underline {\mbi \Lambda} \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE
                \end{equationarray*}
            \end{minipage}
            \\[2ex] \hline
    Slice representations
                &
                \begin{minipage}[c]{.75\textwidth}
               \abovedisplayskip=-1.3ex
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \begin{equationarray*}{p{3ex}llp{8cm}}
                    $\bY_{::q}$ &=& \bA \; \diag(\bc_{q:}) \; \bB^T + \bE_{::q} =\bA  \; \bD_q \,(\bc_{q:}) \; \bB^T + \bE_{::q} &\\
                    $\bY_{i::}$ &=& \bB \; \diag(\ba_{i:}) \;\bC^T  + \bE_{i::} = \bB \; \bD_i \,(\ba_{i:}) \; \bC^T+ \bE_{i::} \\
                    $\bY_{:t:}$ &=& \bA \; \diag(\bb_{t:}) \; \bC^T + \bE_{:t:} = \bA \; \bD_t \,(\bb_{t:}) \; \bC^T  + \bE_{:t:}
                \end{equationarray*}
            \end{minipage}
            \\[3.8ex] \hline
    Vectors &
                \begin{minipage}[c]{.75\textwidth}
               \abovedisplayskip=-1.3ex
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \begin{equationarray*}{p{6ex}llp{8cm}}
                    $\text{vec}(\underline \bY)$ &=& \text{vec}(\bY_{(1)}) = (\bC \odot \bB \odot \bA) \;{\boldsymbol \lambda} + \text{vec}(\bE_{(1)}&
                     \\[0pt] \noalign {\vskip-16pt}
                \end{equationarray*}
            \end{minipage}
            \\[1ex] \hline
    Kronecker products &
            \begin{minipage}[c]{.75\textwidth}
                       \abovedisplayskip=-1.3ex
                       \belowdisplayskip=0pt
                       \minrowclearance .5ex
                       \begin{equationarray*}{p{3ex}llp{8cm}}
                            $\bY_{(1)}$ &=& \bA \; \bD_{(1)} (\bc_q) \;(\bI \otimes \bB)^T + \bE_{(1)} &
                             \\[0pt] \noalign {\vskip-16pt}
                        \end{equationarray*}
                    \end{minipage}
                    \\[1ex] \hline
    Khatri-Rao products &
         \begin{minipage}[c]{.75\textwidth}
                       \abovedisplayskip=-1.3ex
                       \belowdisplayskip=0pt
                       \minrowclearance .5ex
                       \begin{equationarray*}{p{3ex}llp{8cm}}
                            $\bY_{(1)}$ &=& \bA \; (\bC \odot \bB)^T + \bE_{(1)}; \qquad  \bY_{(1)} = \bA \;{\mbi \Lambda} (\bC \odot \bB)^T + \bE_{(1)} & \\
                            $\bY_{(2)}$ &=& \bB \; (\bC \odot \bA)^T + \bE_{(2)}; \qquad \bY_{(2)} = \bB \;{\mbi \Lambda} (\bC \odot \bA)^T + \bE_{(2)} & \\
                            $\bY_{(3)}$ &=& \bC \; (\bB\odot \bA)^T + \bE_{(3)}; \qquad \bY_{(3)} = \bC \;{\mbi \Lambda} (\bB\odot \bA)^T + \bE_{(3)}.
                        \end{equationarray*}
         \end{minipage}
         \\[3.8ex] \hline
    \end{tabular*}
    }
%    \end{center}
\label{table-Parafac}
\end{table}
\minrowclearance 0ex

The above representation of the PARAFAC model has striking similarity to
 the Approximative Joint Diagonalization method, thus having a
clear interpretation for the BSS problem,
 where the matrix $\bA$ represents a mixing matrix, $\bX=\bB^T$ represents unknown sources, and $\bC$
 represents a scaling matrix \cite{deLathauwer-JAD},\cite{deLathauwer05}. In fact, the PARAFAC factorization can be reformulated as simultaneous diagonalization of a set of matrices, which often leads to fast and reliable way to compute this factorization.

 The PARAFAC model has some severe limitations as it represents  observed data by common factors utilizing the same number of components (columns).
 In other words, we do not have enough degrees of freedom as compared to  other models. Moreover,
 the PARAFAC approximation may be ill-posed and may lead to unstable
 estimation of its components. The next sections discuss
 more flexible models.

\subsubsection[Basic Approaches to Solve NTF Problem]{\bf {Basic Approaches to Solve NTF Problem} }

In order to compute the nonnegative component matrices $\{\bA, \bB, \bC\}$ we usually apply  constrained optimization approach as by minimizing a suitable design cost function.
Typically, we minimize  (with respect the component matrices) the following global cost function
\be
D_F(\underline \bY \, ||\, \llbracket \bA,\bB,\bC \rrbracket)= ||\underline \bY - \llbracket \bA,\bB,\bC \rrbracket ||_F^2 + \alpha_{\bf A} \; ||\bA||_F^2 + \alpha_{\bf B} \; ||\bB||_F^2 + \alpha_{\bf C} \; ||\bC||_F^2,
\ee
subject to nonnegativity constraints, where $\alpha_{\bf A}, \alpha_{\bf B}, \alpha_{\bf C}$ are nonnegative regularization parameters.

There are at least three different approaches for solving this optimization problem.
The first approach is to use a vectorized
form of the above cost function in the form  $J(\bx) = \mbox{vec} (\underline \bY - \llbracket \bA,\bB,\bC \rrbracket )= \0 $ and employ the  Nonnegative Least Squares  (NLS) approach.
 Such a method was first applied for NTF by Paatero  \cite{Paatero} and  also Tomasi and Bro \cite{TomasiBro05}, \cite{TomasiBro06}. The Jacobian of such function can be  of large size $I T Q  J \times (I+T+Q) $, yielding  very high computation cost.

 In the second approach,  Acar, Kolda and  Dunlavy  propose to optimize the cost function simultaneously
 with respect to all variables
 using a modern nonlinear conjugate gradient optimization technique \cite{Acar-Kolda-CPD}. However, such a cost function is generally not convex and is not guaranteed  to obtain the optimal solution although results are very promising.

 The most popular approach is to apply the ALS technique (see Chapter \ref{Ch4} for more detail).
  In this approach we compute the gradient of the cost function
 with respect to each individual component matrix (assuming that the others are fixed and independent):
 \be
\nabla_{\bf A} D_F &=& - \bY_{(1)} \; (\bC \odot \bB) + \bA \; [(\bC^T  \bC) \; \* (\bB^T  \bB)
+ \alpha_{\bf A} \; \bI], \\
\nabla_{\bf B} D_F &=& - \bY_{(2)} \; (\bC \odot \bA)  + \bB \; [(\bC^T  \bC) \; \* (\bA^T  \bA)
+ \alpha_{\bf B} \; \bI], \\
\nabla_{\bf C} D_F &=& - \bY_{(3)} \; (\bB \odot \bA) + \bC  \; [(\bB^T  \bB) \; \* (\bA^T  \bA)
+ \alpha_{\bf C} \; \bI ].
\ee
By equating the gradient components to zero and applying the nonlinear projection to maintain nonnegativity of components we obtain efficient  and relatively simple nonnegative ALS update rules for the NTF:
%
\minrowclearance 4pt
\arrayrulecolor{bordercolor}
\begin{equationarray}
{|f
>{\columncolor{highlightcolor}[1.19ex][.5\tabcolsep]}l Nl
>{\columncolor{highlightcolor}[.4\tabcolsep][.8\tabcolsep]}rNl
>{\columncolor{highlightcolor}[.4\tabcolsep][1.2ex]}rk|}
\cline{1-5}
%\begin{equationarray}{lll}
&\bA &\leftarrow& \left[ \bY_{(1)} \; (\bC \odot \bB) \; [(\bC^T  \bC) \; \* (\bB^T  \bB) +
\alpha_{\bf A}  \; \bI]^{-1} \right]_+ \,,& \\
&\bB &\leftarrow& \left[ \bY_{(2)} \; (\bC \odot \bA) \; [(\bC^T  \bC) \; \* (\bA^T  \bA) +
\alpha_{\bf B} \; \bI]^{-1} \right]_+ \,,&\\
&\bC &\leftarrow& \left[ \bY_{(3)} \; (\bB \odot \bA) \; [(\bB^T  \bB) \; \* (\bA^T  \bA)
+ \alpha_{\bf C} \; \bI]^{-1} \right]_+ \,.&
%\end{equationarray}
\\[1.01ex]
\cline{1-5} \noalign {\vskip-16pt}  \arrayrulecolor{black}
\nonumber
\end{equationarray}
%
%\begin{equationarray}{lll}
%\bA &\leftarrow& \left[ \bY_{(1)} \; (\bC \odot \bB) \; [(\bC^T  \bC) \; \* (\bB^T  \bB) +
%\alpha_{\bf A}  \; \bI]^{-1} \right]_+ \,, \\
%\bB &\leftarrow& \left[ \bY_{(2)} \; (\bC \odot \bA) \; [(\bC^T  \bC) \; \* (\bA^T  \bA) +
%\alpha_{\bf B} \; \bI]^{-1} \right]_+ \,,\\
%\bC &\leftarrow& \left[ \bY_{(3)} \; (\bB \odot \bA) \; [(\bB^T  \bB) \; \* (\bA^T  \bA)
%+ \alpha_{\bf C} \; \bI]^{-1} \right]_+ \,.
%\end{equationarray}
%
In Chapter \ref{Ch4} and Chapter \ref{Ch6} we prove that the ALS algorithms are special cases of a quasi-Newton method that implicitly employ information about the gradient and Hessian of a cost function. The main advantage of  ALS algorithms is high convergence speed and its scalability for large-scale problems.

\subsection{NTF1 Model}

 \begin{figure}[t!]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+13.1cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{1}[bc][bc]{\color{black} $1$}
\includegraphics[width=13.1cm,height=4.5cm]{parafac3cb_c}\label{Fig-NTF1_a}}
\setlength{\templength}{(-\textwidth+10.2cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{x}[bc][bc]{\color{black}\small $\times$}
\includegraphics[width=10.2cm,height=3.2cm]{NTF1_c}\label{Fig-NTF1_b}}
\setlength{\templength}{(-\textwidth+11.2cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{q}{\color{black}$q$}
\psfrag{(q=1,2,...,Q)}{\color{black}($q = 1, 2,\ldots,Q$)}
\psfrag{Q}{\color{black}$Q$}
\psfrag{r}{\color{black}$r$}
\psfrag{x}[bc][bc]{\color{black}\small $\times$}
\includegraphics[width=11.2cm,height=4.2cm]{parafac04b_c}\label{Fig-NTF1_c}}
\caption{\subref{Fig-NTF1_a} NTF1 model that (approximately) decomposes  tensor
$\underline \bY \in \Real_+^{I \times T \times Q}$ into a set of
nonnegative matrices $\bA =[a_{ij}] \in \Real_+^{I \times J}, \bC
\in \Real_+^{Q \times J}$ and $\{\bX_1,\bX_2,\ldots, \bX_Q\}$,
$\bX_q=[x_{jtq}] \in \Real_+^{J \times T}$, and $\underline \bE \in
\Real^{I \times T \times Q}$ is a tensor representing errors. Matrices $\bX_q$ are frontal slices of the tensor $\underline \bX \in \Real^{J \times T \times Q}$, typically with $J << I$. \subref{Fig-NTF1_b}
Equivalent representation using joint diagonalization  of frontal slices,
where $\bD_q =\diag(\bc_q)$ are diagonal matrices.
 \subref{Fig-NTF1_c} Global matrix representation using row-wise unfolding of the tensor;
 the sub-matrices  are defined as $\bX_q \stackrel{\triangle}{=} \bD_q
\bX_q, \;\; (q=1,2,\ldots, Q)$.} \label{Fig-NTF1}
\end{figure}
%
\begin{figure}[ht]
%    %\par
%    %(a)
%    \hspace{0.5cm}  (a) \hspace{5.2cm} (b)%\\
   \begin{center}
%      \leavevmode
%  \includegraphics[width=12.1 cm,height=4.3cm]{NTF1_AC}\\
    \includegraphics[width=13.1 cm,height=4.5cm]{KNTF1b_c}
%  \includegraphics[width=12.1 cm,height=4.3cm]{dedicom3DC}\\
%    \hspace{0.1cm}
%     \leavevmode
%    \includegraphics[width=5.7cm,height=6cm] {x_reg_new_1}
    \end{center}
%   % \par
   \caption{Extended NTF1 model for a three-way array. The goal is to estimate the set of nonnegative matrices $\bA, \bC$ and $\{\bX_1,\bX_2, \ldots, \bX_Q$\}.}
    \label{Fig-NTF1extend}
    \end{figure}

Figure  \ref{Fig-NTF1} illustrates the basic 3D NTF1 model, which is an extension of the NTF model \cite{CichZd_ntflab}\inxx{NTF1}.
%
%\begin{quotation}
A given data (observed)
tensor $\underline \bY \in \Real_+^{I \times T \times Q} $ is
decomposed into a set of matrices $\bA \in \Real_+^{I \times J}$ and
$\bC \in \Real_+^{Q \times J}$, as well as a third-order tensor with reduced dimension ($J <
I$), for which the frontal slices $\{\bX_1,\bX_2,...,\bX_Q\}$
have nonnegative entries.
This three-way NTF1 model is given by
%
\be
 \hlbox{
\bY_q=\bA \bD_q \bX_q+\bE_q, \qquad (q=1,2,\ldots,Q),
}
\label{NTF1-model1}
\ee
%
where $\bY_q= \bY_{:\,:\,q} \in \Real_+^{I \times T}$ are the
frontal slices of $\underline \bY \in \Real_+^{I \times T \times Q}$,
$Q$ is the number of the frontal slices, $\bA=[a_{ij}] \in \Real_+^{I
\times J}$ is the basis (mixing matrix) representing common factors,
$\bD_q \in \Real_+^{J \times J}$ is a diagonal matrix that holds the
$q$-th row of  matrix $\bC \in \Real_+^{Q \times J}$ in its main
diagonal,  $\bX_q =[x_{jtq}] \in \Real_+^{J \times T}$ is matrix
representing the sources (or hidden components), and $\bE_q
=\bE_{:\,:\,q} \in \Real^{I \times T} $ is the $q$-th vertical slice
of the tensor $\underline \bE \in \Real^{I \times T \times Q}$
representing the errors or noise depending on the application.
Typically, for BSS problems $T>> I \geq Q > J$.

We wish  to estimate the set of matrices $\bA$, $\bC$, and
$\{\bX_1, \bX_2,\ldots, \bX_Q\}$ subject to  nonnegativity
constraints (and other  constraints such as
sparseness and/or smoothness), given only the observed data
$\underline \bY$.
%
Since the diagonal matrices $\bD_q$ are scaling matrices, they can
 be absorbed into the matrices $\bX_q$ by introducing the
row-normalized matrices $\bX_q := \bD_q \bX_q$, thus giving $\bY_q=\bA
\bX_q+\bE_q$. Therefore, in the multi-way BSS applications only the matrix $\bA$ and the set
of scaled source matrices $\bX_1, \bX_2, \ldots, \bX_Q$ need  be
estimated.
%\end{quotation}
% Here and
%elsewhere, $\Real_+$ denotes the nonnegative orthant with
%appropriate dimensions.
%

For applications where the observed data are incomplete or  have
different dimensions for each frontal slice (as shown in
Figure  \ref{Fig-NTF1extend}) the model can be described as
 \be \bY_q=\bA \bD_q \bX_q+\bE_q, \qquad (q=1,2,\ldots,Q),
\label{NTF1-modelextend} \ee where $\bY_q  \in \Real_+^{I \times
T_q}$ are the frontal slices of the irregular tree-dimensional array,
$\bX_q =[x_{jtq}] \in \Real_+^{J \times T_q}$ are matrices
representing sources (or hidden components), and $\bE_q
=\bE_{:\,:\,q} \in \Real^{I \times T} $ is the $q$-th vertical slice
of the multi-way array
comprising errors.
%or noise depending upon the application. Q is a
%number of the frontal slices, $\bA=[a_{ij}] \in \Real_+^{I \times J}$
%is the basis (mixing matrix) representing common factors, $\bD_q \in
%\Real_+^{J \times J}$ is a diagonal matrix that holds the $q$-th row
%of the matrix $\bD \in \Real_+^{Q \times J}$ in its main diagonal,

 %\begin{figure}[ht]
%\begin{center}
%\includegraphics[width=13.2cm,height=5.9cm]{Parafac1C}\\
%\vspace{0.9cm}
%\includegraphics[width=13.2cm,height=2.7cm]{Parafac26NC}
%\end{center}
%%\vspace{-0.5cm}
% \caption{(a) NTF2 model in which a 3D tensor is
%decomposed into a set of nonnegative matrices:
%$\{\bA_1,\ldots,\bA_Q\}, \; \bD, \; \bX$. (b) Equivalent
%representation in which frontal
% slices of a tensor are factored by a set
%of matrices (tensor $\underline \bE$ representing error is omitted
%for simplicity). } \label{Fig7-chap1}
%\end{figure}

\subsection{NTF2 Model}
\label{sec:NTF2}

%
The dual model to  the NTF1
 is referred to as the 3D NTF2\inxx{NTF2} (by analogy to the
PARAFAC2 model\inxx{PARAFAC2 model}{\footnote{In fact the NTF2 model be can obtained form NTF1 model via simple permutation of tensors and matrices. However, since the frontal slices $\bA_q$ and $\bX_q$ of the core tensors have different physical interpretations we discuss these models separately.}} \cite{Kiers99},\cite{bro_book},\cite{Morup},\cite{Smilde},\cite{Kroonenberg}, see Figure  \ref{Fig-NTF2}).


 \begin{figure}[p]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\subfigure[]{
\psfrag{1}{\color{black}1}
\psfrag{x}[bc][bc]{\color{black}\small$\times$}
\includegraphics[width=\textwidth,height=5.4cm, trim = 0 0 0 0 , clip = true]{parafac02cb_c_fix}\label{Fig-NTF2_a}}
\vfill
\subfigure[]{
\psfrag{T}{\color{black}$T$}
\psfrag{T}{\color{black}$T$}
\psfrag{I}{\color{black}$I$}
\psfrag{J}{\color{black}$J$}
\psfrag{x}[bc][bc]{\color{black}\small$\times$}
\includegraphics[width=\textwidth,height=2.9cm]{parafac26Nb_c}\label{Fig-NTF2_b}}
\setlength{\templength}{(-\textwidth+12.6cm)/2}
\subfigcapmargin \templength
\vfill
\subfigure[]{
\psfrag{T}{\color{black}$T$}
\psfrag{T}{\color{black}$T$}
\psfrag{I}{}
\psfrag{J}{\color{black}$J$}
\psfrag{Q}{\color{black}$Q$}
\psfrag{QI}{\color{black}$QI$}
\psfrag{050Q}[bc][bl]{\color{black}$(QI$}
\psfrag{x}[bc][bc]{\color{black}\small$\times$}
\psfrag{QxIxT}[bc][bc]{\color{black} $(Q I \times T)$}%
\psfrag{QxIxJ}[bc][bc]{\color{black} $(Q I \times J)$}%
\psfrag{JxT}[bc][bc]{\color{black} $(J\times T)$}%
\includegraphics[width=12.6cm,height=7.2cm]{BMCR_NTF2_c_fix}\label{Fig-NTF2_c}}
%\vspace{-0.5cm}
\vfill
 \caption{\subref{Fig-NTF2_a} NTF2 model in which a third-order tensor is
decomposed into a set of nonnegative matrices:
$\{\bA_1,\ldots,\bA_Q\}, \; \bC,$ and $\bX$. \subref{Fig-NTF2_b} Equivalent
representation in which the frontal
 slices of a tensor are factorized by a set
of  nonnegative matrices. \subref{Fig-NTF2_c} Global matrix representation using column-wise unfolding with
sub-matrices $\bA_q \stackrel{\triangle}{=} \bA_q \bD_q$. }
 \label{Fig-NTF2}
\end{figure}

 \begin{figure}[tp]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\setlength{\templength}{(-\textwidth+9.4cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{T}{\color{black}\small $T$}
\includegraphics[width=9.4cm,height=5.6cm]{KNTF2b_c}\label{Fig-NTF2e_a}}
\setlength{\templength}{(-\textwidth+12.2cm)/2}
\subfigcapmargin \templength
\vfill
\subfigure[]{
\psfrag{T}{\color{black}\small $T$}
\psfrag{x}[bc][bc]{\color{black}\small $\times$}
\includegraphics[width=12.2cm,height=2.6cm]{NTF2ediag}\label{Fig-NTF2e_b}}
\setlength{\templength}{(-\textwidth+11.5cm)/2}
\subfigcapmargin \templength
\vfill
\subfigure[]{
\psfrag{T}{\color{black}\small $T$}
\psfrag{I}{\color{black}\small $I$}
\psfrag{J}{\color{black}\small $J$}
\psfrag{Q}{\color{black}\small $Q$}
\psfrag{x}[bc][bc]{\color{black}\small $\times$}
\includegraphics[width=11.5cm,height=7.2cm]{NTF2eunf}\label{Fig-NTF2e_c}}
%\vspace{-0.5cm}
\vfill
 \caption{\subref{Fig-NTF2e_a} Extended NTF2 model. \subref{Fig-NTF2e_b} An equivalent
representation in which the frontal
 slices of a three-way array are factorized by a set
of nonnegative matrices. \subref{Fig-NTF2e_c} Global matrix representation using column-wise unfolding with
sub-matrices $\bA_q \stackrel{\triangle}{=} \bA_q \bD_q$. }
 \label{Fig-NTF2e}
\end{figure}
%
%\begin{figure}
%%    %\par
%%    %(a)
%%    \hspace{0.5cm}  (a) \hspace{5.2cm} (b)%\\
%   \begin{center}
%%      \leavevmode
%%  \includegraphics[width=12.1 cm,height=4.3cm]{NTF1_AC}\\
%%   \includegraphics[width=10.1 cm,height=6.8cm]{KNTF2C}
% \includegraphics[width=11.4 cm,height=6.8cm]{KNTF2b_c}
%%   \includegraphics[width=12.1 cm,height=4.3cm]{dedicom3DC}\\
%%    \hspace{0.1cm}
%%     \leavevmode
%%    \includegraphics[width=5.7cm,height=6cm] {x_reg_new_1}
%    \end{center}
%%   % \par
%   \caption{An extended NTF2 model for irregular 3-way array. The goal is to estimate the set of nonnegative matrices $\{\bA_1,\bA_2, \ldots,\bA_Q\}, \bX, \bC$.}
%    \label{Fig-NTF2extend}
%    \end{figure}
% Here and
%elsewhere, $\Real_+$ denotes the nonnegative orthant with
%appropriate dimensions.

%\begin{quotation}
 A given tensor
$\underline \bY \in \Real_+^{I \times T \times Q} $ is
decomposed into a set of matrices $\{\bA_1,\bA_2,...,\bA_Q\}$, $\bX=\bB^T$ and $\bC$
with nonnegative entries, by the  three-way NTF2 model as
\be
 \hlbox{
\bY_q=\bA_q \bD_q \bX+\bE_q, \qquad (q=1,2,\ldots,Q),
}
\ee
%
where
$\bY_q= \bY_{:\,:\,q} =[y_{itq}] \in \Real_+^{I \times T}$
are the frontal slices of $\underline \bY \in \Real_+^{I \times T \times
Q}$, $Q$ is the number of frontal slices, $\bA_q=[a_{ijq}]
 \in \Real_+^{I \times J}$ are the basis (mixing) matrices, $\bD_q
\in \Real_+^{J \times J}$ is a diagonal matrix that holds the $q$-th
row of  $\bC \in \Real_+^{Q \times J}$ in its main diagonal,
$\bX =[x_{jt}] \in \Real_+^{J \times T}$ is a matrix
representing latent sources (or hidden components or common factors), and
$\bE_q =\bE_{:\,:\,q} \in \Real^{I \times T} $ is the $q$-th frontal
slice of a tensor $\underline \bE \in \Real^{I \times T \times Q}$
comprising error or noise depending on the application. The
goal  is to estimate the set of matrices $\{\bA_q\},
(q=1,2,\ldots,Q)$, $\bC$ and $\bX$, subject to some
nonnegativity constraints and other possible natural constraints
such as sparseness and/or smoothness. Since the diagonal matrices
$\bD_q$ are scaling matrices they can  be absorbed into the
matrices $\bA_q$ by introducing column-normalization, that is, $\bA_q :=
\bA_q \bD_q$. In BSS applications, therefore, only the matrix $\bX$ and the
set of scaled matrices $\bA_1, \ldots, \bA_Q$ need  be
estimated.  This, however, comes at a price, as  we may lose the uniqueness of the
NTF2 representation ignoring the scaling and permutation ambiguities. The
uniqueness  can still  be preserved  by imposing  nonnegativity and sparsity
 constraints.
%\end{quotation}

The  NTF2 model is similar to the well-known PARAFAC2 model\footnote{In the PARAFAC2 model we usually assume that
$\bA^T_q \bA_q =\mbi \Phi \in  \Real^{J \times J}, \;\; \forall q\;\;$ (i.e., it is required that the matrix product $\bA_q$ with its transpose is invariant for all
 frontal slices of a core three-way tensor $\underline \bA$).}
with nonnegativity constraints and to the Tucker models described in the next section
\cite{Morup},\cite{Smilde},\cite{Kroonenberg}. In a special case, when all
matrices $\bA_q$ are identical, the  NTF2 model can be simplified into
the ordinary PARAFAC model (see Section \ref{sec_PARAFAC}) with the
nonnegativity constraints described by \be \bY_q=\bA \bD_q
\bX+\bE_q, \qquad (q=1,2,\ldots,Q). \ee
%
As shown in Figure  \ref{Fig-NTF2e_a} the NTF2 model can be extended to the decomposition
of multi-way arrays with different dimensions using the simultaneous factorizations
%
\be
\bY_q=\bA_q \bD_q \bX+\bE_q, \qquad (q=1,2,\ldots,Q),
\ee where
%$\bY_q= \in \Real_+^{I_q \times T}$
%are frontal slices of irregular 3-way array,
%$\bX =[x_{jt}] \in \Real_+^{J_q \times T}$ is a matrix
%representing latent sources (or hidden components or common factors), and
%$\bE_q =\bE_{:\,:\,q} \in \Real^{I_q \times T} $ is the $q$-th frontal
%slice of a 3-way array (of the same dimensions as the data array)
%comprising errors.
%
$\bY \in \Real_+^{I_q \times T}$,
$\bX \in \Real_+^{J \times T}$, $\bC \in \Real_+^{Q \times J}$,
$\bA_q \in \Real_+^{I_q \times J} \}$, $\bE_q =\bE_{:\,:\,q} \in \Real^{I_q \times T} $ is the $q$-th frontal
slice of a three-way array (of the same dimensions as the data array) and
 $\bD_q \in \Real_+^{J \times J}$ is a diagonal matrix that holds the $q$-th
row of  $\bC$ in its main diagonal.
%
%Since the diagonal matrices
%$\bD_q$ are scaling matrices they can usually be absorbed by the
%matrices $\bA_q$ by introducing column-normalized matrices as $\bA_q :=
%\bA_q \bD_q$, so usually (in BSS applications) only matrix $\bX$ and the
%scaled matrices $\bA_1, \bA_2,\ldots, \bA_Q$ need  to be
%estimated.
%
Using the transformation $\bX_q := \bD_q \bX_q$, we can convert the NTF2 problem to the standard (2-way) NMF problem:
\be
\bar \bY = \bar \bA \bX + \bar \bE,
\ee
where $\bar \bY = \bY^T_{(2)} = [\bY_1; \bY_2; \ldots; \bY_Q] \in \Real_+^{\bar I \times T}$,
$\bar \bA = \bA^T_{(2)} = [\bA_1; \bA_2; \ldots; \bA_Q] \in \Real_+^{\bar I \times J}$,
$\bar \bE = \bE^T_{(2)} = [\bE_1; \bE_2; \ldots; \bE_Q] \in \Real_+^{\bar I \times T}$ and
$\bar I = \sum_q I_q$.

%Our objective here is to estimate the scaled matrices $\{\bA_q\}$
%  and $\bX$ subject to some
%nonnegativity constraints and other possible natural constraints
%such as sparseness and/or smoothness (see Figure  \ref{Fig-NTF2e}).
%
%The  NTF2 model is similar to the well known PARAFAC2 model with
%nonnegativity constraints and Tucker models
%
%In the special case, when all
%matrices $\bA_q$ are identical i.e., $\bA=\bA_1 = \cdots = \bA_Q$,
%then the  NTF2 model can be simplified to the ordinary nonnegative PARAFAC model:
%\be
%\bY_q=\bA \bD_q \bX+\bE_q, \qquad (q=1,2,\ldots,Q) \ee or
%equivalently
%\be
%y_{itq}= \sum_{j} a_{ij} x_{jt} d_{qj}+e_{itq} \ee
%$\bY_q= \bY_{:,:,q} =[y_{itq}]_{I_q \times T} \in \Real_+^{I_q \times T}$
%are frontal slices of $\underline \bY \in \Real_+^{I \times T \times
%Q}$, $Q$ is the number of frontal slices, $\bA_q=[a_{ijq}]_{I \times
%J} \in \Real_+^{I \times J}$ are the basis (mixing matrices),
%$\bD_q \in \Real_+^{J \times J}$ is a diagonal matrix that holds the $q$-th
%row of the $\bC \in \Real_+^{Q \times J}$ in its main diagonal and
%$\bX =[x_{jt}]_{J \times T} \in \Real_+^{J \times T}$ is a matrix
%representing latent sources (or hidden components or common factors), and
%
%
%
%\begin{figure}[ht]
%\begin{center}
%\includegraphics[width=13.2cm,height=5.9cm]{Parafac2C}\\
%\vspace{0.9cm}
%\includegraphics[width=13.2cm,height=2.7cm]{Parafac2bNC}
%\end{center}
%%\vspace{-0.5cm}
% \caption{(a) NTF2 model in which a 3D tensor is
%decomposed into a set of non-negative matrices:
%$\{\bA_1,\ldots,\bA_Q\}, \; \bD, \; \bX$. (b) Equivalent
%representation in which frontal
% slices of a tensor are factored by a set
%of matrices (tensor $\underline \bE$ representing error is omitted
%for simplicity). } \label{Fig4-chap7}
%\end{figure}


%We use the following notation: the $jt$-th
%element of the matrix $\bX$ is denoted by $y_{it}$,
%$x_{itq}=[\bX_q]_{it}$ means the $it$-th element of the $q$-th
%frontal slice $\bX_q$,  $\; \bar \bA_c= [\bA_1;\bA_2;\ldots; \bA_Q] \in
%\Real_+^{QI \times J}$ is a column-wise unfolded matrix of the slices
%$\bA_q$, $\bar a_{pj} = [\bar \bA_c]_{pj}$. Similarly, $\bar \bX_c=
%[\bX_1;\bX_2;\ldots; \bX_Q] \in \Real_+^{QI \times T}$ is the
%column-wise unfolded matrix of the slices $\bX_q$, $\bar x_{jt} =
%[\bar \bX_c]_{jt}$ (see Figure  \ref{Fig5-chap7}).
%
%\begin{figure}[ht]
%\begin{center}
%\includegraphics[width=11.2cm,height=5.9cm]{BMCR_ALS1R_c}\\
%\end{center}
%\vspace{-0.5cm}
% \caption{Column-wise unfolding of the 3D  NTF2 model  which  is
%simplified to 2D NMF model.} \label{Fig5-chap7}
%\end{figure}

%All the NTF models (i.e., NTF, NTF1 and NTF2) have  unique solutions
%(neglecting intrinsic scaling and permutation ambiguities) if the
%representations are sufficiently sparse or smooth.

\subsection{Individual Differences in Scaling (INDSCAL) and
Implicit Slice Canonical Decomposition Model (IMCAND)}


Individual Differences in Scaling (\inx{INDSCAL}) was proposed
by Carroll and Chang \cite{Carroll_Chang} in the same paper in which they introduced
CANDECOMP, and is a special case of the three-way PARAFAC
 for third-order  tensors that are symmetric in two modes.
%
    %
%\begin{table}[ht]
%\caption{Basic Description of PARAFAC (CP) and NTF Family Models.}
%\centering
%%\renewcommand{\arraystretch}{1.}
%%
%%\setlength{\tabcolsep}{0.8cm}
%%    \begin{center}
%  \shadingbox{
%    \begin{tabular*}{\textwidth}[t]{@{\extracolsep{\fill}}ll}  \hline   \\
%{ \raisebox{0mm}[0mm][4mm]{Model}}
%& Description  \\ \hline
% &   \\
%     Nonnegative PARAFAC (NTF)
%                             &
%                                    $y_{itq} = \sum\limits_{j=1}^{J} {a_{ij} \; b_{tj} \; c_{qj}} + e_{itq}$ \\
%              &                    $  \bY_q = \bA \bD_q \bB^T + \bE_q = \sum\limits_{j=1}^{J} {c_{qj} \ba_{j} \; \bb_{j}  + \bE_{q}} $
%                 \\ &    \\ \hline  &   \\
%    {NTF1}  &  $ y_{itq} = \sum\limits_{j=1}^{J} {a_{ij} \; b_{tjq} \; c_{qj}} + e_{itq} $ \\
%               &                    $ \bY_{q} = \bA \; \bD_q \; \bB_q^T + \bE_q = \sum\limits_{j=1}^{J} {c_{qj} \; \ba_{j} \; (\bb^{(q)}_{j})^T} + \bE_q$
%                        \\ &    \\ \hline  &   \\
%    {NTF2} & $ y_{itq} = \sum\limits_{j=1}^{J} {a_{ijq} \; b_{tj} \; c_{qj}} + e_{itq}$\\
%    & $\bY_{q} = \bA_q \; \bD_q \; \bB^T + \bE_q = \sum\limits_{j=1}^{J} {c_{qj} \; \ba^{(q)}_{j} \; \bb_{j}^T} + \bE_q $  \\ &  \\ \hline  &   \\
%    Shifted NTF (S-NTF)
%      & $ y_{itq} = \sum\limits_{j=1}^{J} {a_{(i+s_{tj})j} \; b_{tj} \; c_{qj}} + e_{itq} $
%                            \\ &   \\ \hline  &   \\
%    Convolutive NTF  (cNTF)
%      &  $ y_{itq} = \sum\limits_{j=1}^{J} {a_{ij} \; b_{(t-s)j} \; c^{s}_{qj}} + e_{itq} $
%                        \\ &      \\ \hline  &   \\
%    INDSCAL &   $y_{itq} = \sum\limits_{j=1}^{J} {a_{ij}^2  \; c_{qj}} + e_{itq}$
%                          \\ &    \\ \hline
%    \end{tabular*}
%    }
%\label{table-NTF12}
%\end{table}

The INDSCAL imposes the constraint that the first two factor matrices in the decomposition are
 the same, that is,
 %
 \be
  \hlbox{
 \underline \bY \cong \sum_{j=1}^J \ba_j  \circ \ba_j  \circ \bc_j,}
\ee
or equivalently
\be
 \hlbox{
 \underline
\bY \cong \underline \bI \times_1 \bA  \times_2 \bA \times_3 \bC,
}
 \ee
where $ \underline \bY \in \Real^{ I \times  I \times Q}$ with
$y_{itq} = y_{tiq}, i = 1,\ldots,I, t = 1,\ldots,I, q = 1,\ldots,Q$, and $\underline \bI$ is the cubic identity tensor.
The goal is to find
 an optimal  solution for matrices $\bA$ and $\bC$, subject to the
nonnegativity and sparsity constraints.

In data mining applications  third-order input tensors $\underline \bY \in
\Real^{I \times I \times Q}$ may have a special form where each
frontal slice $\bY_q$  is the product of two
matrices which are typically the matrix $\bX_q \in \Real^{I \times T}$ and
its transpose $\bX_q^T$, thus yielding
%
\be \bY_q  =\bX_q \bX_q^T, \qquad
(q=1,2, \ldots, Q). \ee
%
Such a model is called the IMplicit Slice Canonical
Decomposition\inxx{Implicit Slice Canonical Decomposition}
(\inx{IMSCAND}) Model (see Figure  \ref{Fig-IMSCAND}) where
the PARAFAC or Tucker decompositions of the tensor
$\underline \bY$ are performed  implicitly, that is,
by using  matrices $\bX_q$  and not directly the elements  $y_{itq}$ (which do not need to be  stored on computer) \cite{SeleeIMSCAND}.
%
\begin{figure}[t!]
\centering
    \psfrag{W}{}
    \psfrag{T}[bc][bl]{\color{black}
    \setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-1ex}\scriptsize $T$\end{tabular}}
    \psfrag{I}[bl][bl]{\color{black}
    \setlength{\tabcolsep}{0pt}\begin{tabular}{c}\hspace{-.5ex}\scriptsize $(I \times I \times 6)$\end{tabular}}%}
    \psfrag{i}[bc][bl]{}
    \psfrag{6l}[bc][bl]{}
    \psfrag{1}[bl][bl]{\color{black} \tiny 1}
    \psfrag{2}[bl][bl]{\color{black} \tiny2}
    \psfrag{3}[bl][bl]{\color{black} \tiny3}
    \psfrag{4}[bl][bl]{\color{black} \tiny4}
    \psfrag{5}[bl][bl]{\color{black} \tiny5}
    \psfrag{6}[bl][bl]{\color{black} \tiny6}
    \includegraphics[width=11.7 cm,height=6.8cm,trim = 0 0cm 0 0,clip =true]{IMSCAND_c_min_fix}

   \caption{Illustration of IMplicit Slice Canonical Decomposition (IMSCAND).
    The frontal slices $\bY_q$ are not stored directly but rather represented by a set of matrices $\bX_q$ as $\bY_q=\bX_q \bX_q^T$ for $q=1,2, \ldots, 6$.}
    \label{Fig-IMSCAND}
    \end{figure}

For example,  these slice matrices may represent covariance matrices
in signal processing, whereas in text mining (clustering of scientific
publications from a set of SIAM journals) slices $\bY_q$  are document by document matrices and may have
the following meanings \cite{SeleeIMSCAND}:

\begin{itemize}

\item $\bY_1 = $ similarity between  names of authors,

\item $\bY_2 = $ similarity between words in the abstract,

\item $\bY_3 = $ similarity between author-specified keywords,

\item $\bY_4 = $ similarity between titles,

\item $\bY_5 = $ co-citation information,

\item $\bY_6 = $ co-reference information.

\end{itemize}
%
The first four slices are formed from feature-document matrices for
the specified similarity. If there exists no similarity between two documents, then
the corresponding element in a slice is nonzero.  For the
fifth slice, the element $y_{it5}$ indicates the number of papers
that both documents $i$ and $t$ cite. Whereas the element $y_{it6}$
on the sixth slice is the number of papers cited by both documents $i$ and $t$.


\subsection{Shifted PARAFAC and Convolutive NTF}

\minrowclearance 2ex
\begin{table}[p]
\caption{Basic description of PARAFAC (CP) and NTF  (if we impose additional nonnegativity constraints) family models. Some models are expressed in matrix and/or scalar notations to make it easier understand the differences and compare them with standard PARAFAC. For Shifted NTF (S-NTF),
$s_{qj}$ represents the shift at column $q$ for the $j$-th factor. For Convolutive NTF (CNTF), $s$ is used usually to capture the shifts in the frequency spectrogram.}
\centering
  \shadingbox{
    \begin{tabular*}{\textwidth}[t]{@{\extracolsep{\fill}}ll}  \hline
\multicolumn{1}{c}{Model}
& \multicolumn{1}{c}{Description} \\[1ex] \hline
     Nonnegative PARAFAC (NTF)
                             &
                             \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                                $y_{itq}$ &=& \sum\limits_{j=1}^{J} {a_{ij} \; b_{tj} \; c_{qj}} + e_{itq} &\\
                                $\bY_q$ &=& \bA \bD_q \bB^T + \bE_q = \sum\limits_{j=1}^{J} {c_{qj} \ba_{j} \; \bb^T_{j}  + \bE_{q}}
                            \end{equationarray*}
                        \end{minipage}
            \\[6.2ex] \hline
    {NTF1}  &   \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-0.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                                $ y_{itq}$ &=& \sum\limits_{j=1}^{J} {a_{ij} \; b_{tjq} \; c_{qj}} + e_{itq}  &\\
                                $ \bY_{q}$ &=& \bA \; \bD_q \; \bB_q^T + \bE_q = \sum\limits_{j=1}^{J} {c_{qj} \; \ba_{j} \; (\bb^{(q)}_{j})^T} + \bE_q
                            \end{equationarray*}
                        \end{minipage}
            \\[6.2ex] \hline
%
    {NTF2} &\begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-0.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                               $ y_{itq}$ &=& \sum\limits_{j=1}^{J} {a_{ijq} \; b_{tj} \; c_{qj}} + e_{itq}&\\
                               $\bY_{q}$ &=& \bA_q \; \bD_q \; \bB^T + \bE_q = \sum\limits_{j=1}^{J} {c_{qj} \; \ba^{(q)}_{j} \; \bb_{j}^T} + \bE_q
                            \end{equationarray*}
                        \end{minipage}
            \\[6.2ex] \hline
    Shifted NTF (S-NTF) &
            \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                               $ y_{itq}$&=& \sum\limits_{j=1}^{J} {a_{ij} \; b_{(t+s_{qj})j} \; c_{qj}} + e_{itq} & \\%\\[0pt] \noalign {\vskip-16pt}
                               $\bY_{q}$ &=& \bA \; \bD_{q} \; \mathcal{S}_{s_q}(\bB)^T + \bE_q
                            \end{equationarray*}
                        \end{minipage}
            \\[4ex] \hline
    Convolutive NTF  (CNTF) &
            \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                               $y_{itq}$ &=& \sum_{j=1}^{J}\sum_{s=1}^{S} {a_{ij} \; b_{(t-s+1)j} \; c_{qjs}} + e_{itq} &\\
                               $\bY_q$ &=& \bA \sum_{s=1}^{S}{\bD^{(s)}_{q} \, (\bT_{\uparrow (s-1)}\bB)^T} + \bE_q
%                               \\[0pt] \noalign {\vskip-16pt}
%                               \\%\\[0pt] \noalign {\vskip-16pt}
%                                $\bY_{q}$ &=& \sum_{s=0}^{min(S,t)}\bT_{\uparrow s}\,\bA \; \bD_q \; \bB^T + \bE_q
                            \end{equationarray*}
                        \end{minipage}
            \\[6.1ex] \hline
    C2NTF   &
            \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                               $y_{itq}$ &=& \sum_{j=1}^{J}\sum_{s=1}^{S}\sum_{r=1}^{R}{a_{ij} \, b_{(t-s+1)jr} \, c_{(q-r+1)js}} + e_{itq}&
                               \\[0pt] \noalign {\vskip-16pt}
                            \end{equationarray*}
                        \end{minipage}
            \\[3ex] \hline
    INDSCAL &
                        \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{3ex}llp{8cm}}
                                $y_{itq}$ &=& \sum\limits_{j=1}^{J} {a_{ij} \; a_{tj}  \; c_{qj}} + e_{itq} & \\[0pt] \noalign {\vskip-16pt}
                            \end{equationarray*}
                        \end{minipage}
            \\[3ex] \hline
    \end{tabular*}
    }
\label{table-NTF12}
\end{table}
\minrowclearance 0pt

Harshman {\it et al.} \cite{Harsman03} introduced the shifted PARAFAC (S-PARAFAC)
in order to deal with shift factors in sequential data such as time series or spectra data.
For example, the S-PARAFAC for mode-2 can be described for each entry $y_{itq}$  as
\be
	\hlbox{y_{itq} = \sum_{j=1}^{J}{a_{ij} \, b_{(t+s_{qj})j} \, c_{qj}} + e_{itq},}
\ee
where the shift parameter $s_{qj}$ gives the shift at column $j$ of the factor $\bB$.
We can rewrite this model for frontal slices
\be	
	\bY_q = \bA \, \bD_{q} \, \mathcal{S}_{\bs_q}(\bB)^T + \bE_q, \qquad (q = 1,\ldots,Q),
\ee
where the shift operator (or function) $\mathcal{S}_{\bs_q}(\bB)$  shifts all elements in each column of the matrix $\bB$ by amount $s_{qj}$.
The vector $\bs_q$  is a vector of $J$ shift values taken from row $q$ of the (implied) shift matrix $\bS \in \Real^{Q \times J}$,  and the matrix $\bD_{q}$ is a diagonal matrix containing the $q$-th row of $\bC$.
One limitation of S-PARAFAC is that it only considers one-dimensional shifts, typically time, but does not handle two-dimensional shifts that might be encountered in neuroimages of brain scans \cite{Acar2008},\cite{Moruptech}

Another extension of PARAFAC is Convolutive PARAFAC (CPARAFAC or CNTF)
which is a generalization of CNMF to multiway spectral data.
Morup and Schmidt \cite{Moruptech} introduced this model with the
name Sparse Nonnegative Tensor 2D Deconvolution (SNTF2D).
The single convolutive NTF on mode-2 and mode-3 and with rank-$J$ for the nonnegative tensor $\underline \bY \in \Real_+^{I \times T \times Q}$
returns a factor matrix $\bA \in \Real_+^{I \times J}$ on the first dimension,
a factor matrix $\bB \in \Real_+^{T \times J}$ on the second dimension,
and a set of $S$ factor matrices or tensor $\underline \bC \in \Real_+^{Q \times J \times S}$, and can be expressed as follows
\be
    \hlbox{y_{itq} = \sum_{j=1}^{J}\sum_{s=1}^{S}{a_{ij} \, b_{(t-s+1)j} \, c_{qjs} + e_{itq}}.}
\ee
For $S=1$, CPARAFAC (CNTF) simplifies to PARAFAC (NTF).
%When convolutive filter $\underline \bC$ is sparse, CPARAFAC becomes Shifted PARAFAC.
Matrix representation of this model via frontal slices $\bY_q, \;\; (q = 1,\ldots,Q) \;$ is given by
\begin{equationarray}{lllll}
    \bY_q &\cong& \bA \sum_{s=0}^{S-1}{\bD^{(s+1)}_{q} \, (\bT_{\uparrow (s)}\bB)^T}
          &=& \bA \sum_{s=0}^{S-1}{\bD^{(s+1)}_{q} \, \bB^T \bT_{\mathop {s}\limits_ \to}}
\end{equationarray}
where $\bD^{(s)}_{q}$ is a diagonal matrix containing the fiber $\bc_{q:s}$,
and the shift operators $\bT_{\uparrow (s)}$, $\bT_{\mathop {s}\limits_ \to}$ are defined in Section 1.2.12.
%Equations (\ref{equ_CNMF_transpose1}), (\ref{equ_shiftmatrix}).
In the tensor form the CNTF can be described as
\be
    \underline \bY = \sum_{s=1}^{S} \underline \bI \times_1 \bA \times_2 \bT_{\uparrow (s-1)} \bB \times_3 \bC_s + \underline \bE.
    \label{CNTF11}
\ee
The CPARAFAC can be extended to the double convolutive model (C2PARAFAC or C2NTF) as
\be
    \hlbox{y_{itq} \cong \sum_{j=1}^{J}\sum_{s=1}^{S}\sum_{r=1}^{R}{a_{ij} \, b_{(t-s+1)jr} \, c_{(q-r+1)js}},}
\ee
where the second factor in Equation (\ref{CNTF11}) is no longer a matrix but a tensor of size $(T \times J \times R)$.
%
%\clearpage
%\newpage



%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC03bisb_c.eps}
%\end{center}
%\caption{The standard CP (CANDECOMP-PARAFAC) with the super diagonal $\underline \bG=  \underline \bI$  equals one}
%\label{fig-Parafac}
%\end{figure}

%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{AY-matri-CPcolorbis_Ub.eps}
%\end{center}
%\caption{The n-th-order CP (CANDECOMP-PARAFAC) with the super-diagonal $\underline \bG=  \underline \bI$  equals one}
%\label{fig-Parafac}
%\end{figure}

%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.8cm]{AYmatri-CPcolorbis_U.eps}
%\end{center}
%\caption{The  n-th order CP (CANDECOMP-PARAFAC) with the super diagonal $\underline \bG=  \underline \bI$  equals one}
%\label{fig-Parafac-U}
%\end{figure}
%
%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=8.8cm]{Kruskal_U_c.eps}
%\end{center}
%\caption{The  n-th order Kruskal CP (CANDECOMP-PARAFAC) with the super diagonal}
%\label{fig-Kruskal-U}
%\end{figure}
%

%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{Tucker3_U_c.eps}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC06bisUb_c}
%\end{center}
%\caption{The  n-th order Tucker3}
%\label{fig-Tucker3-U}
%\end{figure}


%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%\includegraphics[width=11.1 cm,height=4.3cm]{ALSPARAFAC06bis_c}\\
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC03_c.eps}\\
%\end{center}
%\caption{Tucker3 model and special cases Kruskal model with super-diagonal kernel $\underline \bG= \underline \bD \in \Real^{J \times J \times J}$ and  standard CP (PARAFAC) with the super diagonal $\underline \bG=  \underline \bI$  equals one}
%\label{fig12-chap1}
%\end{figure}
%

%\clearpage
%\newpage
%
%\begin{figure}
%\begin{center}
%\includegraphics[width=11.1 cm,height=4.3cm]{Tucker3_U_c}
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC03_c.eps}\\
%\end{center}
%\caption{N-th order Tucker 3 model }
%\label{Fig-Tucker3U}
%\end{figure}


\subsection{Nonnegative Tucker Decompositions}



The Tucker  decomposition\inxx{Tucker decomposition},  also called the Tucker3 or  best rank ($J,R,P$)
approximation, can be formulated as follows{\footnote{The Tucker3 model with orthogonal factors is also known as three-way PCA (principal component analysis). The model can be naturally extended to $N$-way Tucker decomposition of arbitrary $N$-th order tensor.}} \cite{tucker64extension},\cite{Tucker66}:

%\begin{quotation}
Given a third-order data tensor $\underline \bY \in \Real^{I \times T \times Q}$ and three positive indices
 $\{J,R,P\} << \{I,T,Q\}$, find a core tensor $\bG=[g_{jrp}] \in \Real^{J \times R \times P}$  and three component matrices called factor or loading matrices or factors: $\bA=[\ba_1,\ba_2, \ldots, \ba_J] \in \Real^{I \times J}, \; \bB=[\bb_1,\bb_2, \ldots, \bb_R] \in \Real^{T \times R}$,  and $\bC=[\bc_1,\bc_2, \ldots, \bc_P] \in \Real^{Q \times P}$, which perform the following approximate decomposition:
 \be
 \hlbox{
 \underline \bY = \sum_{j=1}^J \sum_{r=1}^R \sum_{p=1}^P g_{jrp} \; (\ba_j \circ \bb_r \circ \bc_p) + \underline  \bE
 }
 \ee
 or equivalently in the element-wise form
 \be
  \hlbox{
  y_{itq}= \sum_{j=1}^J \sum_{r=1}^R \sum_{p=1}^P g_{jrp} \; a_{ij} \; b_{tr} \; c_{qp} + e_{itq},
  }
 \ee
 where  $ \ba_j \in \Real^I$,   $\bb_j \in \Real^T$,  and  $\bc_j \in \Real^Q$, (that is, the vectors within the associated  component (factor) matrices
 $\bA,\bB$ and $\bC$,),
  and $g_{jrp}$ are scaling factors which are the entries of a  core tensor $\underline \bG =[g_{jrp}] \in \Real^{J \times R \times P}$.

\begin{figure}[t]
\centering
\psfrag{t}[bc][bc]{\color{black}\small $T$}
\psfrag{x}[bc][bc]{\color{black}\small $\times$}
\includegraphics[width=\textwidth]{ALSPARAFAC06bisb_c_fix}
%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC03_c.eps}\\
\caption{Tucker3 model is a weighted sum of the outer product of three vectors (factors) stored as columns of component matrices $\bA \in \Real^{I \times J}, \bB=\bX^T \in \Real^{T \times R}$
and $\bC \in \Real^{Q \times P}$. The core tensor $\underline \bG \in \Real^{J \times R \times P}$ defines a linking structure between the set of components and $J$,  $\; R$, and $P$ denote the number of components. In order to achieve uniqueness for  the Tucker models it is necessary to impose additional constraints such as sparsity and nonnegativity.}
\label{fig-Tucker3}
\end{figure}

   The original Tucker model  makes the assumption of orthogonality of the factor matrices (in analogy to SVD),
 \cite{Kiers-Smilde},\cite{bro_book},\cite{Kim_Cic_CHoi_alpha_NTD},\cite{KimChoiNTD2007},\cite{Phan-HALS-NTD},\cite{Morup-NC08}. We will, however,  ignore these
constraints.
By imposing nonnegativity constraints the problem of
estimating the component matrices  and  a core tensor is converted into a generalized
NMF problem  called the \inx{Nonnegative Tucker Decomposition} (\inx{NTD}) (see Chapter \ref{Ch7} for details).
 The first implementations of Tucker decomposition with nonnegativity constraints together  with a number of other constraints were given by Kiers, Smilde and Bro in \cite{Kiers-Smilde},\cite{bro_book}.
  The NTD  imposes  nonnegativity constraints for all component matrices and a core tensor, while  a semi-NTD  (in analogy to semi-NMF) imposes nonnegativity constraints to only some components matrices and/or some elements of the core tensor.

\minrowclearance 2ex
\begin{table}[t!]
\caption{Formulations of the Tucker3 model\inxx{Tucker3}
for a third-order tensor $\underline \bY \in \Real^{I \times T \times Q}$ with the core
tensor $\underline \bG \in \Real^{J \times R \times P}$ and the
factor matrices: $\bA=[\ba_1,\ba_2, \ldots, \ba_J] \in \Real^{I \times J}, \;  \bB=[\bb_1,\bb_2,\ldots,\bb_R] \in \Real^{T
\times R}$, and $\bC =[\bc_1,\bc_2,\ldots,\bc_P] \in \Real^{Q \times P}$.}
\centering
\setlength{\tabcolsep}{0.8cm}
\shadingbox{
    \begin{tabular*}{\textwidth}[t]{@{\extracolsep{\fill}}ll} \hline
    \multicolumn{1}{c}{Operator} & \multicolumn{1}{c}{Mathematical Formula}   \\[1ex] \hline
    Outer product  &
                     \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{6ex}llp{4cm}}
                                \underline \bY &= & \displaystyle\sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R}\sum\limits_{p=1}^{P} {g_{jrp} \; \ba_{j} \circ \bb_{r} \circ \bc_{p}} + \underline \bE
                                &\\\noalign{\vskip-16pt}
                            \end{equationarray*}
                        \end{minipage} \\[3ex]  \hline
    Scalar  &   \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{6ex}llp{4cm}}
                                $y_{itq}$ &= &\sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R}\sum\limits_{p=1}^{P} {g_{jrp} \; a_{ij} \; b_{tr} \; c_{qp}} + e_{itq} &\\\noalign{\vskip-16pt}
                            \end{equationarray*}
                        \end{minipage} \\[3ex]  \hline
    Mode-$n$ multiplications & \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{6ex}llp{4cm}}
                                \underline \bY &=& \underline \bG \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE &\\\noalign{\vskip-16pt}
                            \end{equationarray*}
                        \end{minipage} \\[1ex]  \hline
    {Slice representation} &
                            \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-1.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 1ex
                           \begin{equationarray*}{p{6ex}llp{4cm}}
                                $\bY_q$ &=& \bA  \bH_q \bB^T + \bE_q, \quad (q = 1,2,\ldots,Q) & \\
                                $\bH_q$ &=& \displaystyle\sum\limits_{p=1}^{P}{c_{qp} \bG_p}, \quad \bG_p \stackrel{\triangle}{=} \bG_{::p} \in \Real^{J \times R}
                            \end{equationarray*}
                        \end{minipage}
            \\[4.3ex] \hline
    Vector & \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-.8ex
                           \belowdisplayskip=0pt
                           \minrowclearance 0ex
                           \begin{equationarray*}{p{6ex}llp{4cm}}
                                $\text{vec}(\underline \bY)$ &=& \text{vec}(\bY_{(1)}) \cong (\bC \otimes \bB \otimes \bA) \;\text{vec}(\underline \bG) &\\\noalign{\vskip-16pt}
                            \end{equationarray*}
                        \end{minipage} \\[1ex]  \hline
    {Kronecker product} &
                           \begin{minipage}[c]{.52\textwidth}
                           \abovedisplayskip=-1.3ex
                           \belowdisplayskip=0pt
                           \minrowclearance .5ex
                           \begin{equationarray*}{p{6ex}llp{4cm}}
                                $\bY_{(1)}$ &\cong& \bA \;\bG_{(1)} \;(\bC \otimes \bB)^T&\\
                                $\bY_{(2)}$ &\cong& \bB \;\bG_{(2)} \;(\bC \otimes \bA)^T&\\
                                $\bY_{(3)}$ &\cong& \bC \;\bG_{(3)} \;(\bB \otimes \bA)^T
                            \end{equationarray*}
                        \end{minipage}
                  \\[3.5ex] \hline
    \end{tabular*}
    }
\label{table-Tucker3}
\end{table}
\minrowclearance 0ex

\begin{figure}[t!]
\centering
%\subcapraggedrighttrue
%\subcapnoonelinetrue
%\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\psfrag{g}[bc][bc]{}
\psfrag{a}[bc][bc]{}
\psfrag{b}[bc][bc]{}
\psfrag{c}[bc][bc]{}
\psfrag{i}[bc][bc]{}
\psfrag{j}[bc][bc]{}
\psfrag{r}[bc][bc]{}
\psfrag{t}[bc][bc]{}
\psfrag{p}[bc][bc]{}
\psfrag{p=}[bc][bc]{}
\psfrag{qp}[bc][bc]{}
\psfrag{q}[bc][bc]{}
\psfrag{1}[bc][bc]{}
\psfrag{=}[bc][bc]{}
\psfrag{333}[bc][bc]{}
\psfrag{33}[bc][bc]{}
\psfrag{3}[bc][bc]{}
\psfrag{y1}[bl][br]{}%
\psfrag{y2}[bl][br]{}%
\subfigure[Tucker3]{
%\psfrag{y1}[bl][br]{\color{black}
%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-8pt}\hspace{2ex}
%\large $y_{itq} = \displaystyle\sum_{j = 1}^{J}\sum_{r = 1}^{R}
%\sum_{p = 1}^{P} g_{jrp} \, a_{ij} \, b_{tr}\, c_{qp}$
%\end{tabular}}%
\begin{minipage}[c]{1\textwidth}
\begin{minipage}[c]{.38\textwidth}
\large $y_{itq} = \displaystyle\sum_{j = 1}^{J}\sum_{r = 1}^{R}
\sum_{p = 1}^{P} g_{jrp} \, a_{ij} \, b_{tr}\, c_{qp}$
\end{minipage}
\begin{minipage}[c]{.6\textwidth}
\psfrag{x1}[bc][bc]{\color{black}\small$\times$}
\includegraphics[width=1\textwidth,trim = 3.5cm 19.5cm 0cm 0, clip = true]{tuckermodelsb_c_fix}
\end{minipage}
\end{minipage}
\label{Fig-Tucker321_a}
}
\subfigure[Tucker2]{
%\psfrag{y2}[bl][br]{\color{black}
%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-8pt}\hspace{2ex}
%\large $y_{itq} = \displaystyle\sum_{j = 1}^{J}\sum_{r = 1}^{R} g_{jrq} \, a_{ij} \, b_{tr}$
%\end{tabular}}%
\begin{minipage}[c]{1\textwidth}
\begin{minipage}[c]{.38\textwidth}
\large $y_{itq} = \displaystyle\sum_{j = 1}^{J}\sum_{r = 1}^{R} g_{jrq} \, a_{ij} \, b_{tr}$
\end{minipage}
\begin{minipage}[c]{.6\textwidth}
\psfrag{x2}[bc][bc]{\color{black}\small$\times$}
\includegraphics[width=\textwidth,trim = 3.5cm 10.5cm 0cm 8.5cm, clip = true]{tuckermodelsb_c_fix}
\end{minipage}
\end{minipage}
\label{Fig-Tucker321_b}}
\subfigure[Tucker1]{
%\psfrag{y3}[bl][br]{\color{black}
%\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-8pt}\hspace{2ex}
%\large $y_{itq} = \displaystyle\sum_{j = 1}^{J} g_{jtq} \, a_{ij}$
%\end{tabular}}%
\begin{minipage}[c]{1\textwidth}
\begin{minipage}[c]{.38\textwidth}
\large $y_{itq} = \displaystyle\sum_{j = 1}^{J} g_{jtq} \, a_{ij}$
\end{minipage}
\begin{minipage}[c]{.6\textwidth}
\psfrag{x3}[bc][bc]{\color{black}\small$\times$}
\includegraphics[width=1\textwidth,trim = 3.5cm 2cm 0cm 17.5cm, clip = true]{tuckermodelsb_c_fix}
\end{minipage}
\end{minipage}
\label{Fig-Tucker321_c}}
%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%\vspace{1.5cm}
%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC03_c.eps}\\
%
\caption{Summary  of the three related Tucker decompositions.}
\label{Fig-Tucker321}
\end{figure}

There are several equivalent  mathematical descriptions for the Tucker model (see Table \ref{table-Tucker3}).
%
It can be expressed in a compact matrix form using mode-$n$ multiplications
 \be
\hlbox{
 \underline \bY = \underline \bG \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE
 = \llbracket \underline \bG; \bA,\bB, \bC \rrbracket + \underline \bE,
}
 \ee
 where $\widehat {\underline \bY} = \llbracket \underline \bG; \bA,\bB, \bC \rrbracket$
 is the shorthand notation for the Tucker3 tensor decomposition.

Using the unfolding approach we can obtain  matrix forms expressed compactly by the Kronecker products:
\minrowclearance 5pt
\arrayrulecolor{bordercolor}
\begin{equationarray}{| fF Nl
>{\columncolor{highlightcolor}[.2\tabcolsep][.8\tabcolsep]}rNl
>{\columncolor{highlightcolor}[.2\tabcolsep][.8\tabcolsep]}rk|}
\cline{1-5}
& \bY_{(1)} &\cong& \bA \; \bG_{(1)} \; (\bC \otimes \bB)^T,&\\
& \bY_{(2)} &\cong& \bB \; \bG_{(2)} \; (\bC \otimes \bA)^T,&\\
& \bY_{(3)} &\cong& \bC \; \bG_{(3)} \; (\bB \otimes \bA)^T.&
\\[1ex]
\cline{1-5} \noalign {\vskip-16pt}  \arrayrulecolor{black}
\nonumber
\end{equationarray}
\minrowclearance 0pt
%
It is  often convenient to  represent the three-way Tucker model in its vectorized forms
\minrowclearance 4pt
\arrayrulecolor{bordercolor}
\begin{equationarray}{|f>{\columncolor{highlightcolor}[1.19ex][.5\tabcolsep]}l Nl
>{\columncolor{highlightcolor}[.4\tabcolsep][.8\tabcolsep]}rNl
>{\columncolor{highlightcolor}[.4\tabcolsep][.8\tabcolsep]}lNl
>{\columncolor{highlightcolor}[.4\tabcolsep][1.2ex]}rk|}
\cline{1-7}
 &\text{vec} \; (\bY_{(1)}) &\cong & \text{vec} \; (\bA \bG_{(1)}  (\bC \otimes \bB)^T)
&=& (\bC \otimes \bB) \otimes \bA  \; \text{vec} \; (\bG_{(1)}),&\\
&\text{vec} \; (\bY_{(2)}) &\cong& \text{vec} \; (\bB  \bG_{(2)}  (\bC \otimes \bA)^T)
& =& (\bC \otimes \bA) \otimes \bB \; \text{vec} \; (\bG_{(2)}),&\\
&\text{vec} \; (\bY_{(3)}) &\cong& \text{vec} \; (\bC \bG_{(3)}  (\bB \otimes \bA)^T)
&=& (\bB \otimes \bA) \otimes \bC \; \text{vec} \;(\bG_{(3)}).&
\\[1.01ex]
\cline{1-7} \noalign {\vskip-16pt} \arrayrulecolor{black}
\nonumber
\end{equationarray}
\minrowclearance 0pt
%
%
The Tucker model described above is often called the Tucker3 model
because  a third-order tensor is decomposed into three factor
(loading) matrices (say, $\{\bA, \bB, \bC\}$) and a core tensor
$\underline \bG$. In  applications  where we have   two factor
matrices or even only one, the Tucker3 model for a three-way tensor
simplifies into the Tucker2 or Tucker1 models (see Table
\ref{table-Tuckers}). The \inx{Tucker2} model can be obtained from the
Tucker3 model by absorbing one factor by a core tensor
(see Figure  \ref{Fig-Tucker321_b}), that is,
%
\be
 \hlbox{
 \underline \bY \cong \underline \bG
\times_1 \bA \times_2 \bB.
}
 \ee
%
For the \inx{Tucker1} model we have only one
  factor matrix (while two others are absorbed by a core tensor) which is described as
   (see also  Figure  \ref{Fig-Tucker321_c})
%
\be
 \hlbox{
\underline \bY \cong \underline \bG \times_1 \bA.
}
\ee
%
%\begin{table}[ht]
%\caption{Tucker models for a third-order tensor $\underline \bY \in \Real^{I \times T \times Q}$.}
%\renewcommand{\arraystretch}{2.0}
%\setlength{\tabcolsep}{0.3mm}
%    \begin{center}
%      \shadingbox{
%    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}l@{\ \ \ \ \  }l@{\ \ \ \ \  }l} \hline
%    Model & &  Description \\ \hline
%   & & $ y_{itq} = \sum\limits_{j=1}^{J} {g_{jtq} \; a_{ij} } + e_{itq}$ \\
%         Tucker1 & &   $\underline \bY = \underline \bG  \times_1 \bA + \underline \bE, \qquad (\underline \bG \in \Real^{J \times T \times Q})$\\
%           &&         $\bY_q = \bA \bG_{q} + \bE_{q}, \quad (q = 1,2, \ldots, Q)$
%                                  \\
%             &&                     $\bY_{(1)} = \bA \bG_{(1)} + \bE_{(1)}$  \\ \hline
%    %
%   && $ y_{itq} = \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R} \; {g_{jrq} \; a_{ij} \; b_{tr}} + e_{itq}$ \\
%    Tucker2 && $\underline \bY = \underline \bG  \times_1 \bA \times_2 \bB + \underline \bE, \qquad (\underline \bG \in \Real^{J \times R \times Q})$ \\
%    && $\underline \bY = \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R} {\ba_{j} \circ \bb_{r}}\circ \bg_{jr} + \underline \bE$ \\
%      &&    $\bY_{(3)} = \bG_{(3)} \; (\bB \otimes \bA)^T + \bE_{(3)}$  \\ \hline
%     && \\
%    Tucker3&& $y_{itq} = \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R}\sum\limits_{p=1}^{P} {g_{jrp} \; a_{ij} \;b_{tr} \; c_{qp}} + e_{itq} $  \\ && $\underline \bY = \underline \bG  \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE, \qquad (\underline \bG \in \Real^{J \times R \times P})$  \\ &&\\   \hline
%    && \\
%    Shifted Tucker3 &&   $y_{itq} = \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R}\sum\limits_{p=1}^{P} {g_{jrp} \; a_{(i+s_{tj})j} \;b_{tr} \; c_{qp}} + e_{itq} $ \\
%                             \\ \hline
%    \end{tabular*}
%    }
%    \end{center}
%    \label{table-Tuckers}
%\end{table}


\minrowclearance 0pt
\begin{table}[t!]
\caption{Tucker models for a third-order tensor $\underline \bY \in \Real^{I \times T \times Q}$.}
%\renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{0.3mm}
    \minrowclearance 2ex
    \begin{center}
      \shadingbox{
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ll}
    \hline
    \multicolumn{1}{c}{Model} &   \multicolumn{1}{c}{Description} \\[1ex] \hline
    Tucker1 &
             \begin{minipage}[c]{.6\textwidth}
             \abovedisplayskip=0pt
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \vspace{-1ex}
               \begin{equationarray*}{lllp{8cm}}
                     y_{itq} &=& \sum\limits_{j=1}^{J} {g_{jtq} \; a_{ij} } + e_{itq} &\\
                     \underline \bY &=& \underline \bG  \times_1 \bA + \underline \bE, \qquad (\underline \bG \in \Real^{J \times T \times Q})\\
                    \bY_q &=& \bA \bG_{q} + \bE_{q}, \quad (q = 1,2, \ldots, Q)\\
                    \bY_{(1)} &=& \bA \bG_{(1)} + \bE_{(1)}
                \end{equationarray*}
            \end{minipage}  \\[7.5ex] \hline
    %
    Tucker2      &
            \begin{minipage}[c]{.6\textwidth}
               \abovedisplayskip=0pt
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \vspace{-1ex}
               \begin{equationarray*}{lllp{8cm}}
                    y_{itq} &=& \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R} \; {g_{jrq} \; a_{ij} \; b_{tr}} + e_{itq} \\
                    \underline \bY &=& \underline \bG  \times_1 \bA \times_2 \bB + \underline \bE, \qquad (\underline \bG \in \Real^{J \times R \times Q}) &\\
                    \underline \bY &=& \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R} {\ba_{j} \circ \bb_{r}}\circ \bg_{jr} + \underline \bE &\\
                   \bY_{(3)} &=& \bG_{(3)} \; (\bB \otimes \bA)^T + \bE_{(3)}
                \end{equationarray*}
            \end{minipage}  \\[10ex] \hline
   Tucker3 & \begin{minipage}[c]{.6\textwidth}
               \abovedisplayskip=0pt
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \vspace{-1ex}
               \begin{equationarray*}{@{\hspace{1.2ex}}lllp{7cm}}
                   y_{itq} &=& \sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R}\sum\limits_{p=1}^{P} {g_{jrp} \; a_{ij} \;b_{tr} \; c_{qp}} + e_{itq} &  \\
                   \underline \bY &=& \underline \bG  \times_1 \bA \times_2 \bB \times_3 \bC + \underline \bE, \qquad (\underline \bG \in \Real^{J \times R \times P})
                \end{equationarray*}
            \end{minipage}  \\[5ex] \hline

   Shifted Tucker3 &
           \begin{minipage}[c]{.6\textwidth}
               \abovedisplayskip=0pt
               \belowdisplayskip=0pt
               \minrowclearance .5ex
               \vspace{-1ex}
               \begin{equationarray*}{@{\hspace{1.2ex}}lllp{7cm}}
                   y_{itq} &=& \displaystyle\sum\limits_{j=1}^{J}\sum\limits_{r=1}^{R}\sum\limits_{p=1}^{P} {g_{jrp} \; a_{(i+s_{tj})j} \;b_{tr} \; c_{qp}} + e_{itq}&\\\noalign {\vskip-16pt}
                \end{equationarray*}
            \end{minipage}  \\[4ex] \hline
    \end{tabular*}
    }
   \end{center}
    \label{table-Tuckers}
    \minrowclearance 0ex
\end{table}


\begin{table}[t!] %[ht]
\caption{Matrix and tensor representations for various factorization models
(for  most of the  models we impose additional nonnegativity constraints).}
\renewcommand{\arraystretch}{2.0}
\setlength{\tabcolsep}{0.2mm}
    \begin{center}
      \shadingbox{
    \begin{tabular*}{1.05\textwidth}{@{\extracolsep{\fill}}l@{ }l@{ }l} \hline
    Model & Matrix Representation  & Tensor Representation  \\ \hline
   NMF     &   $\bY \cong \bA \;\bX = \bA \; \bB^T$
   & $\bY \cong \bI \times_1 \bA \times_2 \bX^T$ \\ \hline
    SVD     &   $ \bY \cong \bU \; {\mbi \Sigma} \;\bV^T$ & $ \bY \cong {\mbi \Sigma \times_1 \bU \times_2 \bV}$ \\
    &      $ \quad = \displaystyle \sum\limits_{r=1}^{R}{\sigma_r \; \bu_r \bv_r^T} $ & $ \quad = \displaystyle \sum\limits_{r=1}^{R}{\sigma_r \; \bu_r \circ \bv_r}$ \\ \hline
    Three-factor NMF & $\bY \cong \bA \; \bS \;\bX = \bA \; \bS \;\bB^T$
    & $\bY \cong \bS \times_1 \bA \times_2 \bX^T$ \\ \hline
    NTF
     &  $\bY_q \cong \bA \; \bD_q(\bc_{q:}) \;\bB^T$ & $ \underline \bY \cong \underline \bI \times_1 \bA \times_2 \bB \times_3 \bC$ \\
      (nonnegative PARAFAC)  &                 $(q = 1,2,\ldots, Q)$ & $ \quad= \displaystyle \sum\limits_{j=1}^{J}{\ba_j \circ \bb_j \circ \bc_j}$ \\ \hline
    NTF1
     &   $\bY_q \cong \bA \; \bD_q(\bc_{q:}) \;\bB_q^T =\bA \; \bD_q(\bc_{q:}) \;\bX_q$ \\
      &                   $(q = 1,2,\ldots, Q)$
                    & $\underline \bY \cong \underline \bX \times_1 \bA \times_3 \bC$ \\ \hline
    NTF2             &
                         $\bY_q \cong \bA_q \; \bD_q(\bc_{q:}) \; \bB^T = \bA_q \; \bD_q(\bc_{q:}) \; \bX$ \\
      &                   $(q = 1,2,\ldots, Q)$
                    & $\underline \bY \cong \underline \bA \times_2 \bB \times_3 \bC$  \\ \hline
     Tucker1             &
                         $\bY_q \cong \bA \; \bG_{\, ::q}$ &
                     $\underline \bY = \underline \bG \times_1 \bA$ \\
                     &  $(q = 1,2,\ldots, Q)$ \\ \hline
     Tucker2        &   $\bY_q \cong \bA \; \bG_{\, ::q} \; \bB^T$ & $ \underline \bY \cong \underline \bG \times_1 \bA \times_2 \bB$  \\
       &                  $(q = 1,2,\ldots, Q)$ &
                     $ \quad = \displaystyle \sum\limits_{j=1}^{J} \displaystyle \sum\limits_{r=1}^{R}{\ba_j \circ \bb_r \circ \bg_{jr}} $ \\ \hline
    Tucker3        &  $ \bY_q \cong \bA \bH_q \bB^T$ & $\underline \bY \cong \underline \bG \times_1 \bA \times_2 \bB \times_2 \bC $ \\
       &                 $\bH_q = \displaystyle \sum\limits_{p=1}^{P} \bc_{qp} \; \bG_{\, ::p}$ &
                         $\quad = \displaystyle  \sum\limits_{j=1}^{J} \sum\limits_{r=1}^{R} \displaystyle \sum\limits_{p=1}^{P}{g_{jrp} \, (\ba_j \circ \bb_r \circ \bc_p)}$ \\ \hline
    \end{tabular*}
    }
    \end{center}
    \label{table-tensor-reprsent}
\end{table}
It is interesting to note that the approximation of a tensor by factor matrices
and a core tensor often helps  to simplify  mathematical operations and  reduce the computation cost of some operations in multi-linear (tensor) algebra. For example:
\be
\underline \bY = \underline \bX \; \bar \times_3  \; \ba &\approx& (\underline \bG \times_1 \bA \times_2 \bB \times_3 \bC) \; \bar \times_3 \; \ba \nonumber \\
&=& (\underline \bG \; {\bar
\times_3} \bC^T \ba) \times_1 \bA \times_2 \bB \nonumber \\
&=& \underline \bG_{\;Ca} \times_1 \bA \times_2 \bB, \nonumber
\ee
where $\underline \bG_{\;Ca}= \underline \bG \;\bar \times_3 \; \bC^T \ba$. Comparison of tensor decomposition models are summarized in Table 1.6.

%In order to estimate the norm of a large-scale tensor we can use the
%following transformations:
%%
% \be
%||\underline \bY||^2_F &\approx & || [\underline \bLambda; \bA, \bB, \bC ] ||^2_F\nonumber \\
%&=& ||(\bC \odot \bB \odot \bA) \mbi \lambda||^2_F \nonumber \\
%&=& \mbi \lambda^T (\bC \odot \bB \odot \bA)^{T}(\bC \odot \bB \odot \bA) \; \mbi \lambda \nonumber \\
%&=& \mbi \lambda^T  (\bC^T \bC \* \bB^T \bB \* \bA^T \bA) \; \mbi \lambda,  \nonumber
%\ee
%where $\underline \bLambda \in \Real^{ J \times J \times J}$ is a ``superdiagonal" core cube tensor  with nonzero superdiagonal elements $\{\lambda_1, \lambda_2, \ldots, \lambda_J\}$
% and $ \mbi \lambda = [\lambda_1, \lambda_2, \ldots, \lambda_J]^T$.

%\subsubsection{Higher Order Singular Value Decomposition}
%
%The HOSVD can be considered as a special case of the Tucker model:
%
%
%TODO
%% Anh Huy Phan on 14/04/2009

\subsection{Block Component Decompositions}\label{subsec:BCD}

\begin{figure}[ht]
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
%\subfigure[]{
%\psfrag{1}[bc][bc]{\color{black}\footnotesize 1}%
%\psfrag{c}[bc][bc]{\color{black} $\bc$}%
%\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}%
%\psfrag{R}[bc][bc]{\color{black}\footnotesize $R$}%
%\psfrag{L}[bc][bc]{\color{black}\footnotesize $L$}%
%\psfrag{1i}[bc][bc]{\color{black}\tiny$1$}%
%\psfrag{Rs}[bc][bc]{\color{black}\tiny $R$}%
%\includegraphics[height=3.7cm]{BCM-mode3_fix}\label{Fig-BCM_mode3}}
\subfigure[]{
\psfrag{1}[bc][bc]{\color{black}\footnotesize 1}%
\psfrag{c}[bc][bc]{\color{black} $\bc$}%
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}%
\psfrag{R}[bc][bc]{\color{black}\footnotesize $R$}%
\psfrag{L}[bc][bc]{\color{black}\footnotesize $R$}%
\psfrag{1i}[bc][bc]{\color{black}\tiny$1$}%
\psfrag{Rs}[bc][bc]{\color{black}\tiny $R$}%
\includegraphics[width=\textwidth,height=3.7cm]{BCM-JJ1_fix2}\label{Fig-BCM_a}}
%\includegraphics[width=13.1 cm,height=4.1cm]{BCM-LPOb_L}
\subfigure[]{
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}%
\psfrag{1}[bc][bc]{\color{black}\footnotesize$1$}%
\includegraphics[width=\textwidth,height=4.1cm]{BCM-LP_fix2}\label{Fig-BCM_b}}
\subfigure[]{
\psfrag{x}[bc][bc]{\color{black}\footnotesize $\times$}%
%\vspace{1.5cm}
\includegraphics[width=\textwidth,height=4.1cm]{BCM-JSP_fix}\label{Fig-BCM_c}}
\caption{Block Component Decompositions (BCD) for a third-order tensor $\underline \bY \in \Real^{I \times T \times Q}$:
%\subref{Fig-BCM_mode3} BCD rank-$R$ mode-3 with $R$ factors $A_r$ explain the data along the 3-rd dimension,
 \subref{Fig-BCM_a} BCD with rank-$(J_r,1)$, \subref{Fig-BCM_b} BCD with rank -$(J,P)$ and \subref{Fig-BCM_c} BCD with rank-$(J,S,P)$.}
\label{Fig-BCM}
\end{figure}

Block Component Decompositions\inxx{Block Components Decompositions}  (BCDs)\inxx{BCD}  (also called \inx{Block Component Models}) introduced by De Lathauwer and Nion for
 applications in signal processing and wireless communications \cite{Lath-BCM},\cite{Lath-BCM2},\cite{Nion-Sp},\cite{Nion-Lath-IEEE},\cite{Lath-Nion-BCM3}
 can be considered as a sum of
 basic subtensor decompositions (see Figure  \ref{Fig-BCM}).
Each basic subtensor in this sum has  the same kind of factorization or decomposition, typically, Tucker2 or  Tucker3 decomposition,
and the corresponding components have a similar structure (regarding dimensions, sparsity profile and nonnegativity constraints) as illustrated in Figures \ref{Fig-BCM} (a), (b) and (c).

%The simplest BCD model is that for a three-way tensor $\underline \bY\, \in \Real^{I \times T \times Q}$, we need to express this tensor
%as a sum of $R$ sub-tensors $\underline \bY^{(r)} \in \Real^{I \times T \times Q}, r= 1,\ldots,R$ composed by one core matrix $\bA_{r} \in \Real^{I \times T}$
%which is a frontal slice of the tensor $\underline \bA \in \Real^{I \times T \times R}$,
%and one factor vector $\bc_r $ which is the $r$-th column of the matrix $\bC\in \Real^{Q \times R}$ (see Figure  \ref{Fig-BCM_mode3}).
%\be
%    \hlbox{\underline \bY = \sum_{r = 1}^{R} {\underline \bY^{(r)}} =
%                   \sum_{r = 1}^{R} {\bA_r \circ \bc_r}.} \label{equ_BCD_Tucker1}
%\ee
%The purpose is to find a tensor $\underline \bA \in \Real^{I \times T \times Q}$, and factor matrix $\bC \in \Real^{Q \times R}$.
%We note that the frontal slice $\bA_r$ can be considered as a third-order tensor $\underline \bA_r$
%whose 3rd dimension is 1, its matricized versions in three modes are given by
%\begin{equationarray}{lll}
%	\left[ \underline \bA_r\right]_{(1)} &=& \left[ \underline \bA_r\right]_{(2)}^T = \bA_r, \\
%	\left[ \underline \bA_r\right]_{(3)} &=& \vtr{\bA_r}^T.
%\end{equationarray}
%The mode-3 product of the tensor $ \underline \bA_r$ with a vector $\bc_r$
%is exactly the outer product of  $\bA_r$ and $\bc_r$.
%\be
%	 \underline \bA_r  \times_3  \bc_r =   \bA_r \circ \bc_r.
%\ee
%As a result, the BCD model described by Eq. (\ref{equ_BCD_Tucker1}) can be represented in an equivalent form as
%\be
%   \hlbox{ \underline \bY = \sum_{r = 1}^{R} {\underline \bA_r \times_3 \bc_r}.} \label{equ_BCD_Tucker1_2}
%\ee
%This model also has an equivalent matrix factorization form by using the mode-3 matricization approach
%\minrowclearance 2pt
%\begin{equationarray}{rcl cl}
%    \bY_{(3)} &=& \sum_{r = 1}^{R} {\bc_r {\bA_r}_{(3)}} = \sum_{r = 1}^{R} {\bc_r \vtr{\bA_r}^T} \nonumber \\
%              &=& \bC \, \left[\begin{array}{cccc} \vtr{\bA_1} &\vtr{\bA_2} &\cdots &\vtr{\bA_R} \end{array}\right]^T
%              &=&\hlbox{\bC \, \bA_{(3)}.}
%\end{equationarray}
%\minrowclearance 0pt
%With this result, this BCD decomposition of the tensor $\underline \bY$ is exactly the matrix factorization with rank-$R$ of the mode-3 matricization of this tensor  $\bY_{(3)}$ into two factors $\bC$ and the mode-3 matricization $\bA_{(3)}$ of tensor $\underline \bA$,
%and this kind of tensor decomposition is called as the BCD rank-$R$ mode-3 because factors $\bA_{r}$ explain the tensor
%$\underline \bY$ along its mode-3.

The model shown in Figure  \ref{Fig-BCM_a},  called the BCD rank-$(J_r,1)$, decomposes a data tensor  $\underline \bY \in \Real^{I \times T \times Q}$ into a sum of $R$ subtensors $\underline \bY^{(r)} \in \Real^{I \times T \times Q}, \;\;(r = 1,2,\ldots, R)$. Each of the subtensor $\underline \bY^{(r)}$ is factorized into three factors $\bA_r \in \Real^{I \times J_r}$, $\bB_r \in \Real^{T \times J_r }$ and $\bc_r \in \Real^{Q}$.
The mathematical description of this  BCD model is given by
\be
 \hlbox{
\underline \bY =  \sum_{r=1}^R \underline \bY^{(r)} +  \underline \bE = \sum_{r=1}^R (\bA_r \bB_r^T) \circ \bc_r  + \underline \bE,
}
\ee
or equivalently
\begin{equationarray}{rcl}
\underline \bY  &= & \sum_{r=1}^R \underline \bA_r \times_2 \bB_r \times_3  \bc_r
+ \underline \bE,
\end{equationarray}
where tensors $\underline \bA_r \stackrel{\triangle}{=} \bA_r \in \Real^{I \times J_r \times 1}$ are three-way tensors with only one frontal slice.
With this notation, each subtensor $\underline \bY^{(r)}$ is a Tucker2 model with the core tensor $\underline \bA_{(r)}$, and factors $\bB_r$ and $\bc_r$.
Hence, the BCD rank-$(J_r, 1)$ decomposition can be considered as  a sum of simplified Tucker-2 models.
%
The objective is to estimate  component matrices $\bA_r \in \Real^{I \times J_r}$,
$\bB_r  \in \Real^{T \times J_r}$, $\;\; (r=1,2,\ldots,R)$ and
a factor matrix $\bC = [\bc_1, \bc_2, \ldots, \bc_R] \in \Real^{Q \times R}$ subject to optional nonnegativity and sparsity constraints.

Using the unfolding approach, the above BCD model can be
written in several equivalent forms:
%
\minrowclearance 4pt
\begin{equationarray}{rclcl}
 \bY_{(1)} &\cong& \sum_{r= 1}^{R} {\left[\underline \bA_r\right]_{(1)} \left(\bc_r\otimes  \bB_r \right)^T}
	&=&  \sum_{r= 1}^{R} { \bA_r\left(\bc_r\otimes  \bB_r \right)^T} \nonumber\\[2ex]
	&=& \multicolumn{3}{l}{\hlbox{\left[\begin{array}{cccc}\bA_{1} & \bA_{2}& \cdots & \bA_{R} \end{array}\right] \;
        \left[ \begin{array}{cccc} \bc_1\otimes \bB_1 & \bc_2\otimes \bB_2 &
            \cdots & \bc_R \otimes \bB_R
            \end{array}\right]^T,}}
\end{equationarray}
\begin{equationarray}{rclcl}
%
 \bY_{(2)} &\cong& \sum_{r= 1}^{R} { \bB_r \left[\underline \bA_r\right]_{(2)}  \left(\bc_r \otimes \bI_{I} \right)^T}
	&=&  \sum_{r= 1}^{R} { \bB_r \bA_r^T  \left(\bc_r \otimes \bI_{I} \right)^T}  \nonumber\\[1ex]
	&=&  \sum_{r= 1}^{R} { \bB_r \left[\left(\bc_r \otimes \bI_{I} \right) \bA_r \right]^T}
	&=& \sum_{r= 1}^{R} { \bB_r \left(\bc_r \otimes \bA_r \right)^T} \nonumber\\[1ex]
	&=& \multicolumn{3}{l}{\hlbox{\left[
    \begin{array}{cccc} \bB_{1}& \bB_{2}& \cdots& \bB_{R}
    \end{array}
    \right] \;  \left[
    \begin{array}{cccc}
    \bc_1 \otimes \bA_1 & \bc_2 \otimes \bA_2&
        \cdots & \bc_R \otimes \bA_R
        \end{array}\right]^T,}}\\[2ex]
%
%
 \bY_{(3)} &\cong& \sum_{r= 1}^{R} { \bc_r \left[\underline \bA_r\right]_{(3)}  \left(\bB_r \otimes \bI_{I} \right)^T}
	&=&  \sum_{r= 1}^{R} { \bc_r  \vtr{\bA_r}^T  \left(\bB_r \otimes \bI_{I} \right)^T}   \nonumber\\
	&=& \sum_{r= 1}^{R} { \bc_r  \left[ \left(\bB_r \otimes \bI_{I} \right) \vtr{\bA_r}\right]^T}
	&=& \sum_{r= 1}^{R} { \bc_r  \vtr{\bA_r \bB_r^T}^T}   \nonumber\\
	&=& \multicolumn{3}{l}{\hlbox{
            \bC \left[
            \begin{array}{cccc} \vtr{\bA_1 \bB_1^T} &  \vtr{\bA_2 \bB_2^T} & \cdots &  \vtr{\bA_R \bB_R^T}
        \end{array}
        \right]^T.}}
\end{equationarray}
\minrowclearance 0pt
%\be
%\underline \bY_{(2)} &=& \bB \left(\bc_1 \otimes \bA_1 \cdots  \bc_R \otimes \bA_R\right)^T, \\
%\underline \bY_{(3)} &=& \bC [\mbox{vec}(\bA_1 \bX_1) \cdots \mbox{vec} (\bA_R \bX_R)]^T \nonumber \\
%&=& \bC \left((\bA_1 \odot \bB_1) \1_{J_1} \cdots (\bA_R \odot \bB_R) \1_{J_R} \right)^T,
%\ee
%where $\1_{J_r}$ is a column vectors of ones of length  $J_r$.
%where $\bA=[\bA_1, \bA_2, \ldots, \bA_L] \in \Real^{I \times JL}$, $\bX= [\bX_1, \bX_2, \ldots, \bX_L] \in \Real^{J \times JL}$ and $\bC=[\bc_1, \bc_2, \ldots, \bc_L] \in \Real^{Q \times L}$ and $\bC \odot_L \bX$ is defined as follows
%\be
%\bC \odot_L \bX =\left[\bC_1 \otimes \bX_1|\bC_2 \otimes \bX_2| \cdots |\bC_L \otimes \bX_L \right]
%\ee
%
A simple and natural  extension of the model BCD rank-$(J_r,1)$ assumes that the tensors $\underline \bA_r \in \Real^{I \times J \times P}$ contains $P$ (instead of one) frontal slices of size $(I \times J)$ (see Figure  \ref{Fig-BCM_b}). This model is referred to as the BCD with rank-$(J,P)$ and is described as follows
\be
 \hlbox{
\underline \bY = \sum_{r=1}^R \left(\underline \bA_r \times_2 \bB_r \times_3  \bC_r \right) + \underline \bE.
}
\ee
The objective is to find a set of $R$ tensors $\underline \bA_r \in \Real^{I \times J \times P}$, a tensor $\underline \bB \in \Real^{T \times J \times R}$,
and a tensor $\underline \bC \in \Real^{Q \times P \times R}$.
By stacking tensors $\underline \bA_r$ along their third dimension, we form a common tensor $\underline \bA \in \Real^{I \times J \times PR}$.
The mode-1 matricization of this BCD model gives an equivalent matrix factorization model
%using concatenation of matrices and stacking and unfolding tensors
\cite{Nion-Sp},\cite{Lath-Nion-BCM3}:
%
\be
    \bY_{(1)} &\cong& \sum_{r=1}^R {\bA_r}_{(1)} \left(\bC_r \otimes \bB_r \right)^T \\
              &=& \hlbox{\bA_{(1)} \, \left[
              \begin{array}{cccc}
                \bC_1 \otimes \bB_1 & \bC_2 \otimes \bB_2& \cdots &\bC_R \otimes \bB_R
              \end{array}\right]^T.}
\ee
%
The most general, BCD rank-$(J,S,P)$ model is formulated as a sum of $R$ Tucker3 models of
corresponding factors $\bA_r \in \Real^{I \times J}$, $\bB_r \in \Real^{T \times S}$, $\bC_r \in \Real^{Q \times P}$
and core tensor $\underline \bG_r \in \Real^{J \times S \times P}$, and described
in a compact form as (see Figure  \ref{Fig-BCM_c}):
%
\be
 \hlbox{
\underline \bY = \sum_{r=1}^R \left( \underline  \bG_r \times_1
\bA_r \times_2 \bB_r \times_3  \bC_r \right) + \underline \bE.
}
\ee
This model can be also converted in a similar way to a matrix factorization model with set of constrained
component matrices.
%The above BCD decompositions  can be considered as natural
%extensions of the  Tucker models.
%Our objective is to estimate factor matrices and core tensors.
%Using concatenation of factor matrices  and unfolding of core tensors $\underline \bA_r$ or $\underline \bG_r$ we can convert  these  models to standard matrix factorization problems with suitable constraints  \cite{Lath-BCM,Lath-BCM2,Nion-Sp,Nion-Lath-IEEE}.


%or equivalently
%\be
%\underline \bY_{(1)} &=& \bA_{(1)} (\bC \dots_L \bX)^T, \\
%%\underline \bY_{(2)} &=& \bX^T (\bA \dots_L \bC)^T, \\
%%\underline \bY_{(3)} &=& \bC [\mbox{vec}(\bA_1 \bX_1) \cdots \mbox{vec} (\bA_L \bX_L)]^T,
%\ee
%where $\bA=[\bA_1, \bA_2, \ldots, \bA_L] \in \Real^{I \times JL}$, $\bX= [\bX_1, \bX_2, \ldots, \bX_L] \in \Real^{J \times JL}$ and $\bC=[\bc_1, \bc_2, \ldots, \bc_L] \in \Real^{Q \times L}$.
%Our objective is to estimate set of matrices $\{\bA_, \bB_l, \bc_l$, ($l=1,2,\ldots, L$) subject to additional constraints such as nonnegativity and sparsity.
%
%Using row-wise unfolding approach the above BCD model can be written in compact matrix forms
%\be
%\underline \bY_{(1)} &=& \bA (\bC \dots_L \bX)^T, \\
%\underline \bY_{(2)} &=& \bX^T (\bA \dots_L \bC)^T, \\
%\underline \bY_{(3)} &=& \bC [\mbox{vec}(\bA_1 \bX_1) \cdots \mbox{vec} (\bA_L \bX_L)]^T,
%\ee
%where $\bA=[\bA_1, \bA_2, \ldots, \bA_L] \in \Real^{I \times JL}$, $\bX= [\bX_1, \bX_2, \ldots, \bX_L] \in \Real^{J \times JL}$ and $\bC=[\bc_1, \bc_2, \ldots, \bc_L] \in \Real^{Q \times L}$.
%%

%\begin{figure}
%\begin{center}
%\includegraphics[width=14.1 cm,height=4.3cm]{BCM_c}
%%\includegraphics[width=11.1 cm,height=4.3cm]{NTFprez.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC_2.eps}\\
%%\vspace{1.5cm}
%%\includegraphics[width=13.6 cm,height=4.3cm]{ALSPARAFAC03_c.eps}\\
%\end{center}
%\caption{Block Tensor decomposition BCM}
%\label{fig13-chap1}
%\end{figure}
%
%
%\clearpage
%\newpage

%\subsection{Slice Oriented Decomposition}\label{subsec:SOD}
%
%An extension of the BCD model in the previous section is that an $N$-way data tensor $\underline \bY$ is decomposed into sum of $N$ BCD models in which each BCD model $\underline \bY^{(n)}$ expresses the data $\underline \bY$ in each mode $n$.
%For example, for a three-way tensor $\bY \in \Real^{I \times T \times Q}$, this tensor decomposition comprises 3 BCD models
%$\underline \bY^{(n)}, n = 1,2,3$. We apply the BCD rank-$R_n$ mode-$n$ model in (\ref{equ_BCD_Tucker1}),
% (\ref{equ_BCD_Tucker1_2}) and in Figure  \ref{Fig-BCM_mode3} for each element $\underline \bY^{(n)}$ described as follows
%%
%%
%%\minrowclearance 6pt
%%\begin{equationarray}{| fF NlNrNl  >{\columncolor{highlightcolor}[.6\tabcolsep][1.2ex]}rk|}
%%\cline{1-5}
%%&\fchar{\tiny\fchar}&\fchar&\fchararr{42ex}&\\[1ex]
%%\noalign {\vskip-16pt}
%%\tiny
%%\nonumber
%%\label{LS_alg_X7}
%%& \underline \bY &=& \sum_{n=1}^{3} \underline \bY^{(n)} \fchararr{35ex}&\\%= \underline \bY_F +\underline \bY_H +\underline \bY_L,
%%& \fchararr{.5ex}  &=& \sum_{r_1=1}^{R_1}{\underline \bH_{r_1::} \times_1 \ba_{r_1}}+
%%    \sum_{r_2=1}^{R_2}{\underline \bL_{:r_2:} \times_2 \bb_{r_2}}
%%    + \sum_{r_3=1}^{R_3}{\underline \bF_{::r_3} \times_3 \bc_{r_3}},&
%%\\[1ex]
%%\cline{1-5} \noalign {\vskip-16pt}
%%\nonumber
%%\end{equationarray}
%%\minrowclearance 0pt
%\be
%\hlbox{
% \underline \bY =
% 	\sum_{r_1=1}^{R_1}{\underline \bH_{r_1::} \times_1 \ba_{r_1}}+
%    	\sum_{r_2=1}^{R_2}{\underline \bL_{:r_2:} \times_2 \bb_{r_2}}
%    + \sum_{r_3=1}^{R_3}{\underline \bF_{::r_3} \times_3 \bc_{r_3}},}\label{LS_alg_X7}
%\ee
%%
%%
%%\begin{equationarray}{rcl}
%%    \underline \bY &=& \sum_{n=1}^{3} \underline \bY^{(n)} \\%= \underline \bY_F +\underline \bY_H +\underline \bY_L,
%%     &=& \sum_{r_1=1}^{R_1}{\underline \bH_{r_1::} \times_1 \ba_{r_1}}+
%%    \sum_{r_2=1}^{R_2}{\underline \bL_{:r_2:} \times_2 \bb_{r_2}}
%%    + \sum_{r_3=1}^{R_3}{\underline \bF_{::r_3} \times_3 \bc_{r_3}},
%%\end{equationarray}
%where $\underline\bH_{r_1::} \in \Real^{1 \times T \times Q}$ is the $r_1$-th horizontal slice of the tensor $\underline \bH \in \Real^{R_1 \times T \times Q}$,
% $\underline\bL_{:r_2:} \in \Real^{I \times 1 \times Q}$  is the $r_2$-th lateral slice of the tensor $\underline \bL \in \Real^{I \times R_2  \times Q}$,
% $\underline\bF_{::r_3} \in \Real^{I \times T \times 1}$ is the $r_3$-th frontal slice of the tensor $\underline \bF \in \Real^{I \times T \times R_3}$,
% $\bA \in \Real^{I \times R_1}$, $\bB \in \Real^{T \times R_2}$ and $\bC \in \Real^{Q \times R_3}$ are factors
% corresponding to subtensors $\underline \bY^{(1)}$, $\underline \bY^{(2)}$ and $\underline \bY^{(3)}$.
% The purpose is to find 3 tensors $\underline \bH$, $\underline \bL$, $\underline \bF$ and three corresponding factor matrices  $\bA$, $\bB$ and $\bC$.
%This  is another description for the model called the Slice Oriented Decomposition\inxx{Slice Oriented Decomposition} (\inx{SOD}) \cite{Caiafa-Cichocki}.
%This decomposition has the mode-$n$ matricizations as follows
%\minrowclearance 4pt
%\begin{equationarray}{rcl}
%\bY_{(1)} 	%&=& \sum_{n=1}^{N = 3}{\bY^{(n)}_{(1)}} \\
%		&=& \bA \, \bH_{(1)} + \bL_{(1)} \left( \bI_Q \otimes \bB \right)^T + \bF_{(1)} \, \left(\bC \otimes \bI_T \right)^T \nonumber\\
%		&=& \hlbox{ \left[\begin{array}{ccc}\bA & \bL_{(1)} & \bF_{(1)} \end{array}\right] \,
%		 \left[\begin{array}{ccc}\bH_{(1)} &  \bI_Q \otimes \bB^T &  \bC^T \otimes \bI_T \end{array}\right] , } \label{equ_SOD_mat1}\\
%%
%\bY_{(2)} 	%&=& \sum_{n=1}^{N = 3}{\bY^{(n)}_{(2)}} \\
%		&=& \bH_{(2)} \, \left(\bI_Q \otimes \bA \right)^T + \bB \, \bL_{(2)}  + \bF_{(2)} \, \left(\bC \otimes \bI_I \right)^T \nonumber\\
%		&=& \hlbox{ \left[\begin{array}{ccc}\bH_{(2)} & \bB  & \bF_{(2)} \end{array}\right] \,
%		 \left[\begin{array}{ccc}\bI_Q \otimes \bA^T &  \bL_{(2)} &  \bC^T \otimes \bI_I \end{array}\right], } \label{equ_SOD_mat2}\\		
%%
%\bY_{(3)} 	%&=& \sum_{n=1}^{N = 3}{\bY^{(n)}_{(3)}} \\
%		&=& \bH_{(3)} \, \left(\bI_T \otimes \bA \right)^T + \bL_{(3)} \left( \bB \otimes \bI_I\right)^T  + \bC \, \bF_{(3)} \nonumber\\
%		&=& \hlbox{ \left[\begin{array}{ccc}\bH_{(3)} & \bL_{(3)}  & \bC \end{array}\right] \,
%		 \left[\begin{array}{ccc}\bI_T \otimes \bA^T &   \bB^T \otimes \bI_I & \bF_{(3)}\end{array}\right]. } \label{equ_SOD_mat3}
%\end{equationarray}
%\minrowclearance 0pt
%This leads to the equivalent matrix forms of this tensor and allows us to easily estimate core tensors and factor matrices via matrix factorization.
%For example, from (\ref{equ_SOD_mat1}), factorization of the matricized version $\bY_{(1)}$
%into two factor matrices $\bU$ and $\bV$ with $R_1 + R_2 Q + R_3 T$ components with respect to additional constraints depending on the data such as sparsity, nonnegativity, \ldots
%\be
%	\bY_{(1)} = \bU \, \bV.
%\ee
%The $R_1$ first columns of the factor $\bU$ is the matrix $\bA$, the next $R_2 Q$  columns
%is $\bL_{(1)}$, and the rest is $\bF_{(1)}$, \ldots.
%If we process on the mode-2 matricized version $\bY_{(2)}$, the rank of the factorization model is $R_1 Q + R_2 + R_3 I$; whereas this value is $R_1 T + R_2 I + R_3$ when factorizing the matrix $\bY_{(3)}$.
%
%Algorithms of this model for nonnegative data can be derived directly from multiplicative NMF algorithm (see Chapter \ref{Ch3}).
%We note that NMF cannot resolve data biased by base lines which are the constant regions existing all data sources.
%The {\tt{swimmer}} dataset \cite{Donoho} is an example for such case.
%A ``torso'' of 12 pixels in the center of all 256 images (frontal slices) is the base line of this tensor data.
%Almost all standard NMF algorithms failed with this data set due to this strong overlapping pattern.
%An advantage of tensor decomposition is that we can factorize data in different modes.
%For a given offset region in the frontal slices, we usually can find another mode in which this region is not an offset one.\footnote{In a contrary case, all the elements of this tensor are identity.} Decomposition of this tensor on this mode will avoid the offset.
%Finally, combination of decompositions of this tensor on all the modes helps us resolve totally the data with offset. This leads to the SOD model described in (\ref{LS_alg_X7}).
%
%For this reason, SOD is also considered as a more general model than the affine NMF presented in section (\ref{}). SOD can resolve the offset degraded by flickering, occlusion of other offsets, or discontinuity.
%Chapter \ref{Ch7} will illustrate the advantages of this SOD model in comparison with matrix factorization.
%
%
%%% Anh Huy Phan on 14/04/2009
%
%\begin{figure}[t!]
%\centering
%    \psfrag{x}[bc][bc]{\color{black}\scriptsize $\times$}
%    \psfrag{(I)}[bc][bc]{\color{black}\scriptsize $(I)$}
%    \psfrag{(T)}[bc][bc]{\color{black}\scriptsize $(T)$}
%    \psfrag{P}[bc][bc]{}
%    \psfrag{R}[bc][bc]{}
%    \psfrag{p}[bc][bc]{}
%    \psfrag{r}[bc][bc]{}
%    \psfrag{k}[bc][bc]{}
%    \psfrag{1}[bc][bc]{}
%    \psfrag{k =}[bc][bc]{}
%    \psfrag{K}[bc][bc]{}
%    \psfrag{E}{\color{black} \large $\underline \bE$}
%    \psfrag{Y}{\color{black} \large $\underline \bY$}
%    \psfrag{H}{\color{black} \large $\bH_{r_1::}$}
%    \psfrag{L}[tc][tc]{\color{black} \large $\bL_{:r_2:}$}
%    \psfrag{F}{\color{black} \large $\bF_{::r_3}$}
%    \psfrag{cc}[bc][bc]{\color{black} \Large $=$}
%
%    \psfrag{S1}{\color{black} $\displaystyle\sum_{r_1=1}^{R_1}$}
%    \psfrag{S2}{\color{black} $\displaystyle\sum_{r_2=1}^{R_2}$}
%    \psfrag{S3}{\color{black} $\displaystyle\sum_{r_3=1}^{R_3}$}
%
%    \psfrag{#}[bc][bc]{}
%    \psfrag{a}[bl][bc]{\color{black} $\ba_{r_1}$}
%    \psfrag{b}[bc][bc]{\color{black} $\bb_{r_2}$}
%    \psfrag{c}[bc][bc]{\color{black} $\bc_{r_3}$}
%        \psfrag{r1}[bc][bc]{}
%        \psfrag{p1}[bc][bc]{}
%        \psfrag{k1}[bc][bc]{}
%
%    \includegraphics[width=9.5 cm,trim = 0 0cm 0 0cm,clip =true]{SOD-HLF_c_fix3}\\
%%    \hspace{0.1cm}
%%     \leavevmode
%%    \includegraphics[width=5.7cm,height=6cm] {x_reg_new_1}
%%   % \par
%   \caption{Illustration of  Slice Oriented Decomposition (SOD) for a third order tensor. The matrices and vectors are linked  via outer products which construct basic tensors.}
%    \label{Fig-SOD}
%    \end{figure}
%
%%In some applications  slices  of  3-way tensor (horizontal, vertical or
%%frontal) have a degree of similarity, for example, faces of the
%%same person  with different illuminations arranged as frontal slices. For such cases  Caiafa and Cichocki
%%developed a new tensor
%% decomposition called the Slice Oriented Decomposition\inxx{Slice Oriented Decomposition} (\inx{SOD}) \cite{Caiafa-Cichocki}, which
%% for a third-order tensor takes the
%% following form (see Figure  \ref{Fig-SOD}):
%% \be
%%  \hlbox{
%% \underline \bY = \sum_{p=1}^P  (\tilde \ba_p \circ_1 \bH_p) +
%% \sum_{r=1}^R   (\tilde \bb_r \circ_2 \bV_r)
%% + \sum_{k=1}^K  \; (\tilde \bc_k \circ_3 \bF_k)  + \underline \bE,
%% }
%%\ee where  $\bH \in \Real^{Q \times T}$, $\bL \in \Real^{I \times Q}$ and $\bF \in \Real^{I \times T}$  are matrices corresponding to horizontal, vertical (lateral) and frontal slices of data tensor which are linked via outer products with normalized vectors $\tilde \ba \in \Real^I$,  $\tilde \bb \in \Real^T$ and
%% $\tilde \bc \in \Real^Q$.
%%%the nonnegative coefficients $\alpha_p, \beta_r,
%%%\gamma_k$ and indices  $P,R,K$ may take zero values.
%%In order to avoid confusion (how basic tensors are constructed) we define outer products of a matrix by a vector for three modes as follows:
%%\be
%%\tilde \ba \circ_1 \bH &=& \underline \bH \in \Real^{I \times T \times Q} \quad \mbox{with entries} \quad h_{itq}= h_{qt} \; \tilde a_i, \\
%% \tilde \bb \circ_2 \bV &=& \underline \bL \in \Real^{I \times T \times Q} \quad \mbox{with entries} \quad l_{itq}= l_{iq} \; \tilde b_t, \\
%%\tilde \bc \circ_3 \bF &=& \underline \bF \in \Real^{I \times T \times Q} \quad \mbox{with entries} \quad f_{itq}= f_{it} \; \tilde c_q.
%%\ee
%%%
%%Note that each
%%factor $(\ba_p \circ \bH_p)$ builds up a simple third-order tensor of
%%dimension $I \times T \times Q$, whose rank depends on how many
%%bilinear components  can be represented by the matrix $\bH_p$,
%%i.e., $\bH_p =\sum_{j_p=1}^{J_p} \ba_{j_p} \bb^T_{j_p}$.
%%%
%%In many cases the indices $P, R,$ and $K$   have  relatively small values
%%if data have some kind of symmetrical structure or there exists
%%similarities between slices (in any direction). Our goal is to
%%find  sparse and compact representations, that is, to estimate the  nonnegative
%%matrices: $\bH_p \in \Real_+^{T \times Q}, \; \bL_r \in \Real_+^{I
%%\times Q}$ and $ \bF_k \in \Real_+^{I \times T}$, nonnegative vectors:
%%$\tilde \ba_p \in \Real_+^I, \; \tilde \bb_r \in \Real_+^T$ and $
%%\tilde \bc_k \in \Real_+^Q$.
%% This model can
%% be extended to the convolutive or shifted SOD.
%
%%In many cases the indices $P, R,$ and $K$   have  relatively small values
%%if data have some kind of symmetrical structure or there exists
%%similarities between slices (in any direction). Our goal is to
%%find  sparse and compact representations, that is, to estimate the  nonnegative
%%matrices: $\bH_p \in \Real_+^{T \times Q}, \; \bL_r \in \Real_+^{I
%%\times Q}$ and $ \bF_k \in \Real_+^{I \times T}$, nonnegative vectors:
%%$\tilde \ba_p \in \Real_+^I, \; \tilde \bb_r \in \Real_+^T$ and $
%%\tilde \bc_k \in \Real_+^Q$.

\subsection{Block-Oriented Decompositions}
\label{subsec:BOD}


A natural extension of the tensor decomposition models discussed in the previous sections  will be a
decomposition which  uses
sum of subtensors factorized the data tensor along different modes.
% or different orientations of slices.
Such decompositions will be referred to as Block-Oriented Decompositions\inxx{Block-Oriented Decomposition}  (BODs).
The key distinction between BOD and BCD  models is that subtensors in a BCD model attempt to explain the data tensor in the same modes while  BOD models  exploit  at least two up to all possible separate modes for each subtensors.
For example, using Tucker2 model the corresponding BOD2 model
can be formulated as follows
\be
\underline \bY \cong \underline \bG_1 \times_1 \bA_1 \times_2 \bB_1 + \underline \bG_2 \times_1 \bA_2 \times_3 \bC_1
+ \underline \bG_3 \times_2 \bB_2 \times_3 \bC_2,
\ee
%
where core tensors and factor matrices have suitable dimensions.

%Similar models can be defined based on restricted Tucker3 model and also PARATUCK model (see next section).
%is that an $N$-way data tensor $\underline \bY$ is decomposed into sum of $N$ subtensors $\underline \bY^{(n)}$ which express the data $\underline \bY$ along the modes $n$.
%For example, for a three-way tensor $\bY \in \Real^{I \times T \times Q}$, this tensor decomposition comprises 3 mode-$n$ Tucker1 models,
%$n = 1,2,3$, factorizing
%the data $\underline \bY$ in each mode: horizontal, lateral and frontal slices.
%%Each subtensor itself is a Tucker1 model of one core tensor and one factor.
%We call this decomposition the Block-Oriented Decomposition\inxx{Block-Oriented Decomposition}  (BOD).
%Illustration of this decomposition is given in Figure \ref{Fig-BOD}, whereas
%its mathematical expression is given by
Analogously, we can define a simpler BOD1 model which is based on the Tucker1 models (see Figure \ref{Fig-BOD}):
\be
\hlbox{
 \underline \bY \cong \underline \bH \times_1 \bA + \underline \bL \times_2 \bB +\underline \bF \times_3 \bC,}  \label{equ_BOD_tucker1}
\ee
where tensors $\underline \bH \in \Real^{R_1 \times T \times Q}$,
$\underline \bL \in \Real^{I \times R_2  \times Q}$,
$\underline \bF \in \Real^{I \times T \times R_3}$ are core tensors in the Tucker1 models with mode-$n$, $n =1,2,3$,
 $\bA \in \Real^{I \times R_1}$, $\bB \in \Real^{T \times R_2}$ and $\bC \in \Real^{Q \times R_3}$ are corresponding factors.
 The objective is  to find three core tensors $\underline \bH$, $\underline \bL$, $\underline \bF$ and three corresponding factor matrices  $\bA$, $\bB$ and $\bC$.
%
This model is also called the Slice Oriented Decomposition\inxx{Slice Oriented Decomposition} (\inx{SOD}) which was recently proposed and investigated by Caiafa  and Cichocki \cite{Caiafa-Cichocki}, and may have various mathematical and graphical representations.

\begin{rem1}
 The main motivation to use BOD1 model is to eliminate offset in a data tensor and provide unique and meaningful representation of the extracted components. The BOD1 can be considered as a generalization or extension of the affine NMF model presented in section (\ref{subsec:aNMF}).
As we will show in Chapter \ref{Ch7}  the BOD1  model can resolve the problem related with offset degraded by flicker, occlusion  or discontinuity.
\end{rem1}


\begin{figure}[t!]
\begin{center}
    \psfrag{IxTxQ}[tc][bc]{\color{black}\footnotesize $(I\times T \times Q)$}
    \psfrag{IxR1}[tc][bc]{\color{black} \footnotesize $(I \times R_1)$}
    \psfrag{R1xTxQ}[tl][bl]{\color{black} \footnotesize $(R_1 \times T \times Q)$}
    \psfrag{IxR2xQ}[tl][bl]{\color{black} \footnotesize $(I  \times R_2 \times Q)$}
    \psfrag{R2xT}[tc][bc]{\color{black} \footnotesize $(R_2 \times T)$}
    \psfrag{IxTxR3}[tc][bc]{\color{black} \footnotesize $(I \times T \times R_3)$}
    \psfrag{QxR3}[tc][bc]{\color{black} \footnotesize $(Q\times R_3)$}
    \psfrag{Y}{\color{black} \large $\underline \bY$}
    \psfrag{H}{\color{black} \large $\underline \bH$}
    \psfrag{L}{\color{black} \large $\underline \bL$}
    \psfrag{F}{\color{black} \large $\underline \bF$}
    \psfrag{cc}[bc][bc]{\color{black} \Large $=$}

    \psfrag{@}[bc][bc]{\color{black}  $\cong$}
    \psfrag{A}[bc][bc]{\color{black} \large $\bA$}
    \psfrag{C}[bl][bl]{\color{black} \large $\bC$}
    \psfrag{T}[bl][bl]{}
    \psfrag{X = B}[bl][bl]{\color{black} \large $\bX = \bB^T$}%
%    \psfrag{X = B}[bl][bl]{\color{black}
%    \setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-4pt}\hspace{-3pt}$\bX = \bB^T$\end{tabular}}%
    \includegraphics[width=1\textwidth,trim = 0 0cm 0 0cm,clip =true]{SOD_TUCKER_1}
\end{center}
   \caption{Illustration of the Block-Oriented Decomposition (BOD1) for a third-order tensor.
   Three Tucker1 models express the data tensor along each modes. Typically,  core tensors $\underline \bH \in \Real^{R_1 \times T \times Q}, \;\underline \bL \in \Real^{I \times R_2 \times Q}, \; \underline \bF \in \Real^{I \times T \times R_3} $ have much smaller dimensions than a data tensor $\underline \bY \in \Real^{I \times T \times Q}$, i.e., $R_1 <<I, \; R_2<<T$, and $R_3 <<Q$.}
    \label{Fig-BOD}
    \end{figure}

%  We remind that the mode-$n$ Tucker-1 model is exactly a matrix factorization of a three-way tensor along the mode-$n$.
% As the result, BOD is a combination of 3 matrix factorizations of the data tensor along all its three modes.
%The relations of BOD to matrix factorization can be expressed by the mode-$n$ matricizations

Using matricization approach we obtain for BOD1 model several equivalent matrix factorization models:
\minrowclearance 4pt
\begin{equationarray}{rcl}
\bY_{(1)} 	%&=& \sum_{n=1}^{N = 3}{\bY^{(n)}_{(1)}} \\
		&\cong& \bA \, \bH_{(1)} + \bL_{(1)} \left( \bI_Q \otimes \bB \right)^T + \bF_{(1)} \, \left(\bC \otimes \bI_T \right)^T \nonumber\\
		&=& \hlbox{ \left[\begin{array}{ccc}\bA & \bL_{(1)} & \bF_{(1)} \end{array}\right] \,
		 \left[\begin{array}{ccc}\bH_{(1)} &  \bI_Q \otimes \bB^T &  \bC^T \otimes \bI_T \end{array}\right] , } \label{equ_BOD_mat1}\\
%
\bY_{(2)} 	%&=& \sum_{n=1}^{N = 3}{\bY^{(n)}_{(2)}} \\
		&\cong& \bH_{(2)} \, \left(\bI_Q \otimes \bA \right)^T + \bB \, \bL_{(2)}  + \bF_{(2)} \, \left(\bC \otimes \bI_I \right)^T \nonumber\\
		&=& \hlbox{ \left[\begin{array}{ccc}\bH_{(2)} & \bB  & \bF_{(2)} \end{array}\right] \,
		 \left[\begin{array}{ccc}\bI_Q \otimes \bA^T &  \bL_{(2)} &  \bC^T \otimes \bI_I \end{array}\right], } \label{equ_BOD_mat2}\\		
%
\bY_{(3)} 	%&=& \sum_{n=1}^{N = 3}{\bY^{(n)}_{(3)}} \\
		&\cong& \bH_{(3)} \, \left(\bI_T \otimes \bA \right)^T + \bL_{(3)} \left( \bB \otimes \bI_I\right)^T  + \bC \, \bF_{(3)} \nonumber\\
		&=& \hlbox{ \left[\begin{array}{ccc}\bH_{(3)} & \bL_{(3)}  & \bC \end{array}\right] \,
		 \left[\begin{array}{ccc}\bI_T \otimes \bA^T &   \bB^T \otimes \bI_I & \bF_{(3)}\end{array}\right]. } \label{equ_BOD_mat3}
\end{equationarray}
\minrowclearance 0pt
These matrix representations allow us to compute core tensors and factor matrices via matrix factorization.
%For example, from (\ref{equ_BOD_mat1}), factorization of the matricized version $\bY_{(1)}$
%into two factor matrices $\bU$ and $\bV$ with $R_1 + R_2 Q + R_3 T$ components with respect to additional constraints depending on the data such as sparsity, nonnegativity
%\be
%	\bY_{(1)} = \bU \, \bV.
%\ee
%The $R_1$ first columns of the factor $\bU$ is the matrix $\bA$, the next $R_2 Q$  columns
%is $\bL_{(1)}$, and the rest is $\bF_{(1)}$, \ldots.
%If we process on the mode-2 matricized version $\bY_{(2)}$, the rank of the factorization model is $R_1 Q + R_2 + R_3 I$; whereas this value is $R_1 T + R_2 I + R_3$ when factorizing the matrix $\bY_{(3)}$.
%
%
%Algorithms of this model for nonnegative data can be derived directly from multiplicative NMF algorithm (see Chapter \ref{Ch3}).
%We note that NMF cannot resolve data biased by base lines which are the constant regions existing all data sources.
%The {\tt{swimmer}} dataset \cite{Donoho} is an example for such case.
%A ``torso'' of 12 pixels in the center of all 256 images (frontal slices) is the base line of this tensor data.
%Almost all standard NMF algorithms failed with this data set due to this strong overlapping pattern.
%An advantage of tensor decomposition is that we can factorize data in different modes.
%For a given offset region in the frontal slices, we usually can find another mode in which this region is not an offset one.\footnote{In a contrary case, all the elements of this tensor are identity.} Decomposition of this tensor along this mode will avoid the offset.
%Finally, combination of decompositions of this tensor along all the modes helps us resolve
%totally the data with offset. This leads to the BOD model described in (\ref{equ_BOD_tucker1}).


%Chapter \ref{Ch7} will illustrate the advantages of this BOD model in comparison with matrix factorization.

Similar, but more sophisticated BOD  models can be defined based on a restricted Tucker3 model \cite{Stegman-Almeida}
and also PARATUCK2 or DEDICOM models (see the next section).

%%% Anh Huy Phan on 14/04/2009

%\vspace{-0.4cm}

\subsection{PARATUCK2 and DEDICOM Models}


The \inx{PARATUCK2}, developed by Harshman and Lundy \cite{Harshman_book}
is a generalization of the PARAFAC model, that adds some
of the flexibility of Tucker2  model  while retaining
some of PARAFAC's uniqueness properties. The name PARATUCK2
indicates its similarity to both the PARAFAC and the Tucker2 model.
%
 The PARATUCK2 model performs decomposition
of an arbitrary third-order tensor (see Figure  \ref{Fig-dedicom3d_a}) $\underline \bY \in \Real^{I \times T \times Q}$  as follows
\be
 \hlbox{
\bY_q  =
\bA \; \bD_q^{(A)} \; \bR \; \bD_q^{(B)} \; \bB^T +\bE, \qquad
(q=1,2, \ldots, Q),
}
 \ee
 %
 where $\bA \in \Real^{I \times J}$,  $\bB
\in \Real^{T \times P}$, $\bR \in \Real^{ J \times P}$,
$\bD_q^{(A)} \in \Real^{J \times J}$  and $\bD_q^{(B)} \in \Real^{P
\times P}$ are  diagonal matrices representing the $q$-th frontal
slices of the tensors $\underline \bD^{(A)} \in \Real^{J \times J \times
Q}$, and $\underline \bD^{(B)} \in \Real^{P \times P \times Q}$,
respectively.
%
In fact, tensor $\underline \bD^{(A)}$ is formed by a matrix $\bU \in \Real^{J \times Q}$
whose columns are diagonals of the corresponding frontal slices
and tensor $\underline \bD^{(B)}$ is constructed from  a matrix $\bV \in \Real^{P \times Q}$.
\be
    \bD^{(A)}_q = \diag({\bu_q}), \quad \bD^{(B)}_q = \diag({\bv_q}).
\ee
The $j$-th row $\underline \bu_j$ (or $\underline \bv_j$) gives the weights of participation for
the corresponding component $\ba_j$ in the factor $\bA$ (or $\bb_j$ in $\bB$)
with respect to the third dimension.
%\end{quotation}
The terms $\bD^{(A)}_q \bR \bD^{(B) \, T}_q$
 correspond to frontal slices of the
core tensor $\underline \bG$ of a Tucker2 model, but due to the restricted structure of the core  tensor
compared to Tucker2 uniqueness is retained. The core tensor $\underline \bG$ can be described as
\minrowclearance 4pt
\begin{equationarray}{lllll}
    \vtr{\bG_{q}} &=& \vtr{\bD^{(A)}_q \bR \bD^{(B)}_q} &=& \vtr{\diag({\bu_q}) \bR \diag({\bv_q})^T} \nonumber\\
            &=& \left(\diag({\bv_q}) \otimes \diag({\bu_q}) \right)\, \vtr{\bR}
            &=& \diag\left(\bv_q \otimes \bu_q \right)\, \vtr{\bR} \nonumber\\
            &=& \left(\bv_q \otimes \bu_q \right) \* \vtr{\bR},
\end{equationarray}
\minrowclearance 0pt
or simply via frontal slices
\be
    \bG_{q} = \left(\bu_q \, \bv_q^T \right) \* \bR \,,\qquad (q=1,2,\ldots,Q).
\ee
This leads to the mode-3 matricization of the core tensor $\underline \bG$ having following form
\be
    \bG_{(3)} &=& \left[\vtr{\bG_{1}},  \ldots, \vtr{\bG_{R}} \right]^T \nonumber\\
            &=& \left[\left(\bv_1 \otimes \bu_1 \right) \* \vtr{\bR}, \ldots , \left(\bv_R \otimes \bu_R \right) \* \vtr{\bR} \right]^T \nonumber\\
            &=& \left[\bv_1 \otimes \bu_1 , \ldots , \bv_R \otimes \bu_R \right]^T \, \*
                \left[\vtr{\bR} , \ldots , \vtr{\bR}\right]^T \nonumber\\
            &=& \left(\bV \odot \bU\right)^T  \* {\bT}_{(3)} \nonumber\\
            &=& \bZ_{(3)} \* {\bT}_{(3)}
\ee
or
\be
    \underline \bG = \underline \bZ \* {\underline \bT}, \label{equ_PARATUCK_coretensor}
\ee
where $\underline \bZ \in \Real^{J \times P \times Q}$ is a rank-$Q$ PARAFAC tensor represented by two factors $\bU$ and $\bV$
(the third factor for the mode-3 is identity matrix $\bI_{Q}$), that is
\be
    \underline \bZ = \underline \bI \times_1 \bU \times_2 \bV,
\ee
and $\underline \bT \in \Real^{J \times P \times Q}$ is a tensor with identical frontal slices expressed by
 the matrix $\bR$: $\bT_q = \bR, \;\, \forall q$.
Equation (\ref{equ_PARATUCK_coretensor}) indicates that
the core tensor $\underline \bG$ is the Hadamard product of the PARAFAC tensor and a special (constrained) tensor with identity frontal slices.
%
In other words the PARATUCK2 can be considered
as the \inx{Tucker2} model in which the core tensor has special PARAFAC decomposition as illustrated
in Figure  \ref{Fig-dedicom3d_a}.  The PARATUCK2 model is well suited for a certain class of multi-way
problems that involve  interactions between factors.

%2-way DEDICOM (DEcomposition into DIrectional COMponents) model was  introduced by Harshman and Takane
%\cite{Takane1995}
%(see Figure  \ref{Fig-dedicom2d})
%\be
%\bY = \bA \bR \bA^T +\bE,
%\ee
%where $\bY \in \Real^{I \times I}$ is a given symmetric data matrix and  $\bE$ is a matrix representing error
%not explained by the model
%%
%The goal  is to estimate the
%best-fitting matrices:  $\bA \in \Real^{I \times J}$ and $\bR \in \Real^{J \times J}$.
%% such that the standard error cost function, as $\| \bE \|^2_F$,  is minimized.
%To achieve this goal we usually perform the following optimization problem:
%\be
%\min_{\bA,\bR} \; ||\bY - \bA \bR \bA^T||_F^2.
%\ee
%%
%%
%The  matrix $\bA \in \Real_+^{I \times J}$ comprises loadings or weights (with  $J <I$ ),
%and  the square matrix $\bR$ is a matrix that representing the asymmetric relationships
%for the latent dimensions  of $\bA$.
%
%\begin{figure}
%\begin{center}
%\includegraphics[scale = .7]{dedicom2D_c}
%%\includegraphics[scale = .7]{dedicom2D}
%\end{center}
%\caption{DEDICOM model for a square symmetric matrix. With nonnegative constraints it is equivalent to the symmetric NMF.}
%\label{Fig-dedicom2d}
%\end{figure}
%
%
 \begin{figure}[t!]
\centering
\subcapraggedrighttrue
\subcapnoonelinetrue
\subfiguretopcaptrue
\renewcommand*{\subcapsize}{\normalsize}
\subfigure[]{
%\psfrag{x}[bc][bc]{\color{black}\small $\times$}%
%\psfrag{P}[bc][bc]{\color{black}\small $\times$}%
\psfrag{IxTxQ}[bc][br]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-1pt} $(I \times T \times Q)$\end{tabular}}%
\psfrag{IxJ}[bc][br]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-3.5pt} $(I \times J)$\end{tabular}}%
\psfrag{JxJxQ}{\color{black} $(J \times J \times Q)$}%
\psfrag{JxP}{\color{black} $(J \times P)$}%
\psfrag{IxP}{\color{black} $(J \times P)$}%
\psfrag{PxPxQ}{\color{black} $(P \times P \times Q)$}%
\psfrag{PxT}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-3.5pt}\small $(P \times T)$\end{tabular}}%
\psfrag{Yt}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\hspace{-6pt}\fontsize{17}{15}
 $\underline \bY$\end{tabular}}%
 \psfrag{Da}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\hspace{2pt}\fontsize{17}{15}
 $\underline \bD^{(A)}$\end{tabular}}%
 \psfrag{Db}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-8.5pt}\hspace{-16pt}\fontsize{17}{15}
 $\underline \bD^{(B)}$\end{tabular}}%
\psfrag{T}[bc][bc]{\color{black} $T$}
\psfrag{A}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bA$\end{tabular}}%
 \psfrag{B}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bB$\end{tabular}}%
 \psfrag{R}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bR$\end{tabular}}%
\includegraphics[width=\textwidth]{PARATUC2_c1_fix}\label{Fig-dedicom3d_a}}
%
\subfigure[]{
\psfrag{IxIxQ}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-1pt} $(I \times I \times Q)$\end{tabular}}%
\psfrag{IxJ}[bc][bc]{\color{black} $(I \times J)$}%
\psfrag{JxJxQ}[bc][bc]{\color{black} $(J \times J \times Q)$}%
\psfrag{JxJ}{\color{black} $(J \times J)$}%
\psfrag{JxI}[bc][bc]{\color{black} $(J \times I)$}%
\psfrag{T}[bc][bc]{\color{black}\small $T$}%
\psfrag{Dt}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\underline \bD$\end{tabular}}%
\psfrag{Yt}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\hspace{-6pt}\fontsize{17}{15}
 $\underline \bY$\end{tabular}}%
\psfrag{A}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bA$\end{tabular}}%
\psfrag{R}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bR$\end{tabular}}%

\includegraphics[width=\textwidth]{DEDICOMmodel_c1_fix}\label{Fig-dedicom3d_b}}
\setlength{\templength}{(-\textwidth+9.8cm)/2}
\subfigcapmargin \templength
\subfigure[]{
\psfrag{T}[bc][bc]{\color{black}\small $T$}%
\psfrag{Y}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\hspace{-6pt}\fontsize{17}{15}
 $\bY$\end{tabular}}%
\psfrag{A}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bA$\end{tabular}}%
\psfrag{R}[bc][bc]{\color{black}
\setlength{\tabcolsep}{0pt}\begin{tabular}{c}\vspace{-5.5pt}\fontsize{17}{15}
 $\bR$\end{tabular}}%
\psfrag{x}[bc][bc]{\color{black} $\times$}
\includegraphics[width=9.8cm] {dedicom2D_c}\label{Fig-dedicom3d_c}}
\caption{\subref{Fig-dedicom3d_a} PARATUCK2 model performing decomposition of tensor $\underline \bY \in \Real^{I \times T \times Q}$.
    \subref{Fig-dedicom3d_b} DEDICOM model for a symmetric third-order tensor $\underline \bY \in \Real^{I \times I \times Q}$,
    \subref{Fig-dedicom3d_c} DEDICOM model for a square data matrix (usually, we assume that a matrix $\bA$ is orthogonal).}
    \label{Fig-dedicom3d}
\end{figure}


Figure  \ref{Fig-dedicom3d_b} illustrates a special form of PARATUCK2  called the three-way \inx{DEDICOM}
(inx{DEcomposition into DIrectional COMponents}) model \cite{Bader06_report},\cite{ICDM07-DEDICOM},\cite{Kolda08}.
In this case for  a  given symmetric
 third-order  data tensor $\underline \bY \in \Real^{I \times I \times Q}$ with frontal slices
 $\bY_{q}$  the simplified decomposition is:
\be
 \hlbox{
\bY_q  = \bA \; \bD_q \; \bR \; \bD_q \; \bA^T +\bE, \qquad
(q=1,2, \ldots, Q),
}
\ee
%
where $\bA \in \Real^{I \times J}$ is a
matrix of loadings, $\bD_q$ is  a diagonal matrix representing the
$q$-th frontal slice of the tensor $\underline \bD \in \Real^{J
\times J \times Q}$, and $\bR \in \Real^{ J \times J}$  is an
asymmetric matrix.

The three-way DEDICOM model\inxx{DEDICOM} can be considered as natural extension of the 2-way DEDICOM model \cite{Takane}:
% 2-way DEDICOM model was  introduced by Hirshman and Takane \cite{Takane1995}
\be
 \hlbox{
\bY = \bA \bR \bA^T +\bE,
}
\ee
where $\bY \in \Real^{I \times I}$ is a given  data matrix (generally asymmetric), and  $\bE$ is a matrix representing error
not explained by the model.
%
The goal  is to estimate the
best-fitting matrices:  $\bA \in \Real^{I \times J}$ and $\bR \in \Real^{J \times J}$.
% such that the standard error cost function, as $\| \bE \|^2_F$,  is minimized.
To achieve this goal we usually perform the following optimization problem:
\be
\min_{\bf A,\bf R} \; ||\bY - \bA \bR \bA^T||_F^2.
\ee
%
The  matrix $\bA \in \Real_+^{I \times J}$ comprises loadings or weights (with  $J <I$),
and  the square matrix $\bR$ is a matrix that represents the asymmetric relationships
for the latent dimensions  of $\bA$.
%

%Each  matrix $\bD_q$  is  diagonal matrix that
%gives the weights of the columns of $\bA$ for each level in the
% 3-way DEDICOM that belongs to  a broader family of
%multilinear models called PARATUCK2 in analogy to PARAFAC2 and NTF2 models
%which performs decomposition of arbitrary dimensions third-order data tensor
%illustrated in Fig \ref{ (b)} \cite{Kolda08}. %\cite{Harshman1994}.
%They can be
%empirically determined aa a unique best fitting axis orientation in
%$\bA$, without the need for a separate factor rotation \cite{Kolda08}.
% This corresponds to an extension of the factor analysis to three ways by
%PARAFAC \cite{Kolda08} the same kind of  special uniqueness
%property that emerges there.

This uniqueness of the three-way  DEDICOM  gives plausibility to the factors making them  a valid description
with a high confidence that  they  can explain more variance than convenient rotated 2-way
solutions \cite{Kolda08}.
%
%\begin{figure}
%\includegraphics[scale = .7]{dedicom2D}
%\caption{Dedicom model for a matrix decomposition}
%\label{fig19}
%\end{figure}

It should be noted that is some close relationships between PARAFAC2 and three-way  PARATUCK2 and DEDICOM models.
In fact the PARATUCK2 model can be derived from PARAFAC2 model by performing PARAFAC factorization
of a tensor $\underline \bA$. For  the PARFAAC2 model the matrix $\bR=\bH$  is dense, symmetric matrix
(usually positive definite),
while the matrix
$\bR$ in the PARATUCK2 or DEDICOM model is a dense, generally  asymmetric  matrix that captures asymmetric
relationships.


\subsection{Hierarchical Tensor Decomposition}
%
\begin{figure}[t]
%    %\par
%    %(a)
%    \hspace{0.5cm}  (a) \hspace{5.2cm} (b)%\\
   \begin{center}
%      \leavevmode
% \includegraphics[width=12.1 cm,height=4.3cm]{NTF1_AC}\\
%   \includegraphics[width=12.1 cm,height=4.3cm]{KNTFmodelC}\\
    \includegraphics[width=13.1 cm,height=11.3cm]{HT_c}\\
%    \hspace{0.1cm}
%     \leavevmode
%    \includegraphics[width=5.7cm,height=6cm] {x_reg_new_1}
    \end{center}
%   % \par
   \caption{Graphical illustration of hierarchical tensor decomposition.}
    \label{HTD}
    \end{figure}

%
Recently, multi-linear models based on  tensor approximation have received
much attention  as tools for denoising, reduction or compression as
 they have the potential to produce
more compact representations of multi-dimensional data than
traditional dimensionality reduction methods. We will exploit the
aforementioned characteristics of visual 3D data and develop an
analysis and a representation technique based on a hierarchical
tensor-based transformation\inxx{Hierarchical tensor decomposition}. In this technique,
 a multi-dimensional dataset is transformed into a hierarchy of signals
to reflect  multi-scale structures present in the multi-way data.
The signal at each level of
the hierarchy is further divided into a number of tensors with
smaller spatial support to expose  spatial inhomogeneity structures. To achieve a highly compact
representation these smaller  dimension tensors are further transformed and pruned
using a tensor approximation technique  \cite{Wuhier}.

It is interesting to note that the hierarchical scheme is very similar to the BCD model discussed in section (\ref{subsec:BCD}).
A source (data) tensor is expressed as a sum of multiple tensor decomposition models.
However, the hierarchical technique is much more simpler than BCD model.
For BCD model, all factors in all subtensors are simultaneously estimated,
hence, constraints imposed on these factors such as nonnegative can be assured during the estimation process.
However, BCD increases the complexity of the algorithms, especially for very large-scale data set.
For a specified data, we can choose an acceptable trade-off between simplicity and accuracy.

%In the simplest scenario, we may find the best approximation of the given
%input data tensor $\underline \bY$ by PARAFAC, that is, by a sum of
%rank-one tensors and compute the residual $\underline \bE = \underline
%\bY -\hat {\underline \bY}$.  We can then
%estimate a rank of tensor which is a  rough approximation of the
%residual, and the process can be  repeated until a stopping criterion is
%satisfied (for example, the residual entries reach values below
%some small threshold).
%It is obvious that a sum of two PARAFAC models with rank-$J_1$ and rank-$J_2$ gives us a PARAFAC model with rank-$(J_1 + J_2)$.
%Therefore, instead of factorizing a high rank tensor with a large number of latent components,
%the hierarchical scheme allows us performing factorizations with much lower ranks.
%This is a very important feature, especially for large-scale data.
%


\section{Discussion and Conclusions}

In this chapter we have presented a variety of different models,
graphical and mathematical representations for NMF, NTF, NTD and the related matrix/tensor
factorizations and decompositions. Our emphasis  has been on the formulation of
the problems and establishing
relationships and links among different models. Each model usually provides a different interpretation
of the data and may have different applications.
 Various
equivalent representations have been presented  which will
serve as a basis for the development of learning algorithms throughout this book.
%
%we  discuss various algorithms and
%occasionally  illustrate their specific applications.
%Special emphasis is given to large scale problems.

It has been highlighted that  constrained models with nonnegativity and sparsity
constraints  for real-world data cannot  provide a perfect  fit to the observed data
(i.e., they do not explain as much variance in the input data and  may have larger residual errors)
as compared  to
unconstrained factorization and decomposition models.
They, however,  often  produce more
meaningful  physical interpretations.
Although nonnegative factorizations/decompositions
already exhibit some degree of sparsity, the combination of  both
constraints enables a precise control of sparsity.
%
%The PARAFAC and NTF models are essentially unique under certain
%conditions. For example, the NTF model is unique if the factors are
%sufficiently sparse. The Tucker models under the some constraints imposed
%on the core tensor are also unique.

\chapappendix{Uniqueness Conditions for Three-way Tensor Factorizations}

The most attractive feature of the PARAFAC model is its uniqueness property.
Kruskal  \cite{kruskal77} has proved  that, for
fixed error  tensor  $\bE$, the vectors  $\ba_j,  \, \bb_j$,  and  $\bc_j$   of component matrices  $\bA,  \, \bB$ and $\bC$
are unique
up to unavoidable scaling and permutation of columns,{\footnote{If a PARAFAC  solution is unique up to these indeterminacies, it is called essentially unique. Two PARAFAC solutions
that are identical up to the essential uniqueness indeterminacies will be called equivalent.}} provided that
\be
k_A + k_B + k_C  \geq   J+2,
\label{Kruskal-cond}
\ee
%
where $k_A, k_B, k_C$  denote the $k$-ranks of the component matrices. The $k$-rank of a matrix is the
largest number $k$ such that every subset of $k$ columns of the matrix is linearly independent \cite{Stegman06}.
%For example,  if a matrix $\bA$ has an all-zero column then $k_A=0$, if $\bA$ has no all-zero columns,
% but it has two proportional columns, then $k_A=1$, and if
% $\bA$ has no all zero or proportional columns but there are three linearly dependent columns, then $k_A=2$.

Kruskal's uniqueness condition was generalized to $N$-th order tensors with $N > 3$ by Sidropoulos and Bro
\cite{Sidro-Bro}.
A more accessible proof of the uniqueness condition (\ref{Kruskal-cond})  for the PARAFAC model  was given  by Stegeman and Sidiropoulos  \cite{Stegman-Sidropoulos}.
For the case where one of the component matrices $\bA, \, \bB$ and  $\bC$  has full column rank,  weaker
uniqueness conditions than (\ref{Kruskal-cond}) have been derived by Jiang and Sidiropoulos,
De Lathauwer,  and  Stegeman  (e.g., see \cite{Stegman09},\cite{Stegman06}). For example, if a component matrix  $\bC \in \Real^{Q \times J}$ is full column rank, and $\bA \in \Real^{ I \times J}$ and $\bB \in \Real^{T \times J}$ have $k$-rank at least 2, then Kruskal's condition $k_A+k_B\geq J+2$ implies uniqueness of a PARAFAC solution \cite{Stegman09}.


%Ten Berge and Sidiropoulos [24] have shown that Kruskal's  sufficient condition is also necessary for essential
%uniqueness when $J = 2$ or  $J = 3$, but not when $J > 3$. It may be noted that (\ref{Kruskal-cond}) cannot be met for $R=1$.

It should be noted that the above conditions are only valid for the unrestricted PARAFAC model.  If we impose additional constraints such as nonnegativity, sparsity, orthogonality the conditions for uniqueness can be relaxed{\footnote{Moreover, by imposing the nonnegativity or orthogonality  constraints the PARAFAC has an optimal solution, i.e., there is no risk for  degenerate  solutions \cite{Lim-Comon}. Imposing nonnegativity constraints makes degenerative
solutions impossible since no factor can counteract the effect of another
factor and usually improves   convergence  since the search space is greatly
reduced.}} and they can be different \cite{Stegman-Almeida},\cite{Elad-unique}. For example, the NTF, NTF1, NTF2 and NTD models are unique (i.e., without rotational ambiguity) if component matrices and/or core tensor are sufficiently sparse.
% However, this is still an open question and active research area.
 %formulate rigorously such conditions for specific constraints.



\chapappendix{Singular Value Decomposition (SVD)
and Principal Component Analysis (PCA) with Sparsity and/or Nonnegativity Constraints} \label{Sec:SPCA}

SVD\inxx{SVD} and PCA are widely used tools, for example, in medical image analysis for  dimension reduction, model
building, and data understanding and exploration. They have applications in virtually  all areas of science,
machine
learning, image processing, engineering, genetics, neurocomputing, chemistry, meteorology,
computer networks, to name just a few, where large data sets are encountered.
If $\bY \in \Real^{I \times T}$ is a data matrix
encoding $T$ samples of $I$ variables, with $I$ being large, PCA aims at finding a few linear combinations of
these variables, called the principal components, which point in orthogonal directions explaining as much of
the variance in the data as possible.
The purpose of principal component analysis PCA is to derive a
relatively small number of uncorrelated linear combinations
(principal components) of a set of random zero-mean variables
while retaining as much of the information from the original
variables as possible.
%
Among the objectives of Principal Components Analysis\inxx{PCA} are the
following.

\begin{enumerate}

\item  Dimensionality reduction.

\item   Determination of linear combinations of variables.

\item   Feature selection: the choosing of the most useful
variables.

\item   Visualization of multi-dimensional data.

\item   Identification of underlying variables.

\item   Identification of groups of objects or of outliers.

\end{enumerate}
The success of PCA/SVD is due to two main optimal properties:
 Principal components sequentially capture the maximum variability
of $\bY$ thus guaranteeing minimal information loss, and they  are mutually uncorrelated.
Despite
the  power and popularity of PCA, one key drawback is its lack of sparseness (i.e., factor loadings
are linear combinations of all the input variables), yet sparse representations are generally
desirable since they aid human understanding (e.g., with gene expression data), reduce
computational costs and promote better generalization in learning algorithms.
%
In other words, the standard principal components (PCs) can sometimes
be difficult to interpret, because they are linear combinations of all the original variables.
 To facilitate
better interpretation, sparse and/or nonnegative PCA estimate modified PCs with sparse and/or nonnegative
eigenvectors, i.e. loadings with very few nonzero and possibly nonnegative entries\inxx{nonnegative PCA}.
%
  We use the connection of PCA  with SVD of the data matrix
and extract the PCs through solving a low rank matrix approximation problem.
Regularization penalties
are  usually incorporated to the corresponding minimization problem
to enforce sparsity and/or nonnegativity in PC loadings \cite{Zass-PCA},\cite{Shen-Huang-SPCA}.

Standard PCA  is  essentially the same technique as  SVD but usually obtained using slightly
different assumptions. Usually, in PCA
we use normalized data with each variable centered and possibly normalized by the standard deviation.

\section{Standard SVD and PCA}

At first let us consider basic properties of standard SVD and PCA\inxx{standard SVD and PCA}.
%
The SVD of a data matrix $\bY \in \Real^{I \times T}$ assuming without loss of generality that $T>I$
%and  rank of $\bY$ us $J < I$
 leads to  the following matrix factorization %a linear orthogonal expansion of the data given by
\be
\bY = \bU \mbi \Sigma \bV^T =\sum_{j=1}^J \sigma_j \; \bu_j  \; \bv^T_j,
\label{svd1}
\ee
where the matrix $\bU = [\bu_1, \bu_2, \ldots , \bu_I]  \in \Real^{I \times I}$ contains the $I$ left singular
vectors, $\mbi \Sigma \in \Real_+^{I \times T}$
with nonnegative elements on the main diagonal representing the singular values $\sigma_j$ and the matrix
$\bV = [\bv_1,\bv_2, \ldots ,\bv_T]$ $ \in \Real^{T \times T}$ represents the $T$
right singular vectors called the loading factors.
%
The
nonnegative quantities $\sigma_j$, sorted as $\sigma_1 \geq \sigma_2 \geq  \cdots  \geq \sigma_J > $
$\sigma_{J+1} = \sigma_{J+2} = \cdots  = \sigma_I = 0$
can be shown to be the  square roots of the eigenvalues of the data covariance
matrix $\bY \bY^T \in \Real^{ I \times I}$.
The term $\bu_j \bv^T_j$ is an $I \times T$ rank-one matrix  called
often the $j$-th eigenimage of $\bY$. Orthogonality of the SVD expansion ensures
that the left and right  singular vectors  are orthogonal,
 i.e., $\bu^T_i \bu_j =\delta_{ij}$ and $\bv^T_i \bv_j =\delta_{ij}$,with $\delta_{ij}$
 the Kronecker function (or equivalently $\bU^T \bU =\bI$ and $\bV^T \bV =\bI$).
 %
 In many applications, it is most practical to work with the truncated form of the SVD
  where only the first $P < J$, (where $J$ is a rank of $\bY$ with $J < I$) singular
 values are used so that
 %
\be
\bY \cong \bU_P \; \mbi \Sigma_P \; \bV^T_P =\sum_{j=1}^P \sigma_j \; \bu_j \; \bv^T_j,
\label{svdred}
\ee
where $\bU_P = [\bu_1, \bu_2, \ldots , \bu_P]  \in \Real^{I \times P}$,
  $\mbi \Sigma_P= \diag\{\sigma_1,\sigma_2, \ldots,\sigma_P\}$
  and $\bV = [\bv_1,\bv_2, \ldots ,\bv_P] \in \Real^{T \times P}$. This is no longer
  an exact decomposition of the data matrix $\bY$, but according to
  the Eckart-Young theorem\inxx{Eckart-Young theorem}
  it is the best rank-$P$ approximation in the least-squares sense and it is still unique
  (neglecting signs of vectors ambiguity) if the singular values are distinct.

Approximation of the matrix $\bY$ by a rank-one matrix $ \sigma \bu \bv^T$
of two unknown
vectors $\bu =[u_1,u_2,\ldots, u_I]^T \in \Real^I$ and $\bv =[v_1,v_2, \ldots, v_T]^T \in \Real^T$
 normalized to unit length with a  scaling constant
term $\sigma$ can be presented as follows:
\be
\bY =  \sigma \; \bu \; \bv^T + \bE,
\ee
where $\bE \in \Real^{I \times T}$  is a matrix of the residual errors $e_{it}$.
 In order to compute the unknown vectors we
minimize the squared Euclidean error as \cite{Lipovetsky}
\be
J_1=||\bE||_F^2 = \sum_{it} e_{it}^2= \sum_{i=1}^I \sum_{t=1}^T (y_{it} - \sigma \; u_{i} \; v_{t})^2.
\label{svd-cost}
\ee
The necessary  conditions for minimization of (\ref{svd-cost}) are obtained by equating gradients to zero:
\be
\frac{\partial J_1}{\partial u_{i}} = -2 \sigma  \sum_{t=1}^T (y_{it} - \sigma \; u_{i} \; v_{t}) v_{t} &=&0,\\
\frac{\partial J_1}{\partial v_{t}} = -2 \sigma \sum_{i=1}^I (y_{it} - \sigma \; u_{i} \; v_{t}) u_{i} &=&0,
\ee
These equations can be expressed as follows:
\be
\sum_{t=1}^T y_{it} \; v_t = \sigma \; u_i \sum_{t=1}^T v^2_t, \qquad
\sum_{i=1}^I y_{it}  \; u_i = \sigma \; v_t \sum_{i=1}^I u^2_i.
\label{svd2sc}
\ee
Taking into account that the vectors are normalized to unit length, that is,
 $\bu^T \bu = \sum_{i=1}^I u_i^2=1$ and   $\bv^T \bv = \sum_{t=1}^T v_t^2=1$,
  we can write the above equations in a compact
matrix form as
\be
\bY \; \bv = \sigma \; \bu, \qquad \bY^T  \; \bu = \sigma  \; \bv
\label{svd_eig1}
\ee
%
or equivalently (by substituting one of Eqs. (\ref{svd_eig1}) into another)
\be
\bY^T \bY \; \bv = \sigma^2 \bv, \qquad \bY \bY^T \bu = \sigma^2 \bu,
\label{svd_eig2}
\ee
which are classical eigenvalue problems which  estimate the maximum  eigenvalue
$\lambda_1= \sigma_1^2= \sigma^2_{max}$ with the corresponding eigen vectors $\bu_1=\bu$ and $\bv_1 =\bv$.
The solutions of these problems give the best first rank-one approximation of Eq. (\ref{svd1}).

One of the  most important results for nonnegative matrices is the following \cite{Ho2008}:
%
\begin{theorem}
{\bf(Perron-Frobenius)}
\label{theo_Perron-Frobenius}
For a square nonnegative matrix $\bY$ there exists a largest modulus eigenvalue of $\bY$ which is nonnegative
and a corresponding nonnegative eigenvector.
\end{theorem}
%
%{\bf Theorem 1.1 (Perron-Frobenius, see [8])}. Let $\bY$ be a square nonnegative
%matrix. There exist a largest modulus eigenvalue of $\bY$ which is nonnegative
%and a nonnegative eigenvector corresponding to it.
%
The eigenvector satisfying the Perron-Frobenius theorem is usually referred to as the Perron vector of a
nonnegative matrix. For a rectangular nonnegative matrix, a similar
result can be established for the largest singular value and its
corresponding singular vector:
\begin{theorem}
The leading
singular vectors: $\bu_1,$ and  $\bv_1$ corresponding to the largest singular value $\sigma_{max}=\sigma_1$ of a nonnegative matrix $\bY = \bU \mbi \Sigma \bV^T=\sum_i \sigma_i \bu_i \bv_i^T$ (with $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_I$) are nonnegative.
\end{theorem}
%This is a consequence of the basic Perron-Frobenius theorem.
%
Based on this observation, it
is straightforward to compute the best rank-one NMF approximation $ \sigma_1 \bu_1 \bv_1^T$, this idea can be extended to approximate
a higher-order NMF. If we compute the rank-one NMF and
subtract it from the original matrix $\hat \bY_1 =\bY - \sigma_1 \bu_1 \bv_1^T$, the input data matrix will no longer be
nonnegative, however, all negative elements can be forced to be zero or are positive and
the procedure can be repeated \cite{Biggs2008}.
%
In order to estimate the next singular values and the corresponding singular vectors,
 we may apply a deflation approach, that is,
\be
\bY_j = \bY_{j-1} - \sigma_j \bu_j \bv^T_j, \qquad (j=1,2,\ldots, J),
\ee
where $\bY_0 =\bY$.
%
Solving the same optimization problem (\ref{svd-cost}) for the
residual matrix $\bY_j$  yields the  set of  consecutive singular values
and corresponding singular vectors. Repeating reduction of the matrix
yields the next set of the solution until the deflation matrix $\bY_{J+1}$ becomes the zero.

Using the property of the  orthogonality of the eigenvectors,
and the equality $\bu^T \bY \bv =\bv^T \bY \bu =\sigma$
we can estimate the precision of the
matrix approximation with the first $P \leq J$ pairs of singular vectors \cite{Lipovetsky}:
\be
||\bY - \sum_{j=1}^P \sigma_j \bu_j \bv^T_j||_F^2 &=& \sum_{i=1}^I
\sum_{t=1}^T (y_{it}-\sum_{j=1}^J \sigma_j u_{ij} v_{jt})^2 \nonumber \\
&=& \|\bY\|_F^2 - \sum_{j=1}^P \sigma^2_j,
\ee
 and the residual error reduces exactly to zero with the  number of singular values
 equal to the matrix  rank, that is, for $P=J$. Thus, we can write for the rank-$J$ matrix:
 \be
 \|\bY\|_F^2 = \sum_{j=1}^J \sigma^2_j.
 \ee
%
It is interesting to note that  (taking into account that
$\sigma u_i= \sum_{t=1}^T y_{it} v_t / \sum_{t=1}^T
 v^2_t$  (see Eqs. (\ref{svd2sc})) the cost function (\ref{svd-cost}) can be expressed
 as \cite{Lipovetsky}
 \begin{equationarray}{rcl}
 J_1=||\bE||_F^2 &=&\sum_{i=1}^I \sum_{t=1}^T (y_{it} - \sigma \; u_{i} \; v_{t})^2 \nonumber\\
 &=& \sum_{i=1}^I \sum_{t=1}^T y^2_{it} - 2 \sum_{i=1}^I (\sigma \; u_{i}) \sum_{t=1}^T y_{it} v_{t})
 + \sum_{i=1}^I (\sigma \; u_{i})^2 \sum_{t=1}^T  v_{t}^2 \nonumber\\
 &=& \sum_{i=1}^I \sum_{t=1}^T y^2_{it} - \frac{ \sum_{i=1}^I (\sum_{t=1}^T y_{it} v_{t})^2}{\sum_{t=1}^T  v_{t}^2}.
 \end{equationarray}
In matrix notation the cost function can be written as
\be
\|\bE\|_F^2 = \| \bY \|^2_F - \frac{\bv^T \bY^T \bY \bv}{\|\bv\|_2^2} = \| \bY \|^2_F - \sigma^2,
\ee
where the second term is called the Rayleigh quotient.
The maximum value of  the Rayleigh quotient is exactly equal to the maximum eigenvalue
$\lambda_1=\sigma^2_1$.



\section{Sparse PCA}

For sparse PCA\inxx{sparse PCA} we may employ many alternative approaches \cite{Shen-Huang-SPCA}.
%
One of the simplest and most efficient approaches is to apply minimization of the following cost function  \cite{Shen-Huang-SPCA}:
\be
J_{\rho}(\tilde \bv)= \|\bY - \bu  \; \tilde \bv^T\|_F^2 +  \rho (\tilde \bv),
\ee
where $\tilde \bv= \sigma \bv$, $ \rho (\tilde \bv)$ is the additional penalty term which imposes sparsity.
  Typically,   $ \rho (|\tilde \bv|) = 2 \lambda ||\tilde \bv||_1$ or
  $ \rho (\tilde \bv) = \lambda^2 ||\tilde \bv||_0$, where
   $\lambda$ is the nonnegative coefficient that controls the degree of sparsity.{\footnote{We define the degree of sparsity of a PC
   as the number of  zero elements in the corresponding loading vector $\bv$.}}
The cost function can be evaluated in scalar form as follows
\be
J_{\rho}(\tilde \bv) &=& \sum_i \sum_t (y_{it} - u_i  \; \tilde v_t)^2 + \sum_t \rho(\tilde v_t)
= \sum_t \left(\sum_i(y_{it} -u_i \; \tilde v_t)^2 + \rho(v_t)\right) \nonumber \\
&=& \sum_t \left( \sum_i y_{it}^2 -2 \; [\bY^T \bu]_t  \; \tilde v_t + \tilde v_t^2 +\rho(\tilde v_t)\right), \qquad (t=1,2,\ldots,T).
\ee
%
It is not difficult to see (see Chapter \ref{Ch4}) that
 the optimal value of $\tilde v_t$ depends on the penalty term $\rho(\tilde \bv_t)$, in particular
for $\rho(\tilde \bv_t) =2 \lambda ||\tilde \bv||_1$ we use soft shrinkage with threshold $\lambda$
\be
\tilde v_t = P^{(s)}_{\lambda} (x) = \sign(x) [|x| -\lambda]_+
\ee
and for $\rho(\tilde \bv_t) = \lambda^2 ||\tilde \bv||_0$ we use hard shrinkage projection
\be
\tilde v_t = P^{(h)}_{\lambda} (x) = I(x > \lambda) \; x = \begin{cases} x, & \mbox{for} \;\;\; |x| \geq \lambda;
\cr 0,  & \mbox{otherwise,} \end{cases}
\ee
where $x=[\bY^T \bu]_t$.
This leads to the following iterative algorithm proposed by Shen and Huang \cite{Shen-Huang-SPCA}
\begin{enumerate}
\item Initialize: Apply the regular SVD to data matrix $\bX$ and estimate the
maximum singular value $\sigma_1=\sigma_{\max}$ and corresponding singular vectors
$\bu =\bu_1$ and $\bv=\bv_1$, take $\tilde \bv_1= \sigma_1 \bv_1$.
This corresponds to the best rank-one approximation of $\bY=\bY_0$,
%
\item Update:
\be
\tilde \bv_1 &\leftarrow& P_{\lambda} \left(\bY^T \bu_1 \right), \\
 \tilde \bu_1 &\leftarrow& \left(\bY  \tilde \bv_1 \right) / ||\bY \tilde \bv_1||_2,
\ee
%
\item Repeat Step 2  until convergence,
\item Normalize the vector $\tilde \bv_1$  as $\tilde \bv_1 = \tilde \bv_1/||\tilde \bv_1||_2$.
\end{enumerate}
%
Note that for $\lambda=0$ the nonlinear shrinkage function $P_{\lambda}$ becomes a linear function and the above procedure simplifies to the well-known  standard
alternating least squares  SVD algorithm.
The subsequent pair $\{\bu_2, \sigma_2 \bv_2\}$   provides the best rank-one approximation of the corresponding residual matrix $\bY_1=\bY- \sigma_1 \bu_1 \bv_1$.
In other words, subsequent sparse loading vectors $\bv_j$ can be obtained sequentially
via a deflation approach and a rank-one approximation of the residual data matrices $\bY_j$.



\section{Nonnegative PCA}


In some applications it is necessary to incorporate both nonnegativity
and/or sparseness constraints into PCA maintaining the maximal variance property
of PCA and  relaxing orthogonality constraints. The algorithm described in the previous section can be applied
almost directly to nonnegative PCA\inxx{nonnegative PCA} by applying a suitable nonnegative shrinkage function.
However, in such a case,
it should be noted that the orthogonality among vectors $\bv_j$ is completely lost.
%
If we need to control the orthogonality constraint (to some extent) together with nonnegativity constraints,
 we may alternatively  apply
the following optimization problem \cite{Zass-PCA}
\be
\max_{\bf V \geq \bf 0} \frac{1}{2} ||\bV_J^T \bY||_F^2 - \alpha_o || \bI - \bV_J^T \bV_J||_F^2 -\alpha_s ||\bV_J||_1,
\ee
subject to nonnegativity constraints $\bV_J \geq 0$, where $||\bV_J||_1 =\1^T \bV_J  \1$.
The nonnegative coefficients $\alpha_o$ and $\alpha_s$ control a level and tradeoff between sparsity and orthogonality.

Alternatively, we can use nonnegative loading parametrization, for example, exponential parametrization
\cite{Lipovetsky}
\be
v_{t} = \exp (\gamma_{t}), \qquad \forall t,
\ee
where  $\gamma_{t}$ are the estimated parameters. To obtain the loading in the range from zero
to one we can use multinomial parametrization
\be
v_{t} = \frac{\exp(\gamma_{t})}{\sum_t \exp(\gamma_{t})}, \qquad (t=1,2,\ldots,T).
\ee

%
%
%PCA is perhaps one of the oldest and the best-known technique in
%multivariate analysis and data mining. It was introduced by
%Pearson, who used it in a biological context and further developed
%by Hotelling in works done on psychometry. PCA was also developed
%independently by Karhunen in the context of probability theory and
%was subsequently generalized by Lo\`eve [see for example,
%\cite{CiKasSka96,Rosipal01} and references therein].
%%
%The purpose of principal component analysis PCA is to derive a
%relatively small number of uncorrelated linear combinations
%(principal components) of a set of random zero-mean variables
%while retaining as much of the information from the original
%variables as possible.
%%
%Among the objectives of Principal Components Analysis are the
%following.
%
%\begin{enumerate}
%
%\item  dimensionality reduction;
%
%\item   determination of linear combinations of variables;
%
%\item   feature selection: the choosing of the most useful
%variables;
%
%\item   visualization of multidimensional data;
%
%\item   identification of underlying variables;
%
%\item   identification of groups of objects or of outliers.
%
%\end{enumerate}
%%
%Often the principal components (PCs) (i.e., directions on which
%the input data have the largest variances) are usually regarded as
%important or significant, while those components with the smallest
%variances called minor components (MCs) are usually regarded as
%unimportant or associated with noise. However, in some
%applications, the MCs are of the same importance as the PCs, for
%example, in curve and surface fitting or total least squares (TLS)
%problems
%\cite{cichocki:unbehauen93}.\\
%%
%Generally speaking, PCA is related and motivated by the following
%two problems:
%%
%\begin{enumerate}
%
%\item Given  random vectors $\by(t) \in \Real^{I} $, with finite
%second order moments and zero mean, find the reduced $J$-dimensional
%($J<I$) linear subspace that minimizes the expected distance of
%$\by(t)$ from the subspace. This problem arises in the area of data
%compression where the task is to represent all the data with a
%reduced number of parameters while assuring minimum distortion due
%to projection.
%
%\item Given random vectors  $\by(t) \in \Real^{I}$, find the
%$n$-dimensional linear subspace that captures most of the variance
%of the data $\bx$. This problem is related to feature extraction,
%where the objective is to reduce the dimension of the data while
%retaining most of its information content.
%
%\end{enumerate}
%%
%It turns out that both  problems have the same optimal solution
%(in the sense of least-squares error) which is based on the second
%order statistics (SOS), in particular, on the eigen structure of
%the data covariance matrix.
%%
%%{\footnote {If signals are zero mean, the covariance and
%%correlation matrices are identical.}}.
%%
%PCA can be converted to the eigenvalue problem of the covariance
%matrix of $\by(t)$ and it is essentially equivalent to the
%Karhunen-Lo\`eve transform used in image and signal processing. In
%other words, PCA is a technique for computation of  eigenvectors and
%eigenvalues for the estimated covariance matrix
%%{\footnote {The covariance
%%matrix is the correlation matrix of the vector with the mean
%%removed. Since, we consider  zero mean signals, both matrices are
%%equivalent.}}
%%
% \be  {\bR }_{\by \, \by}= E\{ \by(t) \,
%\by^{T}(t)\} = \bV \, \mbi {\Lambda} \, \bV^T \in  \Real^{I \times
%I}, \ee where $\mbi {\Lambda }$ $=$ diag $\left\{ \lambda
%_{1},\lambda _{2},...,\lambda _{I}\right\} $ is a diagonal matrix
%containing the $I$ eigenvalues and $\bV$ $=$ $\left[
%\bv_{1},\bv_{2}, \ldots, \bv_{I}\right] \in \Real^{I \times I} $ is
%the corresponding orthogonal or unitary matrix consisting of the
%unit length eigenvectors referred to as principal eigenvectors. PCA
%can be also done via the singular value decomposition (SVD) of the
%batch data matrix $\bY=[\by(1),\by(2), \ldots , \by(T)]$.
%
%The Karhunen-Lo\'eve-transform determines a linear transformation of
%an input vector $\by$ as \be \by_{\mbox {\tiny P}}= \bV^T_{\cal X}
%\, \bx, \label{karhloeve1} \ee where
% $\by = [ y_1 (t), y_2 (t), \ldots, y_I (t) ]^T$ is the
%zero-mean input vector, $\by_{\mbox {\tiny P}} = [y_1(t), y_2(t),
%\ldots, y_I(t)]^T$ is the output vector called the vector of
%principal components (PCs), and $\bV_{\cal S} = [ \bv_1, \bv_2,$
%$\ldots, \bv_I ]^T$ $ \in \Real^{m \times n}$ is the set of  signal
%subspace eigenvectors, with the orthonormal vectors $\bv_i = [
%v_{i1}, v_{i2},$ $\ldots, v_{im} ]^T$, (i.e., $(\bv_i^T \bv_j =
%\delta_{ij})$ for $j \leq i$, where $\delta_{ij}$ is the Kronecker
%delta  equals to 1 for $i=j$, otherwise zero. The vectors
%$\bv_i\;\;$ $(i=1,2, \ldots,I)$ are
% eigenvectors of the covariance matrix,
%while the variances of the PCs $y_i(t) =\bv_i^T \bx(t)$ are the
%corresponding principal eigenvalues. On the other hand, the $(m-n)$
%minor components are given by
%\begin{equation}
%  {\by}_{\mbox {\tiny M}}  = {\bV}^T_{\cal N} \, {\bx},
%\end{equation}
%where ${\bV}_{\cal N} = \left[ {\bv}_I, {\bv}_{I-1}, \ldots,
%{\bv}_{I-J+1} \right]$ consists of the $(I-J)$ eigenvectors
%associated with
% the smallest eigenvalues.
%%
%%Therefore, the basic problem we try to solve is the standard
%%eigenvalue problem which can be formulated by the equations \be
%%\bR _{\bx \,\bx} \bv_i = \lambda_i \bv_i,   \qquad  (i=1,2,\ldots,n)
%%\label{eq:eigp} \ee where $\bv_i$ are the eigenvectors, $\lambda_i$
%%are the corresponding eigenvalues and $\bR _{\bx \,\bx} = E\{\bx
%%\,\bx^T\}$ is the covariance matrix of zero-mean signal $\bx$ and
%%$E$ is the expectation operator. Note that (\ref{eq:eigp}) can be
%%written in matrix form $\bV^T \, \bR _{\bx \,\bx} \, \bV = \mbi
%%\Lambda$,
%% where $\mbi\Lambda$ is the diagonal matrix
%%of  eigenvalues of the covariance matrix $\bR _{\bx \,\bx}$.
%
% A very important problem arising in
%many application areas is determination of the dimension of the
%signal and  noise subspaces. In other words, a central issue in
%PCA is choosing the number of principal components to be retained.
%To solve this problem, %Signal and noise subspaces
% we usually exploit a fundamental property of PCA:  It
%projects the input data $\bx (t)$ from their original
%$m$-dimensional space onto an $n$-dimensional output subspace
%$\by(t)$ (typically, with $n \ll m$), thus performing a
%dimensionality reduction which retains most of
% the intrinsic information in the input data vectors.  In other words, the
%principal components $y_i(t)=\bv_i^T \bx(t)$ are estimated in such a
%way that, for $n < m$, although the dimensionality of data is
%strongly reduced, the most relevant information is retained in the
%sense that the original input data $\bx$ can be
% reconstructed from the output data (signals) $\by$ by
%using the transformation $\hat{\bx} = \bV_{\cal S} \, \by$, that
%minimizes a suitable cost function. A commonly used criterion is the
%minimization of mean squared error $\|\bx-\bV_{\cal S}^T \,
%\bV_{\cal S} \, \bx\|_2^2$.
%
% PCA enables us to divide observed (measured)  sensor signals:
% $\by(t) =  \by_s(t) + \mbi {e}(t)$
% into two subspaces: the {\em signal subspace}
%corresponding to principal components associated with the largest
%eigenvalues called principal eigenvalues:
% $\lambda _{1},\lambda _{2},...,$ $\lambda _{n}$,
% ($m>n$) and associated eigenvectors $\bV_s=[\bv_1,\bv_2, \ldots, \bv_n]$
%called the principal eigenvectors and the {\em noise subspace}
%corresponding to the minor components associated with the
%eigenvalues $\lambda _{n+1},...,$ $\lambda _{m}.$
%%
%The subspace spanned by the $n$ first eigenvectors $\bv_{i}$ can be
%considered as an approximation of the noiseless signal subspace.
%%
%One important advantage of this approach is that it enables not
%only a reduction in the noise level, but also allows us to
%estimate
% the number of sources on the basis of distribution of eigenvalues.
%However, a problem arising from this approach, is how to correctly
%set or estimate the threshold which divides eigenvalues into the
%two subspaces, especially when the noise is large (i.e., the SNR
%is low).
%
%Let us assume that we model the vector $\by(t) \in \Real^I$ as
%%\be
%\begin{equation}
%\by(t)= \bA \, \bx(t) + \mbi {e} (t), \label{probPCAmod}
%\end{equation}
%% \ee
% where $\bA \in \Real^{m \times n}$
% is a full column rank mixing matrix with
%$m >n$ representing  factor loading , $\bs(t) \in \Real^n$ is a
%vector of zero-mean Gaussian sources with the nonsingular covariance
%matrix $\bR _{\bs \,\bs}=E\{\bs(t) \bs^T(t)\}$ and $\mbi {\nu}(t)
%\in \Real^m$ is a vector of Gaussian zero-mean i.i.d. noise modeled
%by the covariance matrix $\bR _{\mbi {\nu} \mbi {\nu}} =
%\bsigma^2_{\nu} \bI_m$, furthermore, random vectors $\{\bs(t)\}$ and
%$\{ \mbi {\nu}(t) \}$ are uncorrelated.
%
%
%For the model (\ref{probPCAmod}) and under the above assumptions the
%covariance matrix of $\bx(t)$ can be written as \be \bR _{\bx\bx}&=
%& E\{\bx(t) \, \bx^T(t)\}= \bA \, \bR _{\bs \,\bs} \, \bA^T
%+ \bsigma^2_{\nu} \bI_m\nonumber \\
%&=& [\bV_{\cal S}, \bV_{\cal N}] \matrixb{cc}
%\mbi {\Lambda}_{\cal S} & \0 \\
% \0  & \mbi {\Lambda}_{\cal N}  \\
%   \matrixe
%[\bV_{\cal S}, \bV_{\cal N}]^T \nonumber \\
%&=& \bV_{\cal S} \mbi {\Lambda}_{\cal S} \bV^T_{\cal S} + \bV_{\cal
%N} \mbi {\Lambda}_{\cal N} \bV^T_{\cal N}, \ee where $\bA \, \bR
%_{\bs\,\bs} \,\bA^T = \bV_{\cal S}  {\mbi {\Lambda}}_{\cal S}
%\bV^T_{\cal S}$ is a rank-$n$ matrix, $\bV_{\cal S} \in  \Real^{m
%\times n}$ contains the eigenvectors associated with $n$ principal
%(signal+noise subspace) eigenvalues of $\mbi {\Lambda}_{\cal S} =
%\diag (\lambda_1 \geq \lambda_2 \cdots  \geq \lambda_n)$ in a
% descending order. Similarly, the matrix $\bV_{\cal N} \in  \Real^{m
%\times (m-n)}$ contains the $(m-n)$ (noise) eigenvectors that
%correspond to noise eigenvalues $\mbi {\Lambda}_{\cal N} = \diag
%(\lambda_{n+1},  \ldots,  \lambda_m)$ $=\bsigma^2_{\nu} \bI_{m-n}
%$. This means that, theoretically, the $(m-n)$ smallest eigenvalues
%of $\bR _{\bx\bx}$ are equal to $\bsigma^2_{\nu}$, so we could
%determine the dimension of the signal subspace from the multiplicity
%of the smallest eigenvalues under the assumption that
% the variance of the noise is relatively low and we have a
% perfect estimate of the
%covariance matrix. However, in practice, we estimate the sample
%covariance matrix from a limited number of samples and the
%smallest eigenvalues are usually different, so the determination
%of the dimension of the signal subspace is usually not an easy
%task.
%%
%Instead of setting the threshold between the signal and noise
%eigenvalues by using some heuristic procedure or a rule of thumb,
%we can use
% one of the three well-known information theoretic criteria, namely,
%Akaike's information criterion ($AIC$),  the minimum description
%length ($MDL$) and Beyesian Information Criterion (BIC) [see e.g.,
% \cite{CiAm02} and references therein].


\chapappendix{Determining a True Number of Components}

%\subsection{Estimation of Number of Components}


Determining the number of components $J$ for the NMF/NTF models, or
more generally
 determining the dimensions of a core tensor, $J,R,P$, for the Tucker models
 is  very important since the approximately valid model is instrumental in
 discovering or capturing the  underlying structure in the data\inxx{estimation the number of components}.

% However, as  already mentioned, the estimation of the rank of a tensor is  an NP problem, that is,
% there is no finite algorithm complexity for determining the exact rank of a tensor.

There are several approximative and heuristic techniques for
determining the number of components \cite{Bro_kiers},\cite{Timmerman},\cite{Cuelemans2006},\cite{Cuelemans2008},\cite{Costa2008},\cite{Niesing},\cite{He_Tucker}.
%
In an ideal noiseless case when the PARAFAC model is perfectly satisfied,
we can apply a specific procedure for calculating the PARAFAC components
for $J=2,3, \ldots$ until we reach the number of components for which the errors
$\bE= \bY - \hat \bY$  are zero. %gives a fit of $100\%$.
However, in practice, it is not possible to perfectly satisfy this model.
%
Other proposed methods include: residual analysis, visual appearance
of loadings, the number of iterations of the algorithm and core
consistency. In this book, we mostly rely on the PCA approach
and the core consistency diagnostic developed by Bro and Kiers
\cite{Bro_kiers} for finding the number of components  and selecting an appropriate
 (PARAFAC or Tucker) model. The core consistency quantifies the resemblance between the
Tucker3 core tensor and the PARAFAC core, which is a super-identity
or a superdiagonal core tensor, or in other words, a vector of
coefficients. This diagnostic tool suggests whether the PARAFAC model
with the specified number of components is a valid model for the
data.
%
The core consistency above $90\%$ is often used as an
indicator of the trilinear structure of the data, and suggests that the
PARAFAC model  would be an
appropriate model for the data. A core consistency value close to or
lower than $50\%$, on the other hand, would indicate that the
PARAFAC-like model is not  appropriate. This diagnostic method has
been commonly applied in the neuroscience-multi-way literature
%(Estienne {\it et al.}, 2001; Miwakeichi et al., 2004)
\cite{Esti2001},\cite{Miwakeichi}, often together with
other diagnostic tools, in order to determine the number of
components.

An efficient way is to use the
PCA/SVD approach, whereby for a three-way data set we  first
unfold the tensor as  matrices $\bY_{(1)}$ and eventually compute covariance matrix $\bR_y= (1/T) \bY_{(1)} \bY^T_{(1)}$.
%
%Consider a multivariate signal model: the observed array
%$\boldsymbol{Y}\in \mathbb{R}^{m\times T} (m<T)$ can be described as
%follows:
%\begin{equation}\label{Eq:SVDModel}
%\boldsymbol{Y}=\boldsymbol{AX}+\boldsymbol{E},
%\end{equation}
%where $\boldsymbol{A}\in \mathbb{R}^{I \times J}$ is a channel
%matrix, $\boldsymbol{X}\in \mathbb{R}^{J \times T}$ contains $J$
%sources with $T$ samples ($m>n$), $\boldsymbol{E}\in
%\mathbb{R}^{I \times T}$ denotes noise or error. Without loss of
%generality, suppose that the noise $(\boldsymbol{E})_{it}=e_{it},\
%i=1,2,\cdots,I, t=1,\cdots,T$ are mutually independent and follow an

%identical zero-mean Gaussian distribution $N(0, \sigma^2)$. We would
%like to detect the number $n$ of signals from the noisy observation
%$\boldsymbol{Y}$.
%
%Note that the model (\ref{Eq:SVDModel}) is separable in columns as:
%\begin{equation}\label{Eq:EVDModel}
%\boldsymbol{y}_t=\boldsymbol{Ax}_t+\boldsymbol{e}_t,\ t=1,\cdots,T,
%\end{equation}
%where $\boldsymbol{Y}=[\boldsymbol{y}_1,\cdots,\boldsymbol{y}_T]$,
%$\boldsymbol{X}=[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_T]$ and
%$\boldsymbol{E}=[\boldsymbol{e}_1,\cdots,\boldsymbol{e}_T]$. Assume
%that both the sources $\boldsymbol{x}_i(t)$ and noise ${e}_i(t)$
%follow Gaussian distribution, where ${e}_i(t)\sim N(0,\sigma^2)$.
%Then $\boldsymbol{y}_t$ follow $m$-variate Gaussian distribution.
%Without loss of generality, suppose that
%\begin{equation}\label{Eq:MeanAndCovariance}
%\left\{
%\begin{array}{l}
%E\boldsymbol{x}_t=\boldsymbol{\mu}_x\\
%\Sigma_{y}=\text{cov}(\boldsymbol{y}_t,\boldsymbol{y}_t)=
%\sum\limits_{i=1}^n(\lambda_i-\sigma^2)\cdot\boldsymbol{v}_i \; \boldsymbol{y}_i^T+\sigma^2\boldsymbol{I}
%\end{array}
%\right.
%\end{equation}
%where $\lambda_1,\cdots,\lambda_J$ and
%$\boldsymbol{v}_1,\cdots,\boldsymbol{v}_J$ are the eigenvalues and
%eigenvectors, respectively, of $\Sigma_{y}$. So the parameter set of
%model (\ref{Eq:EVDModel}) is as follows:
%\begin{equation}\label{Eq:ParameterSpace}
%\Theta=\{\boldsymbol{\mu}_x; \lambda_1,\cdots,\lambda_J; \sigma^2;
%\boldsymbol{v}_1,\cdots,\boldsymbol{v}_J\}.
%\end{equation}
%Thus the number of parameters totally is
%\begin{equation}\label{Eq:NumberOfParameter}
%\#\{\Theta\}=2J+IJ+1.
%\end{equation}
%
%
%\subsection{Signal and Noise Subspaces - Automatic Choice of
%Dimensionality for PCA}
%%AIC and MDL Criteria for their Estimation}
%\label{sec:AIC}
%
%A very important problem arising in many application areas is
%determination of the dimension of the \inx{signal and  noise
%subspaces}. In other words, a central issue in PCA is choosing the
%number of principal components to be retained \cite{Minka}.
%To solve this problem, %\inxx{Signal and noise subspaces}
% we usually exploit a fundamental property of PCA:  It
%projects the input data $\bx (t)$ from their original
%$I$-dimensional space onto an $J$-dimensional output subspace
%$\by(t)$ (typically, with $J << I$), thus performing a
%dimensionality reduction which retains most of
% the intrinsic information in the input data vectors.  In other words, the
%principal components $y_i(t)=\bv_i^T \bx(t)$ are estimated in such a
%way that, for $J << I $, although the dimensionality of data is
%strongly reduced, the most relevant information is retained in the
%sense that the original input data $\bx$ can be
% reconstructed from the output data (signals) $\by$ by
%using the transformation $\hat{\bx} = \bV_{\cal S} \, \by$, that
%minimizes a suitable cost function. A commonly used criterion is
%the minimization of mean squared error $\|\bx-\bV_{\cal S} \,
%\bV^T_{\cal S} \, \bx\|_2^2$.

Under the assumption that the power of the signals is larger than the power of the noise, the PCA enables us to divide observed (measured)  sensor signals:
 $\bx(t) =  \bx_s(t) + \mbi {\nu}(t)$
 into two subspaces: the {\em signal subspace}
corresponding to principal components associated with the largest
eigenvalues called the principal eigenvalues:
 $\lambda_{1},\lambda_{2},...,$ $\lambda _{J}$,
 ($I>J$) and associated eigenvectors $\bV_J=[\bv_1,\bv_2, \ldots, \bv_J]$
called the principal eigenvectors and the {\em noise
subspace} corresponding to the minor components associated with the
eigenvalues $\lambda _{J+1},...,$ $\lambda _{I}.$
%
The subspace spanned by the $J$ first eigenvectors $\bv_{i}$ can be
considered as an approximation of the noiseless signal subspace.
%
One important advantage of this
approach is that it enables not only a reduction in the
noise level, but also allows us to estimate
 the number of sources on the basis of distribution of the  eigenvalues.
However, a problem arising from this approach is how to correctly set or
estimate the threshold which divides eigenvalues into the two
subspaces, especially when the noise is large
(i.e., the SNR is low).
%
%For the model (\ref{probPCAmod})
%and under the above assumptions
The covariance matrix of the observed data can be written as
\be
\bR_y&= & E\{\by_t \by^T_t\}
%= \bA \, \bR_x \, \bA^T
%+ \sigma^2_{e} \bI \nonumber \\
= [\bV_{\cal S}, \bV_{\cal N}]
\matrixb{cc}
\mbi {\Lambda}_{\cal S} & \b0 \\
 \0  & \mbi {\Lambda}_{\cal N}  \\
   \matrixe
[\bV_{\cal S}, \bV_{\cal N}]^T \nonumber \\
&=& \bV_{\cal S} \mbi {\Lambda}_{\cal S} \bV^T_{\cal S} + \bV_{\cal
N} \mbi {\Lambda}_{\cal N} \bV^T_{\cal N}, \ee
%
where $\bV_{\cal S}  {\mbi {\Lambda}}_{\cal S}
\bV^T_{\cal S}$ is a rank-$J$ matrix, $\bV_{\cal S} \in  \Real^{I
\times J}$ contains the eigenvectors associated with $J$ principal
(signal+noise subspace) eigenvalues of $\mbi {\Lambda}_{\cal S} =
\diag \{\lambda_1 \geq \lambda_2 \cdots  \geq \lambda_J\}$ in a
 descending order. Similarly, the matrix $\bV_{\cal N} \in  \Real^{I
\times (I-J)}$ contains the $(I-J)$ (noise) eigenvectors that
correspond to noise eigenvalues $\mbi {\Lambda}_{\cal N} = \diag
\{\lambda_{J+1},  \ldots,  \lambda_I\}$ $=\sigma^2_{e} \bI_{I-J}
$.
This means that, theoretically, the $(I-J)$ smallest
eigenvalues of $\bR_y$ are equal to $\sigma^2_{e}$, so we
can determine  the dimension of the signal subspace from the
multiplicity of the smallest eigenvalues under the assumption that
 the variance of the noise is relatively low and we have a
 perfect estimate of the
covariance matrix. However, in practice, we estimate the sample
covariance matrix from a limited number of samples and the
smallest eigenvalues are usually different, so the determination
of the dimension of the signal subspace is usually not an easy
task.
%
%Instead of setting the threshold between the signal and noise eigenvalues
%by using some heuristic procedure or a rule of thumb, we can use
% one of the two well-known information theoretic criteria, namely,
%Akaike's information criterion ($AIC$),  the minimum description
%length ($MDL$) and Beyesian Information Criterion (BIC)
%\cite{KARCIAKASPAJ_ijns97,Wax85,Minka}.
%To do this, we compute the
%probability of the data for each possible dimension.
%Wax and Kailath \cite{Wax85} have
%evaluated explicit expressions of the AIC and MDL criteria
%for estimating the number $n$ of sources in the model:
%$\x=\H \s + \mbi {\nu}= \x_s + \mbi {\nu}$.
%
%The formulas of Wax and Kailath are \cite{Wax85}:
%
%Akaike's information theoretic criterion ($AIC$)
% selects the model that minimizes the cost function \cite{Regalia99}
% \be
% AIC = -2 \log (p(\by(1),\by(2),\ldots, \by(T)|\mbi {\hat \Theta})) +2 J,
% \ee
% where $p(\by(1),\by(2),\ldots, \by(T)|\mbi {\hat \Theta})$ is a parameterized
% %family of
% probability density function, $\hat {\mbi \Theta}$ is the maximum likelihood
% estimator of a parameter vector $\mbi \Theta$, and $J$ is the number of free adjusted
% parameters.
%%Here, $N$ is the number of the data vectors $\x(k)$
%%used in estimating the sampled covariance matrix $\bR_{\x\x}$.
%
% The minimum description length ($MDL$) criterion selects the model that
% instead minimizes
% \be
% MDL = - \log (p(\by(1),\by(2),\ldots, \by(N)|\hat {\mbi \Theta})) +
%\frac{1}{2} n \log N.
% \ee
%Assuming that the observed vectors $\{\by(t)\}_{t=1}^T$  are
%zero-mean, i.i.d. Gaussian random vectors, it can be shown
%\cite{Wax85} that the dimension of the signal subspace can be
%estimated by taking the value of $J \in \{1,2,\ldots, I \}$ for
%which
%\begin{eqnarray}
%AIC(J) &=& \hspace{2mm} -2T(I-J) \log \varrho(J) + 2J(2I-J) ,
%\label{AIC} \\
% \nonumber \\
%MDL(J) &=& \hspace{2mm} -T(I-J) \log \varrho(J) + 0.5J(2I-J) \log T
%\label{MDL}
%\end{eqnarray}
%is minimized.
%%
%Here, $T$ is the number of the data vectors $\by(t)$
%used in estimating the data covariance matrix $\bR_{\by \by}$, and
%%
%\be
%\varrho(J) = \hspace{2mm} \frac{(\lambda_{J+1} \lambda_{J+2} \cdots
%\lambda_I)^{\frac{1}{I-J}}}{\frac{1}{I-J} (\lambda_{J+1}+
%\lambda_{J+2} + \cdots + \lambda_I)}
%\label{varrho}
%\ee
%%
%is the ratio of the geometric mean of the $(I-J)$ smallest PCA
%eigenvalues to their arithmetic mean. The estimate $\hat{J}$ of
%the number of terms (sources) is chosen so  it minimizes either
%the $AIC$ or $MDL$ criterion.
%
%Unfortunately, both criteria provide only rough estimates (of the
%number of sources) that are rather very sensitive to variations in
%the SNR and the number of available data samples \cite{Regalia99}.
%Another problem with the $AIC$ and $MDL$  criteria given above is
%that they have been derived by assuming that the data vectors
%$\by(t)$ have a Gaussian distribution \cite{Wax85}. This is done
%for mathematical tractability, by making it possible to derive
%closed form expressions.  The Gaussianity assumption does not
%usually hold exactly in the BSS and other blind signal processing
%applications. Instead of setting the threshold between the signal
%and noise eigenvalues, one might even suppose that the $AIC$ and
%$MDL$ criteria cannot be used in the BSS or ICA problems, because
%there we assume that the source signals $x_j(t)$ are non-Gaussian.
%However, it should be noted that the components of the data
%vectors $\by(t)=\bA \bx(t) +\mbi{e}(t)$ are mixtures of the sources,
%and therefore often have distributions that are not so far from a
%Gaussian one \cite{KARCIAKASPAJ_ijns97}.
%
%%Therefore, while the $MDL$ and $AIC$ criteria yield suboptimal
%%estimates only, they still provide useful formulas that
%%can be used for model order estimation. %   in lack of better criteria.
%%
%Recently Minka proposed very efficient criterion for estimation of

%the true dimensionality of the data on basis of Bayesian model
%selection \cite{Minka}.  The estimate involve an integral over the
%Stiefel manifold which is approximated by Laplace method.
%%\cite{Minka}.
% We refer to this criterion as MInka Bayesian model Selection ($MIBS$)
% which can be summarized as follows: %\cite{Minka}:
% Find a index $J$
% for $1 \leq J \leq J$ such that the cost function is maximized \cite{Minka}
%%
%\be \label{MIBS} MIBS(J)= p(\bY|J) \approx  p_J  \Big(
%\prod_{j=1}^J \lambda_j \Big)^{-T/2} \tilde {\sigma}_J^{-T(I-J)}
%|\bA_J|^{-1/2} (2 \pi)^ {(d_J+J)/2} T^{-J/2}, \ee where
%%
%\be p_J&=&2^{-J} \prod_{i=1}^J \Gamma \big(\frac {I-J+1}{2}\big)
%\; \pi ^{-(I-i+1)/2}, \nonumber \\
%| \bA_n |&= & \prod_{i=1}^J \prod_{j=i+1}^I \big( \hat
%{\lambda}^{-1}_j - \hat {\lambda}^{-1}_i  \big)
%\big(\lambda_i- \lambda_j \big) N, \nonumber \\
%\tilde {\sigma}^2_J &= & \big(\sum_{j=J+1}^J \lambda_j
%\big)/(I-J),
%\qquad d_J= IJ -J (J+1)/2 %\qquad n=1,2,...,m
%\nonumber \ee
% %
%and $\hat \lambda_j$ are identical with $\lambda_j$ expect for $j
%> J $ where $\hat {\lambda}_j = \tilde {\sigma}_J$. In order to
%estimate the latent dimensionality of the data $\bY$, we choose the
%value $J$ that maximizes the approximation to the model evidence
%$p(\bY|J)$ \cite{Minka}.
%
%An approximation of the MIBS leads to the Bayesian Information
%Criterion (BIC) which neglects all terms which do not grow with
%$T$: \be BIC(n)=  \Big( \prod_{j=1}^J \lambda_j \Big)^{-T/2}
%\tilde {\sigma}_J^{-T(I-J)/2} T^{-(d_J+J)/2}. \ee

%One problem with the $AIC$, $MDL$ and $MIBS$ criteria given above
%is that they have been derived by assuming that the data vectors
%$\x(k)$ have a Gaussian distribution \cite{Wax85}. This is done
%for mathematical tractability, by making it possible to derive
%closed form expressions. The Gaussianity assumption does not
%usually hold exactly in the BSS and other signal processing
%applications. Instead of setting the threshold between the signal
%and noise eigenvalues, one might even suppose that the $AIC$,
%$MDL$ and $MIBS$ criteria cannot be used in the BSS or ICA
%problems, because there we assume that the source signals $s_i(k)$
%are non-Gaussian. However, it should be noted that the components
%of the data vectors $\x(k)=\H\s(k) +\mbi{\nu}(k)$ are mixtures of
%the sources, and therefore often have distributions that are not
%so far from a Gaussian one.
%
%Minka's experiments and our own, have shown that his criterion is
%quite robust in respect of distribution of sources and a
%remarkably consistent even with relative few data points.  In
%practical experiments, the
% $MIBS$ criterion have quite often performed very well in
% estimating the true number  of  sources $J$ in BSS and ICA
% problems and even for non-Gaussian sources accurate model
% selection is still feasible.
%%\cite{KARCIAKASPAJ_ijns97}.
%
% The following two conditions may lead to correct estimation of sources.
% %We have found two practical requirements for their successful use.
%Firstly, the number  of  mixtures must be larger than the number
%of  sources. If the number of sources is equal to the number of
%sensors, all criteria inevitably underestimate $J$ by one. The
%second condition is that there must be at least a small amount of
%noise. This also guarantees that the  eigenvalues $\lambda_{J+1},
%\lambda_{J+2},\ldots,\lambda_I$, corresponding to noise, are
%nonzero. It is obvious that zero eigenvalues cause numerical
%difficulties in formulas (\ref{AIC}), (\ref{MDL}) and
%(\ref{MIBS}).
%
%\subsubsection{Some existing methods for dimension estimation in noisy PCA}


A crucial problem  is to decide how many principal
components (PCs) should be retained. A simple ad hoc rule is to
plot the eigenvalues in decreasing
order and search for an elbow where the signal eigenvalues are
on the left side and the noise eigenvalues on the right.  Another
simple technique  is to compute the cumulative percentage of the total
variation explained by the PCs and retain the number of PCs that
represent, say 95\% of the total variation. Such techniques often work well in practice,
but  their disadvantage is that they need a subjective decision
from the user \cite{Ulfarsson08}.  Many sophisticated methods have been
introduced  such as a Bayesian model selection method, which is referred  to
as  the Laplace method.  It is based on computing the evidence
for the data and requires integrating out all the model parameters.
Another method is the BIC (Bayesian Information Criterion)
 method which  can be thought of as an approximation of the Laplace criterion.

A simple heuristic method proposed by He and Cichocki  \cite{He_Tucker},\cite{Niesing}
 computes the Gap  (smoothness) index defined as
\be
GAP(p)=  \frac{\displaystyle  \mbox{var} \left[ \{\tilde \lambda_i \}_{i=p+1}^{I-1}\right]}{\displaystyle \mbox {var} \left[ \{\tilde \lambda_i \}_{i=p}^{I-1}\right]}= \displaystyle \frac{\hat \sigma^2_{p+1}}{\hat \sigma^2_{p}} ,\qquad (p=1,2,\ldots,I-2),
%GAP(p)= \frac{I-p}{I-p-1} \frac{\displaystyle \sum_{i=p+1}^{I-1} \bar \lambda_i^2}{\displaystyle \sum_{i=p}^{I-1} \bar \lambda_i^2} \qquad (p=1,2,\ldots,I-2),
\ee
where $\tilde \lambda_i = \lambda_i -\lambda_{i+1}$ and $\lambda_1 \geq \lambda_2 \geq  \cdots \geq \lambda_I >0$ are eigenvalues of the covariance matrix for the noisy data and the sample variance is computed as follows
\be
\hat \sigma^2_p=\mbox{var} \left[ \{\tilde \lambda_i \}_{i=p}^{I-1}\right]= \frac{1}{I-p} \sum_{i=p}^{I-1} \left( \tilde \lambda_i -
\frac{1}{I-p} \sum_{i=p}^{I-1} \tilde \lambda_i \right)^2.
\ee
The number of components (for each mode)  is selected using the following criterion:
\be
\widehat J = \mbox{arg} \min_{p=1,2,\ldots,I-3} GAP(p).
\ee

Recently, Ulfarsson and  Solo \cite{Ulfarsson08} proposed
 a method called  SURE (Steins Unbiased Risk Estimator)
which allows  the number of  PC components to estimate reliably.


%Automatic selection of dimensionality for PCA is a fundamental
%problem in PCA~\cite{Wax85,Minka01,Stoica04,Ulfarsson08}. Although
%this problem has been widely studied, it is still remains unresolved
%largely. So far, there have been several methods to this problem:
%GMDL or BIC~\cite{Wax85,Minka01,Stoica04}, Laplacian
%method~\cite{Minka01}, SURE (Stein's unbiased risk
%estimator)~\cite{Ulfarsson08}. These methods can be easily
%implemented.
%
%The \emph{minimum description length} (MDL) method selects the model
%by
%\begin{equation}\label{Eq:MDL}
%\aligned
%\widehat{J}&=\arg\min_{k=1,\cdots,I}\text{MDL}(k)\\
%&=\arg\min_{k=1,\cdots,I}\left\{ -\log
%Pr(\boldsymbol{y}_t|\widehat{\Theta}^{(k)})+\frac{1}{2}\left|\Theta^{(k)}\right|\cdot\log
%T \right\},
%\endaligned
%\end{equation}
%where $\left|\Theta^{(k)}\right|$ denotes the number of free
%parameters in $\Theta^{(k)}$. $\left|\Theta^{(k)}\right|$ is
%obtained by counting the number of degrees of freedoms of the space
%spanned by $\Theta^{(k)}$~\cite{Wax85}. In
%(\ref{Eq:ParameterSpace}), note that not all of the parameters are
%independent. The eigenvectors are constrained to have unit norm and
%to be mutually orthogonal. This amounts to reduce $k$ degrees of
%freedom due to the normalization and $k(k+1)/2$ degrees of freedom
%due to the mutual orthogonalization. Combing with
%(\ref{Eq:NumberOfParameter}), we obtain
%\begin{equation}\label{Eq:FreeParameter}
%\left|\Theta^{(k)}\right|=\#\{\Theta^{(k)}\}-k-\frac{k(k+1)}{2}= I k-\frac{k(k+1)}{2}+k+1.
%\end{equation}
%
%From model (\ref{Eq:EVDModel}) and Eq. (\ref{Eq:MDL}), under
%Gaussian prior, we can derive Gaussian MDL (GMDL) as
%\begin{equation}\label{Eq:GMDL}
%\aligned \text{GMDL}(k)=&\frac{T}{2}\sum_{j=1}^k
%\log\left[\lambda_j\left(\frac{1}{I-k}\sum_{i=k+1}^I \lambda_i\right)^{I-k}\right]+\frac{1}{2}\left|\Theta^{(k)}\right|\cdot\log T.
%\endaligned
%\end{equation}
%It is worth mentioning that the GMDL is exactly equivalent to
%Baysian information criterion (BIC)~\cite{Minka01}. More precisely,
%we have $$\text{GMDL}(k)+1=\log[\text{BIC}(k)].$$ GDML was first
%discussed by Wax and Kailath in~\cite{Wax85} and their MDL is as
%follows
%\begin{equation}\label{Eq:WKGMDL}
%\aligned \text{WK-DML(k)}=&\frac{T}{2}
%\log\left[\frac{\prod\limits_{i=k+1}^m\lambda_i^{1/(I-k)}}
%{\frac{1}{I-k}\sum\limits_{i=k+1}^I\lambda_i}\right]^{I-k}
%+\frac{1}{2}\left|\Theta^{(k)}\right|\cdot\log T.
%\endaligned
%\end{equation}
%
%Comparing with (\ref{Eq:GMDL}) and (\ref{Eq:WKGMDL}), we can see
%that $\text{GMDL}(k)$ and $\text{WK-DML(k)}$ are different. From
%experiments, it is observed that WK-GMDL works when the noise is
%very weak. However, it seems that maybe there are some mistakes in
%WK-GMDL. Since Wax and Kailath did not give the details regarding
%the derivation of WK-GMDL and this derivation is not very
%straightforward, we do not exactly know it.
%
%The Laplacian method is derived from a Bayesian framework and is
%based on maximizing a posterior probability (MAP)
%$Pr(\boldsymbol{y}_t|k)$. It is not convenient to obtain an analytic
%expression for $Pr(\boldsymbol{y}_t|k)$ because it requires
%integrating out all the parameters. For this reason, Minka computes

%$Pr(\boldsymbol{y}_t|k)$ using Laplacian
%approximation~\cite{Minka01,Ulfarsson08}. The negative logarithm of
%the Laplacian approximation is given by
%\begin{equation}\label{Eq:LaplacianMethod}
%\aligned -\log Pr(\boldsymbol{y}_t|k)=&-\log
%Pr(\boldsymbol{y}_t|\Theta^{(k)})-\log Pr(p_k)
%-\frac{d+k}{2}\log 2\pi+\frac{1}{2}\log |H_z|+\frac{k}{2}\log T,
%\endaligned
%\end{equation}
%where
%$$
%\left\{
%\begin{array}{l}
%d=mk-\frac{k(k+1)}{2}\\
%Pr(p_k)=2^{-k}\prod\limits_{i=1}^k\Gamma(\frac{I-i+1}{2})\cdot\pi^{-(I-i+1)/2}\\
%|H_z|=\prod\limits_{i=1}^k\prod\limits_{j=i+1}^I T(\widehat{\lambda}_j^{-1}-\widehat{\lambda}_i^{-1})(\lambda_j-\lambda_i)
%\end{array}
%\right.
%$$
%$\Gamma(\cdot)$ is a Gamma function, $Pr(p_k)$ is a noninformative
%prior distribution for $p_k$, $\widehat{\lambda}_j=\lambda_j$ when
% $j\le k$ and $\widehat{\lambda}_j=\widehat{\sigma}^2=\frac{1}{m-k}\sum_{j=k+1}^m\lambda_j$ when
% $j>k$. The Laplacian method selects the model by
%\begin{equation}\label{Eq:LaplacianCriterion}
%\widehat{n}=\arg\min_{k=1,\cdots, I}-\log Pr(\boldsymbol{y}_t|k).
%\end{equation}
%
%The Laplacian criterion can be simplified to BIC
%criterion~\cite{Minka01,Ulfarsson08}: it can yield (\ref{Eq:GMDL})
%if we omit all terms in (\ref{Eq:LaplacianMethod}) that do not grow
%with $T$.

The Laplace, BIC and SURE methods are based on the following considerations ~\cite{Ulfarsson08}.
The \inx{PCA model} is given by
\be
\by_t= \bA \bx_t +\mbi e_t + \bar \by= \mbi \mu_t +\mbi e_t,
\ee
where $\bar \by = (1/T) \sum_{t=1}^T \by_t$ and $\mbi \mu_t = \bA \bx_t + \bar \by$.
The maximum-likelihood estimate (MLE) of PCA  is given by
\be
\widehat \bA = \bV_r (\mbi \Lambda_r - \widehat \sigma^2_r \bI_r)^{1/2} \bQ,  \qquad
\widehat \sigma^2_r = \sum_{j=r+1}^I \lambda_j,
\ee
 where $\bQ \in \Real^{r \times r}$ is an arbitrary  orthogonal rotation matrix,
 $\mbi \Lambda_r = \diag\{\lambda_1,\lambda_2, \ldots, \lambda_r\}$ is a diagonal matrix with
 ordered eigenvalues $\lambda_1>\lambda_2> \cdots > \lambda_r$  and $\bV_r =[\bv_1, \bv_2, \ldots, \bv_r]$ is a matrix  of corresponding eigenvectors of  the data covariance matrix:
 \be
 \bR_y = \frac{1}{T} \sum_{t=1}^T (\by_t -\bar \by) \; (\by_t -\bar \by)^T = \bV_r \mbi \Lambda_r \bV_r^T.
 \ee
Hence, the estimate for $\mbi \mu_t= \bA \bx_t + \bar \by$ is given by
\be
\widehat {\mbi  \mu_t} = \bar \by + \sum_{j=1}^r \bv_j \frac{\lambda_j - \widehat \sigma^2_r}{\lambda_j} \bv^T_j (\by_t -\bar \by).
\ee
Ideally, we would like to choose $r=J$ that minimizes the risk function $R_r =E(||\mbi \mu_t - \widehat {\mbi \mu_t} ||_2^2)$ which is estimated by the SURE formula
\be
\widehat R_r = \frac{1}{T} \sum_{t=1}^T ||\mbi e_t||^2_2 + \frac{2 \sigma^2_e}{T} \sum_{t=1}^T \tr \left(
\frac{\partial \widehat {\mbi \mu_t}}{\partial \by_t^T} \right) - I \sigma_e^2.
\ee
%
In practice, the \inx{SURE algorithm} chooses the number of
components to minimize the SURE formula, that is,
\be
\label{Eq:SURECriterion}
\widehat{J}=\arg\min\limits_{r=1,2,\ldots, I}\widehat{R}_r,
\ee
where the SURE formula  is given by \cite{Ulfarsson08}
\begin{equationarray}{rcll}
\widehat{R}_r &=&(I-r)\widehat{\sigma}_r^2+
\widehat{\sigma}_r^4\sum\limits_{j=1}^r\frac{1}{\lambda_j}
+2\sigma_{e}^2(1-\frac{1}{T})r \nonumber \\
&&-2\sigma_{e}^2\widehat{\sigma}_r^2(1-\frac{1}{T})\sum\limits_{j=1}^r\frac{1}{\lambda_j}+
\frac{4(1-1/T)\sigma_{e}^2\widehat{\sigma}_k^2}{T}\sum \limits_{j=1}^r\frac{1}{\lambda_j}+C_r,
\label{Eq:SUREFormula}
\\
C_r&=&\frac{4(1-1/T)\sigma_{e}^2}{T}\sum\limits_{j=1}^r\sum\limits_{i=r+1}^I \frac{\lambda_j-\widehat{\sigma}_r^2}{\lambda_j-\lambda_i} +\frac{2(1-1/T)\sigma_{e}^2}{T}r(r+1) \nonumber\\
&&-\frac{2(1-1/T)\sigma_{e}^2}{T}(I-1)
\sum\limits_{j=1}^r\left(1-\frac{\widehat{\sigma}_r^2}{\lambda_j}\right), \\
\sigma_{}^2&=&\frac{\text{median}(\lambda_{r+1},\lambda_{r+2},\ldots,\lambda_{I})}{F_{\gamma,1}^{-1}(\frac{1}{2})},
\end{equationarray}
%and
%$\rho=\min(I,T)$,
and  $\gamma=T/I$, $F_{\gamma,1}$ denotes the
Marchenko-Pastur (MP) distribution function with parameter
``$\gamma$".

%Alterative method has been developed by He and Cichocki \cite{He--GAP}
%which attempt to analyze smoothness of distribution of eignevalues
%and detect a fine elbow. For this purpose we  define the GAP function:

Our  extensive numerical experiments indicate that the Laplace method usually
 outperforms  the BIC method while the SURE  method
 can achieve significantly better performance than the Laplace method for
 NMF, NTF and Tucker models.

%\subsection{An experimental example}
%In this section, we would like to test the performance of above
%three methods and  compare them through a numerical experiment.
%Since all methods can rapidly find the results, the computation
%complexity is not an important problem. So we mainly compare their
%percentage of correction selection in different noise levels.
%
%\begin{table}[h!]
%\centering \caption{Performance of  various methods in percentage of correctly estimated number of components ($I=30, J=20,
%T=1000$, 50 Monte Carlo runs for each method.)}\label{Table:NumericalExperiments}
%\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}c@{ }|@{ }c@{ }|c|c|c|c|c|c|c|c}
%\hline SNR level & 25 dB & 24 dB & 20 dB & 18 dB & 16 dB & 15 dB & 14 dB & 13 dB & 12 dB\\
%\hline\hline BIC method & 100\% & 18\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\%\\
%\hline Laplace method & 100\% & 100\% & 98\% & 96\% & 74\% & 42\% & 24\% & 10\% & 2\%\\
%\hline SURE method & 100\% & 100\% & 100\% & 98\% & 96\% & 90\% & 78\% & 68\% & 34\%\\
%\hline
%\end{tabular*}
%\end{table}
%\emph{Example 1}: Consider an array with $I= 30$ sensors  and $J=20$
%sources with $T=1000$ samples. Both 20 sources and the mixing matrix
%$\bA \in \Real_+^{30\times 20}$  were randomly
%generated. Then we obtained the mixtures given by data matrix
%$\boldsymbol{Y}=\boldsymbol{AX}+\boldsymbol{E}$, where the noise
%levels was chosen as shown in Table~\ref{Table:NumericalExperiments}. We
%can see that the SURE is the most reliable among
%these three methods; the BIC method is worse than the other two
%methods.
%%
%We respectively applied the above three methods on the observed array
%$\boldsymbol{Y}$. The detailed results are shown in
%Table~\ref{Table:NumericalExperiments}.

\chapappendix {Nonnegative Rank Factorization Using Wedderborn Theorem -- Estimation of the Number of Components}

Nonnegative Rank Factorization (NRF) is defined  as exact bilinear decomposition:
\be
\bY = \sum_{j=1}^J \ba_j \; \bb_j^T,
\ee
where $\bY \in \Real^{I \times T}$, $\ba_j \in \Real_+^I$ and  $\bb_j \in \Real_+^T$.

%Among the many possible series representations of data matrix $\bY$ by nonnegative
%rank-one matrices $\ba_j \; \bb_j^T$ , the smallest integer $J$ for which such a nonnegative rank-one series representation is attained is called the nonnegative rank of the nonnegative matrix $\bY$ and it is denoted as $\mbox{rank}_+(\bY)$. The nonnegative rank satisfies the following bounds \cite{NRF-Chu}:
% \be
% \mbox{rank}(\bY) \leq  \mbox{rank}_+(\bY) \leq \min\{I,T\}.
% \ee
%It should be noted that an NMF is not necessarily an NRF\inxx{NRF} in the sense that latter demands exact factorization whereas the former is usually only approximation.
% and $J=\mbox{rank}_+ (\bY)$ is nonnegative rank of data matrix.

 In order to perform such decomposition (if it exists), we begin with a simple, but far reaching, result first proved by Wedderburn.
%
\begin{theorem}
Suppose $\bY \in \Real^{I \times T}$, $\ba \in \Real^I$ and $\bb \in \Real^T$. Then
\be
 \mbox{rank}\left(\bY - \sigma^{-1} \bY \; \bb \; \ba^T \; \bY \right) =\mbox{rank} \left(\bY \right)-1,
\ee
if and only if $\sigma = \ba^T \; \bY \; \bb \neq 0$.
\end{theorem}
%
Usually, the Wedderburn theorem is formulated in more general  form:
\begin{theorem}
Suppose $\bY_1 \in \Real^{I \times T}$, $\ba \in \Real^I$ and $\bb \in \Real^T$. Then the matrix
\be
\bY_2 = \bY_1 - \sigma^{-1} \ba \bb^T
\ee
satisfies the rank subtractivity $\mbox{rank}(\bY_2) = \mbox{rank}(\bY_1) -1 $ if and only if there are vectors $\bx \in \Real^T$ and $\by \in \Real^I$ such that
\be
\ba =\bY_1 \bx, \quad \bb= \bY_1^T \; \by, \quad \sigma =\by^T  \; \bY_1 \; \bx \;.
\label{Wedderburn}
\ee
\end{theorem}
%
The Wedderburn rank-one reduction formula (\ref{Wedderburn}) has led to a general matrix
factorization process (e.g., the LDU and QR decompositions, the Lanczos algorithm and
the SVD are special cases) \cite{NRF-Chu}.

The basic idea for the NRF is that, starting with $\bY_1 = \bY$, then so
long as $\bY_j$ are nonnegative, we can repeatedly apply the Wedderburn formula to generate a sequence $\{\bY_j \}$ of matrices by defining
\be
\bY_{j+1} = \bY_{j} - (\by^T_j \, \bY_{j} \; \bx_j)^{-1} \bY_{j} \; \bx_j \; \by_j^T \; \bY_{j},
\qquad (j=1,2,\ldots, J)
\ee
for properly chosen nonnegative vectors  satisfying $\by_j^T \bY_{j} \bx_j \neq 0 $.
We continue such extraction till the residual matrix $\bY_j$ becomes zero  matrix or with negative
elements.
%
Without loss of generality we assume that $\by_j^T \bY_{j} \; \bx_j =1 $ and consider the following  constrained optimization problem \cite{NRF-Chu}
\be
\label{maxmin}
&& \max_{{\bf x}_j \in \Real_+^I, \; {\bf y}_j \in \Real_+^T} \; \min \left( \bY_{j} -  \bY_{j} \; \bx_j \; \by_j^T \bY_{j}\right) \\
&& \mbox{s.t.} \quad \bY_{j} \; \bx_j \geq0, \quad \by_j^T  \; \bY_{j} \geq 0, \quad  \by_j^T \; \bY_{j} \; \ba_j =1. \nonumber
%\label{maxmin}
\ee
There are some  available routines for solving the above optimization problem, especially, the MATLAB routine
``fminmax"  implements a sequential quadratic programming method. It should be noted that this method does not guarantee
finding a global solution, but only a suboptimal local solution.
On the basis of this idea  Dong, Lin and  Chu developed an algorithm for the NRF
using the Wedderburn rank reduction formula \cite{NRF-Chu}.

\begin{itemize}

\item
 Given a nonnegative data matrix $\bY$  and a small threshold
of machine $\varepsilon > 0$ set $j = 1$ and $\bY_1= \bY$.

\item Step 1. If $|| \bY_j|| \geq \varepsilon$ , go to Step 2. Otherwise, retrieve the following information and stop.

1.  $\; \mbox{rank}(\bY) = \mbox{rank}_+(\bY) = j-1$.

2.  The NRF of $\bY$ is approximately given by the summation $\bY \cong \sum_{k=1}^{j-1} \bY_k \; \bx_k \; \by_k^T \; \bY_k$ with an error less than $\varepsilon$.

\item Step 2. Randomly select a feasible initial value $(\bx^{(0)}_j, \by^{(0)}_j)$
 satisfying the nonnegativity constraints.

\item Step 3. Solve the maximin problem (\ref{maxmin}).

\item Step 4. If the objective value at the local maximizer $(\bx_j, \by_j)$ is negative, go to Step 5. Otherwise,
do update as follows and go to Step 1.

1. Define $\bY_{j+1} := \bY_j  - \bY_j \; \bx_j \; \by_j^T \; \bY_j$.


2. Set $j= j + 1$.

\item Step 5. Since the algorithm may get stuck at a local minimum try to restart Steps 2 and 3 multiple times. If
it is decided within reasonable trials that no initial value can result in nonnegative
values, report with caution that the matrix $\bY$ does not have an NRF and stop \cite{NRF-Chu}.
\end{itemize}
%\chapappendix{C. Nonnegative ICA} \label{Sec:NICA}
%
%%\subsection{Nonnegative ICA }
%
%In the NMF models we do not assume explicitly or implicitly that  sources (rows of the matrix $\bX$) are mutually statistically independent.
%However, in some applications it is useful impose constraints that the sources are nonnegative
%and their zero mean values components  are statistically independent as possible.
%Nonnegative ICA model can be formulated as the following model:
%\be
%\bY_{\pm} = \bA_{\pm} \bX_+  + \bE,
%\ee
%where observed data $\bY$ and mixing matrix  $\bA$ can be bipolar and for the source matrix
%$\bX$ are imposed two constraints nonnegativity and simultaneously mutual independence of its rows.
%
%Generally, ICA can be defined as follows: The ICA of a random vector
%        $ \bx (t) \in \Real^J $ is obtained by
%        finding an $J \times I$, (with $ I \geq J $), full rank
%        separating (transformation) matrix $\bW$ such that the output signal vector
%        $\hat \bx(t) = [\hat x_1(t), \hat x_2(t), \ldots, \hat x_J(t)]^T$ (independent components) estimated by
%        \be
%        \hat \bx (t) = \bW \, \by (t),
%        \ee
%        are as independent as possible
%         evaluated  by an information-theoretic
%       cost function such as minima of Kullback-Leibler divergence \cite{Hyv01}.
%
%  Compared with principal component
%analysis (PCA), which removes second-order correlations from
%observed signals, ICA further removes higher-order dependencies.
% Independence of random variables is a more
%general concept than decorrelation.  Roughly speaking, we say that
%random variables $x_p$ and $x_j$ are statistically independent if
%knowledge of the values of $x_p$ provides no information about the
%values of $x_j$.
%%
%Mathematically, the independence of $x_i$ and $x_j$ can be
%expressed by the relationship
%\begin{equation}
%p(x_p, x_j)=p(x_p)p(x_j), \label{eq:5}
%\end{equation}
%where $p(x)$ denotes the probability density function (pdf) of the
%random variable $y$. In other words,  signals  are independent if
%their joint pdf  can be factorized.
%
%If signals are independent, then the generalized
%covariance matrix of $f(x_p)$ and $g(x_j)$, where $f(x)$ and
%$g(x)$ are different, odd nonlinear activation functions (e.g.,
%$f(x)=\tanh(x)$ and $g(x)=x$ for super-Gaussian sources)
% is a non-singular diagonal matrix:
%%\begin{eqnarray}
%%&& \bR_{\bf\,\bg}= E\{\f(\by)\g^T(\by)\}=\nonumber  \\
%%&&\left[\begin{array}{ccc}
%%E\{f(y_1)g(y_1)\}&&0\\
%%&\ddots&\\
%%0&&E\{f(y_n)g(y_n)\}\end{array}\right], \nonumber \\
%%\label{eq:6}
%%\end{eqnarray}
%\begin{eqnarray}
%&& \bR_{f g}= E\{\mbi f (\hat \bx)\bg^T(\hat \bx)\}= \left[\begin{array}{ccc}
%E\{f(\hat x_1) g(\hat x_1)\}&&0\\
%&\ddots&\\
%0&&E\{f(\hat x_J) g(\hat x_J)\}\end{array}\right], \nonumber \\
%\label{eq:6}
%\end{eqnarray}
%i.e., the  covariances $E\{f(\hat x_p) g(\hat x_j)\}$ are all zero for $p
%\neq j$. It should be noted that for odd $f(\hat x)$ and $g(\hat x)$, if the
%probability density function of each zero-mean source signal is
%even, then the terms of the form $E\{f(\hat x_i)\}E\{g(\hat x_p)\}$ equal
%zero.  The true general condition for statistical independence of
%signals is the vanishing of high-order cross-cumulants
%\cite{Cichocki94a,Cichocki96cas,CichBogner97}.
%
%The above diagonalization principle can be expressed as
%\cite{Fiori03}
%%
%\be \bR^{-1}_{fg}=\mbi {\Lambda}^{-1}, \ee  where
%$\mbi {\Lambda}$ is any diagonal positive definite matrix
%(typically, $\mbi {\Lambda}=\bI $ or $\mbi {\Lambda}
%=\diag(\bR_{fg})$). By pre-multiplying the above equation by
%separating matrix $\bW$ and $\mbi {\Lambda}$, we obtain: \be \mbi
%{\Lambda} \bR^{-1}_{fg} \bW =\bW, \ee which suggest the following
%iterative multiplicative learning algorithm
%\be \tilde {\bW}(l+1)&=& \mbi {\Lambda} \bR^{-1}_{fg} \bW(l), \\
%\bW(l+1) &=& \tilde \bW(l+1) \left[ \tilde \bW^T(l+1) \tilde
%\bW(l+1)\right]^{-1/2} , \ee where the last equation represents the
%symmetric orthogonalization to keep algorithm stable. The above
%algorithm is simple and fast but need prewhitening the data.
%%\cite{CiAm02,Cruces2}.
%
%%The main objective of this section is to present some
%%modifications and extensions of existing learning algorithms for
%%ICA/BSS when  preprocessing are imposed for a raw observed data.
%
%In fact, a wide class of ICA algorithms can be expressed in
%general form as (see Table 1) \cite{CiAm02}
% \be
%\nabla \bW(l)=\bW(l+1)-\bW(l)= \eta \bF(\hat \bx) \bW(l),
% %\frac{ d \, \bW (t)}{d \, t} = \mu (t) \,
%%\F(\by(t)) \, \bW(t),
%\label{amaralg2}
% \ee
%where $\hat \bx(t)=\bW(l) \by(t)$ and the matrix $\bF(\hat \bx)$ can take
%different forms, for example $\bF(hat \bx)= \mbi {\Lambda}_J - \mbi f (hat \bx)
%\bg^T(\hat \bx)$ with suitably chosen nonlinearities \ \ \ \
%$\mbi f(\hat \bx)=[f(\hat x_1),...,f(\hat x_J)]$ and $\bg(\hat \bx)=[g(\hat x_1),...,g(\hat x_J)]$
%\cite{Cichocki94a,Cruces-AlvarezSA2002b,CCC00NN,Cruces-AlvarezSA2003b,CiAm02}.
%
%Assuming prior knowledge of the source distributions $p_i(x_p)$,
%we can estimate $\bW$ using maximum likelihood (ML): \be J(\bW,\by)=
%-\frac{1}{2}\log|\det(\bW\bW^T)|- \sum_{i=1}^n \log(p_i(y_i) \ee
%Using natural gradient descent to increase likelihood we get: \be
%\bW(l+1)= \eta \; \left [\bI- \mbi f (\by) \by^T\right]\bW(l), \ee where
%$\mbi f (\by) = [f_1(y_1), f_2(y_2), \ldots, f_n(y_n) ]^T$  is an
%entry-wise nonlinear score function defined by: \be
%f_i(y_i)=-\frac{p'_i(y_i)}{p_i(y_i)}=-\frac{d \log(p_i(y_i)}{d
%(y_i)}. \ee

%\putbib[zbiorcza,NotreDame,bibligraphy_NMF_book]

\putbib[bibligraphy_NMF_book]
%}
\end{bibunit}

